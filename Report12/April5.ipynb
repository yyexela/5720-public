{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c601e08-1f95-4d0e-a8f9-7256f79a0e3c",
   "metadata": {},
   "source": [
    "# APPM X720 Biweekly Report\n",
    "\n",
    "### *Alexey Yermakov*\n",
    "### *April 5, 2022*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a7914f-e588-4f97-a4b9-1e4a3540964e",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "For this report I take a close look at BERT and RoBERTa. This report is action-packed with looking at BERT on a model level, doing data pre-processing, performing model pre-training, attaching a head to RoBERTa, and finally observing how the model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357d1a8-78da-4363-987c-7190b9460399",
   "metadata": {},
   "source": [
    "# Main Content\n",
    "\n",
    "For this report, I wanted to explore BERT through [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf), which is a \"replication study of BERT\". The authors of RoBERTa are from Facebook AI and and University of Washington. I decided to use RoBERTa after Dr. Raissi's comment in class that there was a nice repository available online to use it. Any code block I have will specify if it is my code or not.\n",
    "\n",
    "### **Understanding the architecture**\n",
    "\n",
    "I started out by re-reading the [\"Attention Is All You Need\"](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper which introduces NLP transformers. Recall that the encoder-decoder architecture looks like so:\n",
    "\n",
    "<img src=\"./Images/Transformer.png\" alt=\"Transformer\" width=\"700\">\n",
    "\n",
    "The left part of the figure is the encoder, which is used in BERT, and the right part is the decoder, which is used in GPT. Since this report looks at RoBERTa, we're going to be dealing with the left part of the figure. In trying to understand this architecture, I made a helpful figure of what the positional embeddings look like. The paper gives a nice formula, but essentially what's happening is you get a sine wave across the dimensions of the model:\n",
    "\n",
    "<img src=\"./Images/PositionEmbedding.png\" alt=\"Positional Embedding\" width=\"700\"> \n",
    "\n",
    "I don't have a good intuition as to *why* this helps the model learn the position, but [this video](https://www.youtube.com/watch?v=iDulhoQ2pro) does a decent job of a reasonable sounding answer. Essentially, the model learns that different values added to the embedding matrix represent different positions for the model, which is exactly what we want the position embedding to be doing.\n",
    "\n",
    "Anyways, going through the details of the architecture, I made a detailed flow chart showing all of the math and intermediate matrix sizes that goes into the encoder part of the transformer. The decoder part can be understood much more easily as well, but that's for GPT and not the focus of this report. Hopefully this figure helps, since explaining the model in words would be much harder. I color coded the blocks to match the encoder from the \"Attention Is All You Need\" paper. Also, note that \"x\" is a matrix multiplication and \"\\*\" is a scalar multiplication.\n",
    "\n",
    "<img src=\"./Images/detailed_encoder.png\" alt=\"Encoder Flow Chart\" width=\"900\"> \n",
    "\n",
    "Once we understand what the encoder does, we can look at a higher-level view of BERT with L=3 encoder layers.\n",
    "\n",
    "<img src=\"./Images/N=3.png\" alt=\"N=3 model\" width=\"500\"> \n",
    "\n",
    "There are three main hyperparameters used in the BERT model. `L`, which is the number of encoders, `H`, which is the hidden dimension of the encoders, and `A`, which is the number of self-attention heads in the multi-head attention blocks of each encoder. BERT-base has L=12, H=768, and A=12 for a total of 110 million parameters (source: [BERT paper](https://arxiv.org/pdf/1905.05950.pdf)). BERT-large has L=24, H=1024, and A=16 for a total of 340 million parameters (source: [BERT paper](https://arxiv.org/pdf/1905.05950.pdf)). There is also another parameter which isn't really mentioned, which is the number of input/output tokens T, but conventionally that value is T=512.\n",
    "\n",
    "Now that we understand the BERT architecture, I'll move on to using it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff158355-f4f0-4032-a5d4-0b9ce18fc6b6",
   "metadata": {},
   "source": [
    "## **Pre-Training the Model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1634a00-efe1-45ea-af1d-70240de32f5f",
   "metadata": {},
   "source": [
    "I found [this nice tutorial](https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.pretraining.md) on how to pre-train RoBERTa. I first cloned the [fairseq](https://github.com/facebookresearch/fairseq) repository, which contained the code from the [RoBERTa paper](https://arxiv.org/pdf/1907.11692.pdf). For this section, the below commands were run from the top-most directory of this repository.\n",
    "\n",
    "#### **Pre-Processing/BPE**\n",
    "\n",
    "First, we download the [`wikitext 2013` dataset](https://huggingface.co/datasets/wikitext).\n",
    "\n",
    "```bash\n",
    "# This is not my code, the code is from the tutorial\n",
    "wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
    "unzip wikitext-103-raw-v1.zip\n",
    "```\n",
    "\n",
    "and then we encode it with GPT-2 BPE, which is a variant of Byte Pair Encoding.\n",
    "\n",
    "```bash\n",
    "# This is not my code, the code is from the tutorial\n",
    "mkdir -p gpt2_bpe\n",
    "wget -O gpt2_bpe/encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
    "wget -O gpt2_bpe/vocab.bpe https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n",
    "for SPLIT in train valid test; do \\\n",
    "    python -m examples.roberta.multiprocessing_bpe_encoder \\\n",
    "        --encoder-json gpt2_bpe/encoder.json \\\n",
    "        --vocab-bpe gpt2_bpe/vocab.bpe \\\n",
    "        --inputs wikitext-103-raw/wiki.${SPLIT}.raw \\\n",
    "        --outputs wikitext-103-raw/wiki.${SPLIT}.bpe \\\n",
    "        --keep-empty \\\n",
    "        --workers 60; \\\n",
    "done\n",
    "```\n",
    "\n",
    "What this does is take in the `wikitext-103-raw` dataset for each split of `train`, `valid`, and `test` and\n",
    "\n",
    "1) Split each byte apart, giving us our initial tokenization\n",
    "2) use the pre-computed merges in `gpt2_bpe/vocab.bpe` to merge the bytes (based on merges used when GPT2 was trained for that paper)\n",
    "3) Convert each token into an integer from `gpt_bpe/encoder.json` (this is just a mapping from words to integers)\n",
    "\n",
    "It's important to note that GPT-2 used byte level encoding in its literal sense instead of characters, since [unicode](https://en.wikipedia.org/wiki/Unicode) characters can take up more than a single byte (see [UTF-23](https://en.wikipedia.org/wiki/UTF-32)).\n",
    "\n",
    "Anyways, after running the above code we can compare the data from the files.\n",
    "\n",
    "`wiki.valid.raw`\n",
    "```\n",
    " = Homarus gammarus = \n",
    " \n",
    " Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into planktonic larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \n",
    "```\n",
    "\n",
    "`wiki.valid.bpe`\n",
    "```\n",
    "28 8074 20272 9106 3876 385 796\n",
    "\n",
    "28718 20272 9106 3876 385 837 1900 355 262 3427 43657 393 2219 43657 837 318 257 4693 286 26573 276 43657 422 262 10183 10596 10692 837 19517 6896 290 3354 286 262 2619 6896 764 632 318 7173 3519 284 262 1605 43657 837 367 13 45630 41141 764 632 743 1663 284 257 4129 286 3126 12067 357 1987 287 1267 290 257 2347 286 718 37075 357 1511 18360 1267 837 290 13062 257 39089 5166 286 28421 764 554 1204 837 262 6804 5937 389 4171 837 691 5033 366 43657 2266 366 319 10801 764 337 803 8833 287 262 3931 837 9194 9653 543 389 5281 416 262 12366 329 510 284 257 614 878 289 19775 656 39599 1122 291 37346 764 8074 20272 9106 3876 385 318 257 4047 48243 2057 837 290 318 6768 4978 1262 43657 32195 837 4632 1088 262 3517 36217 764\n",
    "```\n",
    "\n",
    "Great! So BPE worked. However, there's one more step before we can train the model:\n",
    "\n",
    "```bash\n",
    "# This is not my code, the code is from the tutorial\n",
    "wget -O gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
    "fairseq-preprocess \\\n",
    "    --only-source \\\n",
    "    --srcdict gpt2_bpe/dict.txt \\\n",
    "    --trainpref wikitext-103-raw/wiki.train.bpe \\\n",
    "    --validpref wikitext-103-raw/wiki.valid.bpe \\\n",
    "    --testpref wikitext-103-raw/wiki.test.bpe \\\n",
    "    --destdir data-bin/wikitext-103 \\\n",
    "    --workers 4 # I changed this from 60 to 4 since I don't have a super computer\n",
    "```\n",
    "\n",
    "The point of this is to \"pre-process\" the generated BPE files from before (like `wiki.valid.bpe`) to something that the model can expect as input. The output files are binary and can't be read in a text editor. In all honestly I'm not 100% sure what this does, but it's not the focus of this report so I will just take it as it is. The [file](https://github.com/facebookresearch/fairseq/blob/main/fairseq_cli/preprocess.py) that runs this code isn't super well documented either `:(`. Regardless, this process is necessary to pre-train RoBERTa on the wikitext103 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2eebc9-0dd2-42b3-ba0d-3c8a14a761c8",
   "metadata": {},
   "source": [
    "#### **Pre-Training RoBERTa**\n",
    "\n",
    "The next piece of code given to me in the tutorial is to train RoBERTa.\n",
    "\n",
    "```bash\n",
    "# This is not my code, the code is from the tutorial\n",
    "DATA_DIR=data-bin/wikitext-103\n",
    "\n",
    "fairseq-hydra-train -m --config-dir examples/roberta/config/pretraining --config-name base task.data=$DATA_DIR checkpoint.restore_file=./roberta.base/model.pt 2>&1 | tee run.txt\n",
    "```\n",
    "\n",
    "Unfortunately, this code *does not work*! I get the following error:\n",
    "\n",
    "```bash\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'data-bin/wikitext-103/dict.txt'\n",
    "```\n",
    "\n",
    "but the file *does* exist:\n",
    "\n",
    "```bash\n",
    "$ls -al data-bin/wikitext-103/dict.txt \n",
    "-rw-r--r-- 1 alexey 590K Mar 19 14:02 data-bin/wikitext-103/dict.txt\n",
    "```\n",
    "\n",
    "Strange. Using the absolute path fixed this:\n",
    "\n",
    "```bash\n",
    "DATA_DIR='/home/alexey/Desktop/fairseq/data-bin/wikitext-103/'\n",
    "```\n",
    "\n",
    "I stopped the training because I need to configure the hyper-parameters of the training method to account for my measly 1-GPU computer with an NVIDIA 1070 graphics card with 4GB of memory `c:`.\n",
    "\n",
    "```\n",
    "(from running RoBERTA)\n",
    "[2023-03-19 17:13:44,795][fairseq.utils][INFO] - rank   0: capabilities =  6.1  ; total memory = 7.921 GB ; name = NVIDIA GeForce GTX 1070\n",
    "```\n",
    "\n",
    "```\n",
    "(from the tutorial page)\n",
    "Note: The above command assumes training on 8x32GB V100 GPUs\n",
    "```\n",
    "\n",
    "The `base` architecture of RoBERTa from the file `model.py` is the following:\n",
    "\n",
    "```bash\n",
    "# NOT MY CODE! Code from fairseq/models/roberta/model.py\n",
    "@register_model_architecture(\"roberta\", \"roberta\")\n",
    "def base_architecture(args):\n",
    "    args.encoder_layers = safe_getattr(args, \"encoder_layers\", 12)\n",
    "    args.encoder_embed_dim = safe_getattr(args, \"encoder_embed_dim\", 768)\n",
    "    args.encoder_ffn_embed_dim = safe_getattr(args, \"encoder_ffn_embed_dim\", 3072)\n",
    "    args.encoder_attention_heads = safe_getattr(args, \"encoder_attention_heads\", 12)\n",
    "\n",
    "    args.dropout = safe_getattr(args, \"dropout\", 0.1)\n",
    "    args.attention_dropout = safe_getattr(args, \"attention_dropout\", 0.1)\n",
    "    args.activation_dropout = safe_getattr(args, \"activation_dropout\", 0.0)\n",
    "    args.pooler_dropout = safe_getattr(args, \"pooler_dropout\", 0.0)\n",
    "\n",
    "    args.max_source_positions = safe_getattr(args, \"max_positions\", 512)\n",
    "    args.no_token_positional_embeddings = safe_getattr(\n",
    "        args, \"no_token_positional_embeddings\", False\n",
    "    )\n",
    "\n",
    "    # BERT has a few structural differences compared to the original Transformer\n",
    "    args.encoder_learned_pos = safe_getattr(args, \"encoder_learned_pos\", True)\n",
    "    args.layernorm_embedding = safe_getattr(args, \"layernorm_embedding\", True)\n",
    "    args.no_scale_embedding = safe_getattr(args, \"no_scale_embedding\", True)\n",
    "    args.activation_fn = safe_getattr(args, \"activation_fn\", \"gelu\")\n",
    "    args.encoder_normalize_before = safe_getattr(\n",
    "        args, \"encoder_normalize_before\", False\n",
    "    )\n",
    "    args.pooler_activation_fn = safe_getattr(args, \"pooler_activation_fn\", \"tanh\")\n",
    "    args.untie_weights_roberta = safe_getattr(args, \"untie_weights_roberta\", False)\n",
    "\n",
    "    # Adaptive input config\n",
    "    args.adaptive_input = safe_getattr(args, \"adaptive_input\", False)\n",
    "\n",
    "    # LayerDrop config\n",
    "    args.encoder_layerdrop = safe_getattr(args, \"encoder_layerdrop\", 0.0)\n",
    "    args.encoder_layers_to_keep = safe_getattr(args, \"encoder_layers_to_keep\", None)\n",
    "\n",
    "    # Quantization noise config\n",
    "    args.quant_noise_pq = safe_getattr(args, \"quant_noise_pq\", 0)\n",
    "    args.quant_noise_pq_block_size = safe_getattr(args, \"quant_noise_pq_block_size\", 8)\n",
    "    args.quant_noise_scalar = safe_getattr(args, \"quant_noise_scalar\", 0)\n",
    "\n",
    "    # R4F config\n",
    "    args.spectral_norm_classification_head = safe_getattr(\n",
    "        args, \"spectral_norm_classification_head\", False\n",
    "    )\n",
    "```\n",
    "\n",
    "The `base` training hyper-parameters of RoBERTa are from the file `base.yaml` and are the following:\n",
    "\n",
    "```yaml\n",
    "# NOT MY CODE! Code from `examples/roberta/config/pretraining/base.yaml`\n",
    "# @package _group_\n",
    "common:\n",
    "  fp16: false\n",
    "  log_format: json\n",
    "  log_interval: 200\n",
    "\n",
    "checkpoint:\n",
    "  no_epoch_checkpoints: true\n",
    "\n",
    "task:\n",
    "  _name: masked_lm\n",
    "  data: ???\n",
    "  sample_break_mode: complete\n",
    "  tokens_per_sample: 512\n",
    "\n",
    "criterion: masked_lm\n",
    "\n",
    "dataset:\n",
    "  batch_size: 16\n",
    "  ignore_unused_valid_subsets: true\n",
    "\n",
    "optimizer:\n",
    "  _name: adam\n",
    "  weight_decay: 0.01\n",
    "  adam_betas: (0.9,0.98)\n",
    "  adam_eps: 1e-06\n",
    "\n",
    "lr_scheduler:\n",
    "  _name: polynomial_decay\n",
    "  warmup_updates: 10000\n",
    "\n",
    "optimization:\n",
    "  clip_norm: 0\n",
    "  lr: [0.0005]\n",
    "  max_update: 125000\n",
    "  update_freq: [16]\n",
    "\n",
    "model:\n",
    "  _name: roberta\n",
    "  max_positions: 512\n",
    "  dropout: 0.1\n",
    "  attention_dropout: 0.1\n",
    "```\n",
    "\n",
    "I scaled the architecture to have a smaller hidden dimension, less encoder layers, and less attention heads. I pre-trained two models, one with an inverted bottleneck in the Feed Forward block (which is what's normally in the model) and one with a bottleneck (which is not what's typical). The bottleneck was an idea that Dr. Raissi gave me when I met with him in office hours, to observe the effect of the bottleneck. I also modified the pre-training hyper-parameters to learn faster (`lr`), a smaller batch size (`batch_size`), and more \"gradient accumulation\" (`update_freq`). What is \"gradient accumulation\"? The `README.pretraining.md` explains it better than I can:\n",
    "\n",
    "```\n",
    "(from the tutorial page)\n",
    "**Note:** Each GPU uses\n",
    "a batch size of 16 sequences (`dataset.batch_size`) and accumulates gradients to\n",
    "further increase the batch size by 16x (`optimization.update_freq`), for a total batch size\n",
    "of 2048 sequences. If you have fewer GPUs or GPUs with less memory you may need\n",
    "to reduce `dataset.batch_size` and increase dataset.update_freq to compensate.\n",
    "Alternatively if you have more GPUs you can decrease `dataset.update_freq` accordingly\n",
    "to increase training speed.\n",
    "```\n",
    "\n",
    "So, I did exactly what it said for less GPUs.\n",
    "\n",
    "This change caused the number of updates `S` to go from `1,000,000` in the original report down to `125,000`. However, given the amount of time this took to train (2 days for Model 1 and 4 days for Model 2), I think this is ok lol.\n",
    "\n",
    "Since the above code dump is kind of gross, I made some nice tables!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1088982-a004-4fab-9b22-251134d6eaf3",
   "metadata": {},
   "source": [
    " <table>\n",
    "  <tr>\n",
    "    <th>Model Hyper Parameter</th>\n",
    "    <th>RoBERTa Base</th>\n",
    "    <th>Custom Model 1</th>\n",
    "    <th>Custom Model 2</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Encoder (L)ayers</td>\n",
    "    <td>12</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Attention (H)eads</td>\n",
    "    <td>12</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Hidden Dimension (A)</td>\n",
    "    <td>768</td>\n",
    "    <td>192</td>\n",
    "    <td>768</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>FFN Dimension</td>\n",
    "    <td>3072</td>\n",
    "    <td>768</td>\n",
    "    <td>192</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(T)okens</td>\n",
    "    <td>512</td>\n",
    "    <td>512</td>\n",
    "    <td>512</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Total Parameters</td>\n",
    "    <td>110M</td>\n",
    "    <td>10M</td>\n",
    "    <td>42M</td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    " <table>\n",
    "  <tr>\n",
    "    <th>Training Hyper Parameter</th>\n",
    "    <th>RoBERTa Base</th>\n",
    "    <th>Custom Model 1 and 2</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>fp16</td>\n",
    "    <td>true</td>\n",
    "    <td>false</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>batch_size</td>\n",
    "    <td>16</td>\n",
    "    <td>4</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>lr</td>\n",
    "    <td>0.0005</td>\n",
    "    <td>0.0007</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>update_freq</td>\n",
    "    <td>16</td>\n",
    "    <td>64</td>\n",
    "  </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54da32-0976-48ef-9974-5575dc5c0e77",
   "metadata": {},
   "source": [
    "The results from training my custom models are shown below (I got the loss values from parsing `runXXX.txt` in my `pre_training_output/` directory using `parse.py` in the same folder). I first import my libraries then make some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2928178f-e012-40e7-ae24-e6273736121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"GPU Available?\",torch.cuda.is_available())\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e838cd1e-91c5-4497-ab7f-46fab71ba3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 4.725\n",
      "Final validation loss: 4.559\n",
      "Epoch 59 training loss: 4.896\n",
      "Epoch 59 validation loss: 4.634\n"
     ]
    }
   ],
   "source": [
    "train_loss_1 = [12.869, 10.294, 9.821, 9.404, 8.99, 8.096, 7.3, 6.324, 5.947, 5.741, 5.598, 5.495, 5.418, 5.359, 5.311, 5.273, 5.239, 5.211, 5.187, 5.164, 5.145, 5.128, 5.111, 5.098, 5.085, 5.074, 5.063, 5.052, 5.043, 5.035, 5.026, 5.019, 5.012, 5.005, 4.998, 4.993, 4.987, 4.98, 4.976, 4.97, 4.965, 4.961, 4.955, 4.951, 4.946, 4.942, 4.938, 4.935, 4.931, 4.927, 4.923, 4.919, 4.916, 4.913, 4.91, 4.906, 4.902, 4.899, 4.896, 4.894, 4.891, 4.887, 4.884, 4.882, 4.878, 4.876, 4.873, 4.87, 4.867, 4.864, 4.861, 4.858, 4.855, 4.852, 4.85, 4.847, 4.843, 4.842, 4.839, 4.835, 4.833, 4.829, 4.827, 4.824, 4.821, 4.818, 4.816, 4.812, 4.81, 4.806, 4.802, 4.8, 4.797, 4.793, 4.791, 4.787, 4.784, 4.781, 4.777, 4.775, 4.772, 4.768, 4.765, 4.761, 4.758, 4.754, 4.751, 4.747, 4.743, 4.74, 4.737, 4.732, 4.729, 4.726, 4.725]\n",
    "valid_loss_1 = [10.537, 10.0, 9.537, 9.192, 8.297, 7.219, 6.22, 5.645, 5.397, 5.246, 5.15, 5.067, 5.008, 4.958, 4.921, 4.895, 4.874, 4.845, 4.824, 4.811, 4.8, 4.788, 4.766, 4.766, 4.757, 4.746, 4.746, 4.731, 4.731, 4.717, 4.718, 4.72, 4.707, 4.703, 4.697, 4.694, 4.686, 4.684, 4.686, 4.677, 4.673, 4.667, 4.668, 4.666, 4.668, 4.666, 4.665, 4.658, 4.651, 4.654, 4.643, 4.645, 4.638, 4.641, 4.643, 4.639, 4.634, 4.633, 4.634, 4.634, 4.631, 4.627, 4.623, 4.62, 4.623, 4.62, 4.616, 4.616, 4.615, 4.61, 4.608, 4.608, 4.608, 4.607, 4.608, 4.607, 4.603, 4.601, 4.597, 4.599, 4.598, 4.597, 4.596, 4.59, 4.592, 4.592, 4.588, 4.588, 4.586, 4.584, 4.583, 4.58, 4.579, 4.579, 4.576, 4.576, 4.576, 4.57, 4.574, 4.571, 4.57, 4.572, 4.568, 4.568, 4.565, 4.564, 4.564, 4.563, 4.563, 4.561, 4.561, 4.56, 4.56, 4.559, 4.559]\n",
    "\n",
    "plt.plot(train_loss_1, label='Training loss', color='blue')\n",
    "plt.plot(valid_loss_1, label=\"Validation loss\", color='red')\n",
    "plt.title('Loss for Model 1')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Final training loss:\", train_loss_1[-1])\n",
    "print(\"Final validation loss:\", valid_loss_1[-1])\n",
    "\n",
    "print(\"Epoch 59 training loss:\", train_loss_1[58])\n",
    "print(\"Epoch 59 validation loss:\", valid_loss_1[58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e54555e8-f9bb-4773-8919-d67a34377b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 3.235\n",
      "Final validation loss: 4.492\n",
      "Epoch 59 training loss: 3.363\n",
      "Epoch 59 validation loss: 4.839\n"
     ]
    }
   ],
   "source": [
    "train_loss_2 = [11.528, 9.46, 7.1, 5.985, 5.478, 5.172, 4.965, 4.816, 4.701, 4.599, 4.482, 4.384, 4.302, 4.232, 4.171, 4.117, 4.07, 4.026, 3.987, 3.951, 3.918, 3.888, 3.859, 3.833, 3.809, 3.785, 3.764, 3.744, 3.724, 3.705, 3.687, 3.67, 3.654, 3.638, 3.623, 3.608, 3.595, 3.581, 3.567, 3.555, 3.542, 3.531, 3.519, 3.507, 3.496, 3.486, 3.475, 3.463, 3.454, 3.445, 3.435, 3.425, 3.416, 3.407, 3.397, 3.389, 3.38, 3.372, 3.363, 4.671, 4.436, 4.313, 4.222, 4.149, 4.087, 4.032, 3.983, 3.94, 3.899, 3.863, 3.829, 3.798, 3.768, 3.741, 3.715, 3.691, 3.669, 3.647, 3.627, 3.608, 3.589, 3.572, 3.554, 3.538, 3.524, 3.509, 3.494, 3.481, 3.467, 3.455, 3.442, 3.43, 3.418, 3.407, 3.397, 3.386, 3.376, 3.366, 3.357, 3.347, 3.339, 3.329, 3.321, 3.313, 3.305, 3.297, 3.289, 3.282, 3.274, 3.267, 3.26, 3.253, 3.247, 3.24, 3.235]\n",
    "valid_loss_2 = [10.07, 7.939, 5.986, 5.327, 5.008, 4.84, 4.739, 4.686, 4.628, 4.588, 4.557, 4.539, 4.538, 4.521, 4.515, 4.527, 4.523, 4.537, 4.525, 4.545, 4.553, 4.562, 4.564, 4.594, 4.588, 4.601, 4.602, 4.618, 4.63, 4.628, 4.641, 4.652, 4.679, 4.68, 4.683, 4.695, 4.705, 4.713, 4.715, 4.717, 4.732, 4.742, 4.737, 4.754, 4.754, 4.76, 4.778, 4.789, 4.776, 4.784, 4.783, 4.803, 4.815, 4.812, 4.84, 4.834, 4.839, 4.841, 4.839, 4.207, 4.161, 4.146, 4.132, 4.145, 4.142, 4.157, 4.172, 4.19, 4.193, 4.205, 4.22, 4.237, 4.247, 4.263, 4.273, 4.282, 4.296, 4.31, 4.32, 4.324, 4.341, 4.343, 4.356, 4.364, 4.377, 4.37, 4.386, 4.4, 4.403, 4.413, 4.414, 4.42, 4.425, 4.435, 4.435, 4.45, 4.45, 4.455, 4.456, 4.456, 4.461, 4.466, 4.473, 4.476, 4.475, 4.481, 4.482, 4.487, 4.483, 4.485, 4.487, 4.489, 4.489, 4.49, 4.492]\n",
    "\n",
    "plt.plot(train_loss_2, label='Training loss', color='blue')\n",
    "plt.plot(valid_loss_2, label=\"Validation loss\", color='red')\n",
    "plt.vlines(59,3,12, color='black', label='Computer Crash', linestyle='--')\n",
    "plt.title('Loss for Model 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Final training loss:\", train_loss_2[-1])\n",
    "print(\"Final validation loss:\", valid_loss_2[-1])\n",
    "\n",
    "print(\"Epoch 59 training loss:\", train_loss_2[58])\n",
    "print(\"Epoch 59 validation loss:\", valid_loss_2[58])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671fa2e5-e32e-4563-a036-be3fec6b3f05",
   "metadata": {},
   "source": [
    "Fascinating! So first of all, I'm really glad with the first figure's curves, it shows that pre-training process works (thank goodness!) and that the model is getting better with each epoch. Further, we see that the validation loss is *better* than the training loss! This is interesting since my model is a shell of the RoBERTa base model, yet it's still performing well.\n",
    "\n",
    "What I found most interesting is the second model. The spike around epoch 60 is unfortunately because my computer crashed during the training process. Since I don't have a spare computer, I was using my desktop while it was training for other things. Maybe it was for Minecraft, maybe not, we will never know... (it **was** for Minecraft). However, the Facebook code has a checkpoint feature where the model state is saved and training can be resumed. It's clear, however, that this doesn't save everything since the loss curve acts strangely around epoch 60. I believe this to be because the Adam optimizer is used with weight decay, and restarting the training process at epoch 60 reset the weight decay, causing the loss to behave strangely.\n",
    "\n",
    "Before my computer crashed, though, we can see that the validation loss is getting worse, showing model over-fitting. I think this is because the bottleneck in the feed forward block is causing information loss.\n",
    "\n",
    "Thus, we can heuristically conclude that the bottle-necked model (Custom Model 2) is performing overall *worse* than the inverted-bottleneck model despite the final loss being better. I believe this conclusion is true since model 2's final validation loss is almost higher than model 1's. Model 2's validation loss *is* higher than model 1's at epoch 59, when my computer crashed. What's fascinating is we came to the conclusion that model 2 is worse than model 1 despite the total number of parameters being larger for model 2 than for model 1, at about 4x as many. This is because the hidden dimension and the feed-forward dimensions are swapped between the two models. So, I think it's fair to conclude that the inverted bottleneck layer is really helpful for model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5cb4cc-2f98-4be2-a75d-e954a8b1e166",
   "metadata": {},
   "source": [
    "## Attaching a head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1451e71-060c-4dbb-a3c8-7dec075b613b",
   "metadata": {},
   "source": [
    "Now that we have a suitable pre-trained model, I would like to be able to attach a head and get some downstream task working. I decided to challenge myself and try working with [SQuAD 1.1](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/), which is a question-answering dataset. Each data point has a question and a \"context\" paragraph within which the answer appears, and the job of the model is to provide a start and end index for where in the \"context\" the answer to the question appears. SQuAD 1.1 always has an answer where-as SQuAD 2.0 sometimes has questions which are unanswerable. I will use SQuAD 1.1 since it's a tad bit easier to work with. I'll first apply the head to a pre-trained model from Facebook (since I can expect it to perform better than my model above) and then I'll apply it to my own model and compare.\n",
    "\n",
    "Now, moving on to the coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256c7f3-9f37-420a-89b8-b9d9746a5798",
   "metadata": {},
   "source": [
    "I will be sample code from the [RoBERTa](https://github.com/facebookresearch/fairseq/tree/main/examples/roberta) repository. I'll first load a RoBERTa model with the BERT-base parameters (L=12, H=768, and A=12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80f10e72-10b5-40e1-a0c7-c5e20d7667d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/alexey/.cache/torch/hub/pytorch_fairseq_main\n",
      "2023-04-02 11:27:51 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz from cache at /home/alexey/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350\n",
      "2023-04-02 11:27:53 | INFO | fairseq.tasks.masked_lm | dictionary: 50264 types\n",
      "2023-04-02 11:27:55 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 512, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19812, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 999999, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 999999, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 512}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=1, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=999999, max_sentences=16, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=16, curriculum=0, distributed_world_size=512, distributed_rank=0, distributed_backend='nccl', distributed_port=19812, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_base', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0006], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=768, encoder_layers=12, encoder_attention_heads=12, encoder_ffn_embed_dim=3072, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/alexey/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_base', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/alexey/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0006]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0006]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    }
   ],
   "source": [
    "# Code is not mine! Copied (and modified to roberta.base instead of roberta.large) from here:\n",
    "# https://github.com/facebookresearch/fairseq/tree/main/examples/roberta\n",
    "input_len = 512 # Max number of input tokens to the RoBERTa model\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.base')\n",
    "roberta.eval() # disable dropout (or leave in train mode to finetune)\n",
    "roberta.cuda() # Move RoBERTa to GPU\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5f39b-8534-48a2-9b50-6e6947f57d3f",
   "metadata": {},
   "source": [
    "When working with this model, whenever we pass in information (a string, a sentence, or a list of strings/sentences) we want it to be encoded. Below is an example of what encoding looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3851b31c-5160-4150-99b1-394bc636efcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding ['Hello world!', 'goodbye world!'] :\n",
      "\n",
      "Encoded version:\n",
      "tensor([    0, 31414,   232,   328,     2,     2,  8396, 33542,   232,   328,\n",
      "            2])\n",
      "\n",
      "Decoded back to ['Hello world!', 'goodbye world!']\n"
     ]
    }
   ],
   "source": [
    "def encode_example():\n",
    "    input_list = ['Hello world!', 'goodbye world!']\n",
    "    print(\"Encoding\", input_list, \":\")\n",
    "    print()\n",
    "    print(\"Encoded version:\")\n",
    "    tokens = roberta.encode(*input_list)\n",
    "    print(tokens)\n",
    "    print()\n",
    "    print(\"Decoded back to\", roberta.decode(tokens))\n",
    "\n",
    "with torch.no_grad():\n",
    "    encode_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440f802-782f-4c99-8918-3b241c156eb5",
   "metadata": {},
   "source": [
    "Easy enough! Through trial and error I found that the following tokens exist:\n",
    "\n",
    "The ones from the dictionary (`gpt2_bpe/dict.txt`):\n",
    "\n",
    "```\n",
    "13 850314647\n",
    "262 800385005\n",
    "11 800251374\n",
    "284 432911125\n",
    "...\n",
    "```\n",
    "\n",
    "where each token from the previous code block (ex. `31414`) indexes into `dict.txt` *by row* and gives an integer (first column) which can be decoded from `gpt2_bpe/encoder.json`:\n",
    "\n",
    "```\n",
    "{\"!\": 0, \"\\\"\": 1, \"#\": 2, \"$\": 3, \"%\": 4, \"&\": 5, \"'\": 6, \"(\": 7, \")\": 8, \"*\": 9, \"+\": 10, \",\": 11, \"-\": 12, \".\": 13, \"/\": 14, \"0\": 15, \"1\": 16, \"2\": 17, \"3\": 18, \"4\": 19, \"5\": 20, \"6\": 21, \"7\": 22, \"8\": 23, \"9\": 24, \":\": 25, \";\": 26, \"<\": 27, \"=\": 28, \">\": 29, \"?\": 30, \"@\": 31, \"A\": 32, \"B\": 33, \"C\": 34, \"D\": 35, \"E\": 36, \"F\": 37, \"G\": 38, \"H\": 39, \"I\": 40, \"J\": 41, \"K\": 42, \"L\": 43, \"M\": 44, \"N\": 45, \"O\": 46, \"P\": 47, \"Q\": 48, \"R\": 49, \"S\": 50, \"T\": 51, \"U\": 52, \"V\": 53, \"W\": 54, \"X\": 55, \"Y\": 56, \"Z\": 57, \"[\": 58, \"\\\\\": 59, \"]\": 60, \"^\": 61, \"_\": 62, \"`\": 63, ...\n",
    "```\n",
    "\n",
    "The second column of `dict.txt` is the frequency of the token during the GPT2 BPE process (which again, was pre-computed). Through trial and error I found four extra tokens that were not in the dictionary.\n",
    "\n",
    "A `<start>` token indicating the start of a sentence.  \n",
    "A `<end>` token indicating the end of a sentence.  \n",
    "A `<mask>` token indicating a masked token (used during pre-training).  \n",
    "A `<pad>` token for input padding.\n",
    "\n",
    "```\n",
    "0: <start>  \n",
    "1: <pad>  \n",
    "2: <end>  \n",
    "3-50263: dictionary\n",
    "50264: <mask>\n",
    "```\n",
    "\n",
    "Now that we can use tokenize our input, we can observe how the RoBERTa model formats its output.\n",
    "\n",
    "To put a head on the model, I need to know what the output of the last layer of RoBERTa looks like. To this end, I used examples from the [RoBERTa repo](https://github.com/facebookresearch/fairseq/tree/main/examples/roberta) to help out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d8e791f-f8f7-40c1-aaf1-4424c0f84ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is 'Hello world!'\n",
      "Tokens are:\n",
      "tensor([   0,  725,    2,    2,  242,    2,    2,  462,    2,    2,  462,    2,\n",
      "           2,  139,    2,    2, 1437,    2,    2,  605,    2,    2,  139,    2,\n",
      "           2,  338,    2,    2,  462,    2,    2,  417,    2,    2,  328,    2])\n",
      "Of size:\n",
      "torch.Size([36])\n",
      "RoBERTa output is:\n",
      "tensor([[[-0.0540,  0.0623, -0.0063,  ..., -0.0639, -0.0850, -0.0029],\n",
      "         [-0.0173, -0.2597,  0.1312,  ...,  0.1381,  0.0548, -0.0518],\n",
      "         [-0.0579,  0.0434, -0.0455,  ..., -0.1328, -0.1057, -0.0329],\n",
      "         ...,\n",
      "         [ 0.0810, -0.3107,  0.2950,  ...,  0.2949, -0.1659, -0.1569],\n",
      "         [-0.0691, -0.2388, -0.1425,  ..., -0.2976, -0.1276,  0.0167],\n",
      "         [-0.0580,  0.0433, -0.0455,  ..., -0.1327, -0.1058, -0.0329]]],\n",
      "       device='cuda:0')\n",
      "Of size:\n",
      "torch.Size([1, 36, 768])\n",
      "\n",
      "Input is 'goodbye world!'\n",
      "Tokens are:\n",
      "tensor([   0,  571,    2,    2,  139,    2,    2,  139,    2,    2,  417,    2,\n",
      "           2,  428,    2,    2,  219,    2,    2,  242,    2,    2, 1437,    2,\n",
      "           2,  605,    2,    2,  139,    2,    2,  338,    2,    2,  462,    2,\n",
      "           2,  417,    2,    2,  328,    2])\n",
      "Of size:\n",
      "torch.Size([42])\n",
      "RoBERTa output is:\n",
      "tensor([[[-0.0588,  0.0581, -0.0103,  ..., -0.0603, -0.0791, -0.0156],\n",
      "         [-0.0020, -0.1855,  0.1100,  ...,  0.3966, -0.0476,  0.0210],\n",
      "         [-0.0653,  0.0399, -0.0531,  ..., -0.1301, -0.0924, -0.0488],\n",
      "         ...,\n",
      "         [ 0.1110, -0.3614,  0.2556,  ...,  0.3439, -0.1283, -0.1933],\n",
      "         [-0.0325, -0.2500, -0.1530,  ..., -0.2760, -0.0773, -0.0370],\n",
      "         [-0.0653,  0.0399, -0.0531,  ..., -0.1301, -0.0924, -0.0488]]],\n",
      "       device='cuda:0')\n",
      "Of size:\n",
      "torch.Size([1, 42, 768])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_features_example():\n",
    "    for input_list in ['Hello world!', 'goodbye world!']:\n",
    "        print(f\"Input is '{input_list}'\")\n",
    "        tokens = roberta.encode(*input_list)\n",
    "        print(\"Tokens are:\")\n",
    "        print(tokens)\n",
    "        print(\"Of size:\")\n",
    "        print(tokens.shape)\n",
    "        last_layer_features = roberta.extract_features(tokens)\n",
    "        print(\"RoBERTa output is:\")\n",
    "        print(last_layer_features)\n",
    "        print(\"Of size:\")\n",
    "        print(last_layer_features.shape)\n",
    "        print()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    extract_features_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9374bd58-9261-4283-82f2-af87bb91cf26",
   "metadata": {},
   "source": [
    "Great! So it's pretty easy to get the last layer's output. However, you should notice that the output size is not the same, which is going to be a problem when adding a head to the model. We want all of the 512 positions for the input of the BERT architecture to be available for the model to choose when selecting the `start` and `end` positions of the \"context\" paragraph for question-answering. I'll fix this, but first let's download the SQuAD dataset. I used Hugging Face's [SQuAD](https://huggingface.co/datasets/squad) dataset, which is very easy to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25dd996-2dd5-4f14-bd9c-8d838246bafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 19:18:33 | WARNING | datasets.builder | Found cached dataset squad (/home/alexey/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03a90d1f9dc40568294e4a57e23322f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf675268-f824-49de-8bc2-d09b98921ec0",
   "metadata": {},
   "source": [
    "Let's see what we're dealing with by examining the structure of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b16617b-2364-4024-90e3-8e30abfc923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n",
      "\n",
      "{'id': '57301a88b2c2fd1400568878', 'title': 'Liberia', 'context': 'The Americo-Liberian settlers did not identify with the indigenous peoples they encountered, especially those in communities of the more isolated \"bush.\" They knew nothing of their cultures, languages or animist religion. Encounters with tribal Africans in the bush often developed as violent confrontations. The colonial settlements were raided by the Kru and Grebo people from their inland chiefdoms. Because of feeling set apart and superior by their culture and education to the indigenous peoples, the Americo-Liberians developed as a small elite that held on to political power. It excluded the indigenous tribesmen from birthright citizenship in their own lands until 1904, in a repetition of the United States\\' treatment of Native Americans. Because of the cultural gap between the groups and assumption of superiority of western culture, the Americo-Liberians envisioned creating a western-style state to which the tribesmen should assimilate. They encouraged religious organizations to set up missions and schools to educate the indigenous peoples.', 'question': 'What did Americo-liberians exclude tribes from?', 'answers': {'text': ['citizenship in their own lands'], 'answer_start': [638]}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print()\n",
    "print(dataset['train'][80000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beaf5f6-1bf4-4b81-8355-1501a793d406",
   "metadata": {},
   "source": [
    "So the dataset is basically a huge JSON file, making it easy to parse. It has two splits, `train` and `validation` which each contain data. I'll primarily just be using the `context`, `question`, and `answers` sections for my model. The `context` contains the `answer` to the `question`.\n",
    "\n",
    "What I do next is make some helper functions to tokenize the dataset. Tokenizing the data points is tricky because, as mentioned in [this article](https://towardsdatascience.com/how-to-train-bert-for-q-a-in-any-language-63b62c780014):\n",
    "\n",
    "> Our targets are the start and end positions of the answer, which we previously built using the *character* start and end positions within the `context` strings. However, we will be feeding tokens into Bert, so we need to provide the *token* start and end positions.\n",
    "\n",
    "So, I want to convert `answer_start` from the dataset from a character index to a token index. I cheated a bit and tokenized everything before the answer and everything after the answer separately and combined the two lists. This then allowed me to get the token start index by the length of the \"before the answer\" tokens. This might cause some performance loss since this method of tokenization isn't how the entire data point will always be tokenized.\n",
    "\n",
    "If this isn't clear, the main point is that the method below is how I tokenize the data and it mght not be 100% efficient. I tokenize a data point below and detokenize it to show what's going into the model.\n",
    "\n",
    "Also, instead of the `text` segment, I use an answer length integer when tokenized so I know how many tokens are in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e9cfb3-9a8b-4fd5-ad2a-c4ab72dba49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From dataset:\n",
      "\n",
      "Context:\n",
      "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Answer:\n",
      "Santa Clara, California\n",
      "-----------------------------\n",
      "\n",
      "From BPE encoding:\n",
      "Where did Super Bowl 50 take place?Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Answer:\n",
      "Santa Clara, California\n"
     ]
    }
   ],
   "source": [
    "def get_squad_BPE(squad_data_pt):\n",
    "    ans_idx = squad_data_pt['answers']['answer_start'][0]\n",
    "    ans_chars = len(squad_data_pt['answers']['text'][0])\n",
    "    \n",
    "    question_text = squad_data_pt['question']\n",
    "    pre_ans_text = squad_data_pt['context'][:ans_idx]\n",
    "    ans_text = squad_data_pt['context'][ans_idx:ans_idx + ans_chars]\n",
    "    post_ans_text = squad_data_pt['context'][ans_idx + ans_chars:]\n",
    "    \n",
    "    question_enc = roberta.encode(question_text)\n",
    "    pre_ans_enc = roberta.encode(pre_ans_text)[:-1]\n",
    "    ans_enc = roberta.encode(ans_text)[1:-1]\n",
    "    post_ans_enc = roberta.encode(post_ans_text)[1:]\n",
    "    \n",
    "    ans_idx = len(question_enc) + len(pre_ans_enc)\n",
    "    ans_len = len(ans_enc)\n",
    "    \n",
    "    return torch.cat([question_enc, pre_ans_enc, ans_enc, post_ans_enc]), ans_idx, ans_len\n",
    "\n",
    "def show_squad_bpe():\n",
    "    data_pt = dataset['validation'][2]\n",
    "    squad_bpe, ans_idx, ans_len = get_squad_BPE(data_pt)\n",
    "    print(\"From dataset:\")\n",
    "    print(\"\\nContext:\")\n",
    "    print(data_pt['context'])\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(data_pt['answers']['text'][0])\n",
    "    \n",
    "    print(\"-----------------------------\")\n",
    "    \n",
    "    print(\"\\nFrom BPE encoding:\")\n",
    "    print(roberta.decode(squad_bpe))\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(roberta.decode(squad_bpe[ans_idx:ans_idx+ans_len]))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    show_squad_bpe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d4bd1-0171-4089-867d-448ebf404dee",
   "metadata": {},
   "source": [
    "Great! So what we can do next is pre-process the data so that our training takes a lot less time. What I'll do is BPE encode each data point from SQuAD and pad it with `<pad>` tokens so each input is `512` token large. I'll also skip all data points which have tokens more than `512`. Below is some code which saves Python objects in binary files, this is how I'll save the data locally. It uses a neat package from python called [pickle](https://docs.python.org/3/library/pickle.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2812a687-e4b9-48a6-a04a-4a5c73df1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(file_name, obj):\n",
    "    obj_b = pickle.dumps(obj)\n",
    "    with open(file_name,'wb') as save_f:\n",
    "        save_f.write(obj_b)\n",
    "\n",
    "    return None\n",
    "\n",
    "def load_object(file_name):\n",
    "    with open(file_name,'rb') as load_f:\n",
    "        obj_b = load_f.read()\n",
    "        return pickle.loads(obj_b)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a679b-0b57-4aa0-b5e2-189cc2343789",
   "metadata": {},
   "source": [
    "The code below does what I mentioned before, saving the training and validation datasets locally, ready to be used in RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0934f79-06b3-447b-b36d-dcf83da4a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(split, save_dir):\n",
    "    squad_bpes, ans_idxs, ans_lens = (None, None, None)\n",
    "    \n",
    "    for i in range(len(dataset[split])):\n",
    "        # Convert to BPE\n",
    "        data_pt = dataset[split][i]\n",
    "\n",
    "        squad_bpe, ans_idx, ans_len = get_squad_BPE(data_pt)\n",
    "        ans_idx = torch.tensor([ans_idx])\n",
    "        ans_len = torch.tensor([ans_len])\n",
    "        \n",
    "        # Check the data point is not too large\n",
    "        if len(squad_bpe) > 512:\n",
    "            continue\n",
    "\n",
    "        squad_bpe = torch.cat([squad_bpe,torch.ones(input_len-len(squad_bpe), dtype=torch.int64)])\n",
    "        \n",
    "        # Add to our dataset\n",
    "        if i == 0:\n",
    "            squad_bpes = squad_bpe\n",
    "            ans_idxs = ans_idx\n",
    "            ans_lens = ans_len\n",
    "        else:\n",
    "            squad_bpes = torch.vstack([squad_bpes, squad_bpe])\n",
    "            ans_idxs = torch.hstack([ans_idxs, ans_idx])\n",
    "            ans_lens = torch.hstack([ans_lens, ans_len])\n",
    "    \n",
    "    # Save objects\n",
    "    save_object(f\"{save_dir}/{split}_squad_bpes\", squad_bpes)\n",
    "    save_object(f\"{save_dir}/{split}_ans_idxs\", ans_idxs)\n",
    "    save_object(f\"{save_dir}/{split}_ans_lens\", ans_lens)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    pre_process('validation', 'Validation')\n",
    "    pre_process('train', 'Train')\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512a46e-8fd2-4802-92dd-284b12ac9b67",
   "metadata": {},
   "source": [
    "Now that the data is pre-processed, I actually want to batch the data to make the training even faster! From my trial and error testing of the code later in the notebook, I found that this helps a lot with run-time speed. I batch the data in groups of 32 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e79e7ad6-a94a-4b21-ad0f-b73f2200e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(batch_sz, split, load_dir, save_dir):\n",
    "    # Load pre-processed data\n",
    "    squad_bpes = load_object(f\"{load_dir}/{split}_squad_bpes\")\n",
    "    ans_idxs = load_object(f\"{load_dir}/{split}_ans_idxs\")\n",
    "    ans_lens = load_object(f\"{load_dir}/{split}_ans_lens\")\n",
    "    \n",
    "    # Helper variables to keep track of everything\n",
    "    batch_bpe, batch_idx, batch_len = (None, None, None)\n",
    "    batch_bpes, batch_idxs, batch_lens = (None, None, None)\n",
    "    \n",
    "    num_in_batch = 0\n",
    "    \n",
    "    for i in range(squad_bpes.shape[0]):\n",
    "        # Accumulate the data in the batch\n",
    "        if num_in_batch == 0:\n",
    "            batch_bpe = squad_bpes[i]\n",
    "            batch_idx = ans_idxs[i]\n",
    "            batch_len = ans_lens[i]\n",
    "        else:\n",
    "            batch_bpe = torch.vstack([batch_bpe, squad_bpes[i]])\n",
    "            batch_idx = torch.hstack([batch_idx, ans_idxs[i]])\n",
    "            batch_len = torch.hstack([batch_len, ans_lens[i]])\n",
    "            \n",
    "        num_in_batch += 1\n",
    "        \n",
    "        # Stack the batches\n",
    "        if num_in_batch == batch_sz:\n",
    "            batch_bpe = batch_bpe.unsqueeze(0)\n",
    "            batch_idx = batch_idx.unsqueeze(0)\n",
    "            batch_len = batch_len.unsqueeze(0)\n",
    "            \n",
    "            if batch_bpes == None:\n",
    "                batch_bpes = batch_bpe\n",
    "                batch_idxs = batch_idx\n",
    "                batch_lens = batch_len\n",
    "            else:\n",
    "                batch_bpes = torch.vstack([batch_bpes, batch_bpe])\n",
    "                batch_idxs = torch.vstack([batch_idxs, batch_idx])\n",
    "                batch_lens = torch.vstack([batch_lens, batch_len])\n",
    "            \n",
    "            num_in_batch = 0\n",
    "            \n",
    "    # Save data\n",
    "    save_object(f\"{save_dir}/{split}_squad_bpes\", batch_bpes)\n",
    "    save_object(f\"{save_dir}/{split}_ans_idxs\", batch_idxs)\n",
    "    save_object(f\"{save_dir}/{split}_ans_lens\", batch_lens)\n",
    "\n",
    "make_batches(32, 'validation', 'Validation', 'Validation_32')\n",
    "make_batches(32, 'train', 'Train', 'Train_32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff14f8ab-87ed-4f0c-96ba-c0ada52f3dc4",
   "metadata": {},
   "source": [
    "The code block below shows the sizes of the batched data, verifying that the batching was successful. Thus, for validation data I have 328 sets of batches of size 32. Each input is 512 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "044f9c81-ba31-4aa7-b005-6e13794fea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens:   torch.Size([328, 32, 512])\n",
      "answer indices: torch.Size([328, 32])\n",
      "answer lengths: torch.Size([328, 32])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    squad_bpes = load_object('Validation_32/validation_squad_bpes')\n",
    "    print(\"input tokens:  \", squad_bpes.shape)\n",
    "    \n",
    "    ans_idxs = load_object('Validation_32/validation_ans_idxs')\n",
    "    print(\"answer indices:\", ans_idxs.shape)\n",
    "    \n",
    "    ans_lens = load_object('Validation_32/validation_ans_lens')\n",
    "    print(\"answer lengths:\", ans_lens.shape)\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b28b52-90a3-401b-adde-945ed0203aad",
   "metadata": {},
   "source": [
    "This code shows that RoBERTa can evaluate batches of inputs. Once again, I used the [RoBERTa Github Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/roberta) to help with this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "337e635b-9f89-4e57-a3da-728630d40826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size (batched): torch.Size([328, 32, 512])\n",
      "single batch size:      torch.Size([32, 512])\n",
      "roberta output size:    torch.Size([32, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "def test_batch():\n",
    "    squad_bpes = load_object(f\"Validation_32/validation_squad_bpes\")\n",
    "    print(\"dataset size (batched):\", squad_bpes.shape)\n",
    "    squad_bpes = squad_bpes[0]\n",
    "    print(\"single batch size:     \", squad_bpes.shape)\n",
    "    out = roberta.extract_features(squad_bpes)\n",
    "    print(\"roberta output size:   \", out.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25f360-597f-48ff-a48e-599a3f9d78a9",
   "metadata": {},
   "source": [
    "These next few code blocks were what I used to test the various features of PyTorch I'll be using to write the head of my model. Recall that the goal of the head for SQuAD is to return the start and end index of the input tokens for the answer of the question. Since the output of the RoBERTa-base model is (768 x 512), we want two vectors:\n",
    "\n",
    "- S, size (1 x 768), for the *start* position\n",
    "- E, size (1 x 768), for the *end* position\n",
    "\n",
    "where matrix multiplying these vectors with the RoBERTa output results in a (1 x 512) sized vector which we can soft-max to get probabilities for each input position as being either the start or end of the question. This is implemented in PyTorch as a [Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html). \n",
    "\n",
    "The code below implements this by having a model with two parameters, S and E, and the `forward` function takes in an example of what the output of a RoBERTa final layer is and returns the start and end logits (logits are the outputs of softmax).\n",
    "\n",
    "A lot of the code below is inspired from [this PyTorch tutorial](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "219c1055-9158-4c0c-a7d7-d203382a1744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S:\n",
      "Parameter containing:\n",
      "tensor([0.3333, 0.3333, 0.3333], requires_grad=True)\n",
      "\n",
      "E:\n",
      "Parameter containing:\n",
      "tensor([0.3333, 0.3333, 0.3333], requires_grad=True)\n",
      "\n",
      "x:\n",
      "tensor([[[ 0.0847,  0.1499,  0.9340,  0.5284,  0.4223,  1.2086,  0.1089,\n",
      "           0.3661,  0.1478,  1.0801],\n",
      "         [ 1.3761,  0.5365,  1.3152,  0.0532,  1.0324,  1.4838,  0.0750,\n",
      "           0.4529,  1.1189,  1.2805],\n",
      "         [ 1.1872,  0.1170,  2.3401,  0.3625,  0.6703,  1.2795,  1.2705,\n",
      "           0.2398,  0.6365,  0.4558]],\n",
      "\n",
      "        [[-0.0424, -0.0750, -0.4670, -0.2642, -0.2112, -0.6043, -0.0545,\n",
      "          -0.1831, -0.0739, -0.5401],\n",
      "         [-0.6880, -0.2682, -0.6576, -0.0266, -0.5162, -0.7419, -0.0375,\n",
      "          -0.2264, -0.5594, -0.6403],\n",
      "         [-0.5936, -0.0585, -1.1701, -0.1813, -0.3352, -0.6398, -0.6352,\n",
      "          -0.1199, -0.3183, -0.2279]]])\n",
      "\n",
      "start:\n",
      "tensor([[[0.7898, 0.5991, 0.9084, 0.6159, 0.7432, 0.8793, 0.6742, 0.6293,\n",
      "          0.7214, 0.8035],\n",
      "         [0.2102, 0.4009, 0.0916, 0.3841, 0.2568, 0.1207, 0.3258, 0.3707,\n",
      "          0.2786, 0.1965]]])\n",
      "torch.Size([1, 2, 10])\n",
      "\n",
      "end:\n",
      "tensor([[[0.7898, 0.5991, 0.9084, 0.6159, 0.7432, 0.8793, 0.6742, 0.6293,\n",
      "          0.7214, 0.8035],\n",
      "         [0.2102, 0.4009, 0.0916, 0.3841, 0.2568, 0.1207, 0.3258, 0.3707,\n",
      "          0.2786, 0.1965]]])\n",
      "torch.Size([1, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Define the test model\n",
    "class SquadModelTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.input_len = 10\n",
    "        self.hidden_dim = 3\n",
    "        super(SquadModelTest, self).__init__()\n",
    "        \n",
    "        # Our parameters which we're optimizing\n",
    "        self.S = nn.Parameter(torch.ones(self.hidden_dim, requires_grad=True)/self.hidden_dim)\n",
    "        self.E = nn.Parameter(torch.ones(self.hidden_dim, requires_grad=True)/self.hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        start = self.S @ x\n",
    "        start = F.softmax(start,dim=0)\n",
    "        \n",
    "        end = self.E @ x\n",
    "        end = F.softmax(end,dim=0)\n",
    "        \n",
    "        # Return logits for start and end\n",
    "        return start, end\n",
    "\n",
    "# Initialize the model\n",
    "model = SquadModelTest()\n",
    "\n",
    "# Prints showing what's going on\n",
    "def SquadModelText_Prints():\n",
    "    print(\"\\nS:\")\n",
    "    print(model.S)\n",
    "    print(\"\\nE:\")\n",
    "    print(model.E)\n",
    "    x1 = torch.tensor([[0.0847, 0.1499, 0.9340, 0.5284, 0.4223, 1.2086, 0.1089, 0.3661, 0.1478, 1.0801],\n",
    "                       [1.3761, 0.5365, 1.3152, 0.0532, 1.0324, 1.4838, 0.0750, 0.4529, 1.1189, 1.2805],\n",
    "                       [1.1872, 0.1170, 2.3401, 0.3625, 0.6703, 1.2795, 1.2705, 0.2398, 0.6365, 0.4558]])\n",
    "    x2 = x1*-1*0.5\n",
    "    x = torch.vstack([x1.unsqueeze(0), x2.unsqueeze(0)])\n",
    "    print(\"\\nx:\")\n",
    "    print(x)\n",
    "    start, end = model.forward(x)\n",
    "    start = start.unsqueeze(0)\n",
    "    end = end.unsqueeze(0)\n",
    "    print(\"\\nstart:\")\n",
    "    print(start)\n",
    "    print(start.shape)\n",
    "    print(\"\\nend:\")\n",
    "    print(end)\n",
    "    print(end.shape)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    SquadModelText_Prints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f46d4-668c-4cfa-9b47-2f5990d18d28",
   "metadata": {},
   "source": [
    "What we see from the above is the following:\n",
    "- The initialized values of the S and E vectors are 1/(N*H), this is just something I decided to use heuristically\n",
    "- The sample RoBERTa `x` which I'm using as an example is batched (2 outputs), showing that this model works with batches\n",
    "- The outputs for `forward` are also batched (that is, the logits for start and end)\n",
    "\n",
    "Below, we can see that the only two parameters of the model as the S and E vectors. This is a very small head so it will be quick to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f7c0fc7-bcb0-4de6-9291-a9fb4ad91804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.3333, 0.3333, 0.3333], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3333, 0.3333, 0.3333], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0112df22-937c-4863-8a99-3b0fe9d3f7c0",
   "metadata": {},
   "source": [
    "The next thing I want to define is the loss function. This was a bit hard to understand for me from the paper, however, it is seemingly correct here. The way it's calculated is the average of the Cross Entropy loss for the start and end indices. This is verified from looking at the original [BERT code](https://github.com/google-research/bert/blob/master/run_squad.py), which I realized I should look at by this [StackExchange thread](https://ai.stackexchange.com/questions/11900/understanding-how-the-loss-was-calculated-for-the-squad-task-in-bert-paper). Below I define the loss function I'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4805a199-5cea-424b-b024-1f67a0674c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(start, end, start_ex, end_ex):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    loss1, loss2 = (loss_fn(start, start_ex), loss_fn(end, end_ex))\n",
    "    loss = (loss1+loss2)*.5\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e8a49-7084-48c8-965b-a9c63099ddc0",
   "metadata": {},
   "source": [
    "Next, I run two examples on this loss function. The first computes the loss for a single input and the second computes the loss for a b=2 batched input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79a3eaed-caa8-4acc-a6eb-3d0267cfe16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b=2 loss\n",
      "tensor(2.3734)\n",
      "\n",
      "b=1 loss\n",
      "tensor(2.3342)\n"
     ]
    }
   ],
   "source": [
    "def run_my_loss_example_batched():\n",
    "    x1 = torch.tensor([[0.0847, 0.1499, 0.9340, 0.5284, 0.4223, 1.2086, 0.1089, 0.3661, 0.1478, 1.0801],\n",
    "                       [1.3761, 0.5365, 1.3152, 0.0532, 1.0324, 1.4838, 0.0750, 0.4529, 1.1189, 1.2805],\n",
    "                       [1.1872, 0.1170, 2.3401, 0.3625, 0.6703, 1.2795, 1.2705, 0.2398, 0.6365, 0.4558]])\n",
    "    x2 = x1*-1*0.5\n",
    "    x = torch.vstack([x1.unsqueeze(0), x2.unsqueeze(0)])\n",
    "    starts, ends = model.forward(x)\n",
    "    start_exs = torch.tensor([1, 2])\n",
    "    end_exs = torch.tensor([8, 6])\n",
    "    \n",
    "    loss = my_loss(starts, ends, start_exs, end_exs)\n",
    "    print(loss)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    print(\"b=2 loss\")\n",
    "    run_my_loss_example_batched()\n",
    "\n",
    "print()\n",
    "\n",
    "def run_my_loss_example():\n",
    "    x = torch.tensor([[0.0847, 0.1499, 0.9340, 0.5284, 0.4223, 1.2086, 0.1089, 0.3661, 0.1478, 1.0801],\n",
    "                      [1.3761, 0.5365, 1.3152, 0.0532, 1.0324, 1.4838, 0.0750, 0.4529, 1.1189, 1.2805],\n",
    "                      [1.1872, 0.1170, 2.3401, 0.3625, 0.6703, 1.2795, 1.2705, 0.2398, 0.6365, 0.4558]])\n",
    "    starts, ends = model.forward(x)\n",
    "    starts = starts.unsqueeze(0)\n",
    "    ends = ends.unsqueeze(0)\n",
    "    x = x.unsqueeze(0)\n",
    "    start_exs = torch.tensor([1])\n",
    "    end_exs = torch.tensor([8])\n",
    "    \n",
    "    loss = my_loss(starts, ends, start_exs, end_exs)\n",
    "    print(loss)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"b=1 loss\")\n",
    "    run_my_loss_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d3305-0a35-4f51-b9b1-42260bd57a4f",
   "metadata": {},
   "source": [
    "To convince you this implementation is correct, I copied the [BERT loss code](https://github.com/google-research/bert/blob/master/run_squad.py), modified it to work in PyTorch, and print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0962fceb-9555-4525-a4bb-1d8632723abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3342)\n"
     ]
    }
   ],
   "source": [
    "# Modified from: https://github.com/google-research/bert/blob/master/run_squad.py\n",
    "# shout out https://ai.stackexchange.com/questions/11900/understanding-how-the-loss-was-calculated-for-the-squad-task-in-bert-paper\n",
    "def run_bert_loss_example():\n",
    "    x = torch.tensor([[0.0847, 0.1499, 0.9340, 0.5284, 0.4223, 1.2086, 0.1089, 0.3661, 0.1478, 1.0801],\n",
    "                      [1.3761, 0.5365, 1.3152, 0.0532, 1.0324, 1.4838, 0.0750, 0.4529, 1.1189, 1.2805],\n",
    "                      [1.1872, 0.1170, 2.3401, 0.3625, 0.6703, 1.2795, 1.2705, 0.2398, 0.6365, 0.4558]])\n",
    "    start, end = model.forward(x)\n",
    "    start = start.unsqueeze(0)\n",
    "    end = end.unsqueeze(0)\n",
    "    start_ex = torch.tensor([1])\n",
    "    end_ex = torch.tensor([8])\n",
    "    \n",
    "    def compute_loss(logits, positions):\n",
    "        one_hot_positions = F.one_hot(positions, num_classes=10)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        loss = -torch.mean(torch.sum(one_hot_positions * log_probs, axis=-1))\n",
    "        return loss\n",
    "\n",
    "    start_loss = compute_loss(start, start_ex)\n",
    "    end_loss = compute_loss(end, end_ex)\n",
    "\n",
    "    total_loss = (start_loss + end_loss) / 2.0\n",
    "\n",
    "    print(total_loss)\n",
    "\n",
    "with torch.no_grad():\n",
    "    run_bert_loss_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d505eee7-6670-4737-b567-c54ba748b18a",
   "metadata": {},
   "source": [
    "Notice how the `b=1` loss from 2 code blocks prior is the same as the loss just calculated! This shows that my loss function is correct. (unless it's just an unfortunate coincidence!)\n",
    "\n",
    "The next thing I show is what training looks like. The code is inspired from [this PyTorch tutorial](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9214133e-7a0c-4bc0-af20-2963a5f5bb50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  iteration 1 loss: 2.33419132232666\n",
      "  iteration 101 loss: 227.46589422225952\n",
      "  iteration 201 loss: 215.3203227519989\n",
      "  iteration 301 loss: 203.94820046424866\n",
      "  iteration 401 loss: 194.29406368732452\n",
      "  iteration 501 loss: 186.71931540966034\n",
      "  iteration 601 loss: 180.76741874217987\n",
      "  iteration 701 loss: 175.9528923034668\n",
      "  iteration 801 loss: 171.9645926952362\n",
      "  iteration 901 loss: 168.61652374267578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "168.61652374267578"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = SquadModelTest()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Iterate over the same data point 1000 times\n",
    "def train_one_epoch():\n",
    "    x = torch.tensor([[0.0847, 0.1499, 0.9340, 0.5284, 0.4223, 1.2086, 0.1089, 0.3661, 0.1478, 1.0801],\n",
    "                      [1.3761, 0.5365, 1.3152, 0.0532, 1.0324, 1.4838, 0.0750, 0.4529, 1.1189, 1.2805],\n",
    "                      [1.1872, 0.1170, 2.3401, 0.3625, 0.6703, 1.2795, 1.2705, 0.2398, 0.6365, 0.4558]])\n",
    "    start, end = model.forward(x)\n",
    "    start = start.unsqueeze(0)\n",
    "    end = end.unsqueeze(0)\n",
    "    start_ex = torch.tensor([1])\n",
    "    end_ex = torch.tensor([8])\n",
    "    \n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i in range(1000):\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        start, end = model(x)\n",
    "        start = start.unsqueeze(0)\n",
    "        end = end.unsqueeze(0)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = my_loss(start, end, start_ex, end_ex)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "         \n",
    "        if i % 100 == 0:\n",
    "            last_loss = running_loss # loss per batch\n",
    "            print('  iteration {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "train_one_epoch()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0113abf8-d442-40db-8d4f-9def7b3ebf24",
   "metadata": {},
   "source": [
    "Great! We can see that our loss is decreasing for the iterations. Lets see if the model parameters have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2b48e76-c8ac-4f09-b48c-8669075d45cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-8.0907, -0.6667, -6.8391], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-7.0291, 11.0124, -8.6226], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.S)\n",
    "print(model.E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285102f7-01cc-4504-add3-41c416f24500",
   "metadata": {},
   "source": [
    "Excellent! The model parameters are changing from the loss function and back-propagation. This is enough for us to write the actual head of the model. Everything is the same for the most part, except for the `forward` function which now takes in the tokens for a data point (from the pre-processed dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cbe15ce4-4d4f-42fb-af7e-869b620ecc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom RoBERTa head\n",
    "class SquadModel(nn.Module):\n",
    "    def __init__(self, input_len, hidden_dim):\n",
    "        self.input_len = input_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        super(SquadModel, self).__init__()\n",
    "        \n",
    "        # start and end parameters/matrices\n",
    "        self.S = nn.Parameter(torch.ones(self.hidden_dim, requires_grad=True)/self.hidden_dim)\n",
    "        self.E = nn.Parameter(torch.ones(self.hidden_dim, requires_grad=True)/self.hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad(): # Saves GPU memory since we aren't keeping track of gradients\n",
    "            x = roberta.extract_features(x) # BPE Squad input\n",
    "        x = torch.swapdims(x, 1, 2) # Correct dimensions so that matrix multiplication works\n",
    "        \n",
    "        start = self.S @ x\n",
    "        start = F.softmax(start,dim=0)\n",
    "        \n",
    "        end = self.E @ x\n",
    "        end = F.softmax(end,dim=0)\n",
    "        return start, end\n",
    "\n",
    "model = SquadModel(512, 768) # N=512, H=768\n",
    "model.to('cuda') # Use GPU\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5386a3-cfb9-47bd-9c8c-0e79f87338e3",
   "metadata": {},
   "source": [
    "The below function trains one epoch of our training loop. This epoch computes the loss and does back-propagation for every batch of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5073a2-5177-4175-8c2a-e5e828764b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    # Load dataset (this is very fast since it's local)\n",
    "    load_dir = 'Train_32'\n",
    "    split = 'train'\n",
    "    squad_bpes = load_object(f\"{load_dir}/{split}_squad_bpes\")\n",
    "    ans_idxs = load_object(f\"{load_dir}/{split}_ans_idxs\")\n",
    "    ans_lens = load_object(f\"{load_dir}/{split}_ans_lens\")\n",
    "    \n",
    "    # Keep track of loss\n",
    "    num_avg = 100\n",
    "    last_loss = 0. # Loss for past num_avg data points\n",
    "    saved_avg = 0. # Save last avg loss\n",
    "    total_loss = 0. # Save total loss\n",
    "    \n",
    "    for i in range(squad_bpes.shape[0]):\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get data points\n",
    "        squad_bpe = squad_bpes[i]\n",
    "        start_idxs = ans_idxs[i]\n",
    "        start_idxs = start_idxs.to('cuda')\n",
    "        end_idxs = ans_idxs[i] + ans_lens[i]\n",
    "        end_idxs = end_idxs.to('cuda')\n",
    "        \n",
    "        # Get output from model\n",
    "        starts, ends = model.forward(squad_bpe)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = my_loss(starts, ends, start_idxs, end_idxs)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        last_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        if i % num_avg == num_avg - 1:\n",
    "            print(f'avg loss for batches {i-num_avg+1} to {i+1} is {last_loss/(num_avg)}')\n",
    "            saved_avg = last_loss/num_avg\n",
    "            last_loss = 0.\n",
    "\n",
    "    return total_loss/(squad_bpes.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16cf0ea-7d1d-4429-af60-b874140e1790",
   "metadata": {},
   "source": [
    "The below code gets the average batched loss for the dataset, which I use to keep track of the training loss for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49df3bd6-cee9-4f02-84e0-e0d4e4af2929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_loss(split, load_dir):\n",
    "    squad_bpes = load_object(f\"{load_dir}/{split}_squad_bpes\")\n",
    "    ans_idxs = load_object(f\"{load_dir}/{split}_ans_idxs\")\n",
    "    ans_lens = load_object(f\"{load_dir}/{split}_ans_lens\")\n",
    "    \n",
    "    running_loss = 0.\n",
    "    for i in range(squad_bpes.shape[0]):\n",
    "        squad_bpe, ans_idx, ans_len = squad_bpes[i], ans_idxs[i], ans_lens[i]\n",
    "        \n",
    "        start_ex = ans_idx.to('cuda')\n",
    "        end_ex = (ans_idx+ans_len).to('cuda')\n",
    "        \n",
    "        start, end = model.forward(squad_bpe)\n",
    "        \n",
    "        loss = my_loss(start, end, start_ex, end_ex)\n",
    "        running_loss += loss\n",
    "    \n",
    "    return running_loss/squad_bpes.shape[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    #avg_loss = get_dataset_loss('validation', 'Validation_32')\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2744f325-5dcf-4528-b9e0-8ecba476bd90",
   "metadata": {},
   "source": [
    "The below code does the training process for multiple epochs and saves the model after each epoch. I keep track of the validation and training loss for future plots.dataset (spoiler alert: since I didn't save these values to disk like I did the model, when my computer crashed in the middle of training I lost some epoch losses `:(`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419264d-ee2a-4ba5-bea9-6d4bc3e1e111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "EPOCHS = 100\n",
    "EPOCHS_LOAD = 0\n",
    "val_loss_list = list()\n",
    "train_loss_list = list()\n",
    "\n",
    "model.load_state_dict(torch.load('models/saved_model'))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + EPOCHS_LOAD + 1))\n",
    " \n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_train_loss = train_one_epoch()\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        avg_val_loss = get_dataset_loss('validation', 'Validation_32')\n",
    "        #avg_train_loss = get_dataset_loss('train', 'Train_32')\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "\n",
    "    print('LOSS train {} valid {}'.format(avg_train_loss, avg_val_loss))\n",
    "\n",
    "    # Save model state\n",
    "    #torch.save(model.state_dict(), 'models/saved_model')\n",
    "    \n",
    "    # Should've saved losses here :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57474010-5100-4714-a335-0b01cd582a81",
   "metadata": {},
   "source": [
    "*sample output:*\n",
    "```\n",
    "EPOCH 1:\n",
    "avg loss for batches 0 to 100 is 6.233641419410706\n",
    "avg loss for batches 100 to 200 is 6.222269806861878\n",
    "avg loss for batches 200 to 300 is 6.205061135292053\n",
    "avg loss for batches 300 to 400 is 6.173415274620056\n",
    "avg loss for batches 400 to 500 is 6.14119978427887\n",
    "avg loss for batches 500 to 600 is 6.090276947021485\n",
    "avg loss for batches 600 to 700 is 6.046102566719055\n",
    "avg loss for batches 700 to 800 is 6.015944228172303\n",
    "avg loss for batches 800 to 900 is 5.987998256683349\n",
    "avg loss for batches 900 to 1000 is 5.955470862388611\n",
    "avg loss for batches 1000 to 1100 is 5.931954503059387\n",
    "avg loss for batches 1100 to 1200 is 5.919117207527161\n",
    "avg loss for batches 1200 to 1300 is 5.88982015132904\n",
    "avg loss for batches 1300 to 1400 is 5.884027600288391\n",
    "avg loss for batches 1400 to 1500 is 5.885845336914063\n",
    "avg loss for batches 1500 to 1600 is 5.8604001569747926\n",
    "avg loss for batches 1600 to 1700 is 5.866475329399109\n",
    "avg loss for batches 1700 to 1800 is 5.861432657241822\n",
    "avg loss for batches 1800 to 1900 is 5.837738137245179\n",
    "avg loss for batches 1900 to 2000 is 5.831047825813293\n",
    "avg loss for batches 2000 to 2100 is 5.836017422676086\n",
    "avg loss for batches 2100 to 2200 is 5.829210591316223\n",
    "avg loss for batches 2200 to 2300 is 5.821058931350708\n",
    "avg loss for batches 2300 to 2400 is 5.817157845497132\n",
    "avg loss for batches 2400 to 2500 is 5.801015195846557\n",
    "avg loss for batches 2500 to 2600 is 5.813700895309449\n",
    "avg loss for batches 2600 to 2700 is 5.7922753763198855\n",
    "LOSS train 5.795922756195068 valid 5.799070835113525\n",
    "EPOCH 2:\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89d472d5-5d24-492c-a445-b677cbc56214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGxCAYAAABBZ+3pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB480lEQVR4nO3dd3xUVf7/8dfMJDOTThJSSUjovUgv0qQJiuKKIirF7optFV3RdcVdv4uwi6Kurj9dAStYUNZdREBKECmCEmqAQBIgkBACpCeTMuf3x01GhkAKJHMnyef5eMwD5s6dez8zybl533PvPdeglFIIIYQQQrgxo94FCCGEEEJURwKLEEIIIdyeBBYhhBBCuD0JLEIIIYRwexJYhBBCCOH2JLAIIYQQwu1JYBFCCCGE25PAIoQQQgi3J4FFCCGEEG5PAosQbmzJkiUYDAZ27typdynVeuutt2jbti1msxmDwUBWVla9raviezEYDGzcuLHS60op2rZti8FgYPjw4XW6boPBwJw5c2r9vpSUFAwGA0uWLKlyvsOHDzNr1ix69+5Ns2bNCAoKYvDgwXz11VdXVrAQjYQEFiHEVYuPj+fxxx9nxIgRrF+/nq1bt+Ln51fv6/Xz8+ODDz6oND0uLo6jR4+6pIa6tmbNGlauXMmtt97Kl19+yaeffkq7du247bbb+Mtf/qJ3eULoxkPvAoQQDd/+/fsBeOCBB+jXr1+dLLOgoABvb+8q55k8eTKffvopb7/9Nv7+/o7pH3zwAQMHDiQnJ6dOanGlO+64g5kzZ2IwGBzTxo0bR2ZmJvPmzeOPf/wjFotFxwqF0If0sAjRCGzevJmRI0fi5+eHt7c3gwYNYuXKlU7zFBQUMGvWLFq1aoXVaiUoKIg+ffqwdOlSxzxJSUnccccdREZGYrFYCAsLY+TIkcTHx1923cOHD+fuu+8GoH///hgMBmbMmOF4fdGiRfTo0cOxzltuuYWEhASnZcyYMQNfX1/27t3LmDFj8PPzY+TIkdV+7ilTpgA4fYbs7GyWL1/Ovffee8n3nDt3jkceeYQWLVpgNptp3bo1L7zwAjabzWm+nJwcHnjgAYKDg/H19eX666/n8OHDl1xmYmIid955J6GhoVgsFjp16sTbb79dbf2X0rx5c6ewUqFfv34UFBRw7ty5K1quEA2d9LAI0cDFxcUxevRounfvzgcffIDFYuGdd95hwoQJLF26lMmTJwPw1FNP8fHHH/PKK69wzTXXkJ+fz759+zh79qxjWePHj6esrIz58+fTsmVLMjMz2bJlS5Xno7zzzjssXbqUV155hcWLF9OxY0dCQkIAmDt3Ls8//zxTpkxh7ty5nD17ljlz5jBw4EB27NhBu3btHMspLi7mpptu4qGHHuK5556jtLS02s/u7+/PpEmTWLRoEQ899BCghRej0cjkyZNZuHCh0/xFRUWMGDGCo0eP8vLLL9O9e3d+/PFH5s6dS3x8vCPkKaWYOHEiW7Zs4c9//jN9+/blp59+Yty4cZVqOHDgAIMGDaJly5YsWLCA8PBwVq9ezeOPP05mZiYvvfRStZ+jJjZs2EBISAihoaF1sjwhGhwlhHBbixcvVoDasWPHZecZMGCACg0NVbm5uY5ppaWlqmvXrioqKkrZ7XallFJdu3ZVEydOvOxyMjMzFaAWLlxYJ3WeP39eeXl5qfHjxzvNe/z4cWWxWNSdd97pmDZ9+nQFqEWLFtV6fRs2bFCA2rdvn1JKqb59+6oZM2YopZTq0qWLGjZsmON97777rgLUF1984bS8efPmKUCtWbNGKaXUqlWrFKDeeOMNp/n+7//+TwHqpZdeckwbO3asioqKUtnZ2U7zPvroo8pqtapz584ppZRKTk5WgFq8eHGNPuOF3n///UvWI0RTIoeEhGjA8vPz2b59O5MmTcLX19cx3WQyMXXqVFJTUzl06BCgHVJYtWoVzz33HBs3bqSwsNBpWUFBQbRp04a///3vvPbaa+zatQu73X7FtW3dupXCwkKnw0MA0dHRXHfddaxbt67Se2699dZar2fYsGG0adOGRYsWsXfvXnbs2HHZw0Hr16/Hx8eHSZMmOU2vqLGipg0bNgBw1113Oc135513Oj0vKipi3bp13HLLLXh7e1NaWup4jB8/nqKiIrZt21brz3ShVatWMXPmTCZNmsRjjz12VcsSoiGTwCJEA3b+/HmUUkRERFR6LTIyEsBxyOfNN9/kj3/8IytWrGDEiBEEBQUxceJEEhMTAe1y3XXr1jF27Fjmz59Pr169CAkJ4fHHHyc3N7fWtVWs93K1XXgoCsDb29vpxNmaMhgM3HPPPXzyySe8++67tG/fniFDhly2pvDw8ErniISGhuLh4eGo6ezZs3h4eBAcHOw0X3h4eKXllZaW8tZbb+Hp6en0GD9+PACZmZm1/kwVVq9eze9+9ztGjx7Np59+eslzW4RoKiSwCNGABQYGYjQaSUtLq/TaqVOnAO0kTgAfHx9efvllDh48SHp6Ov/617/Ytm0bEyZMcLwnJiaGDz74gPT0dA4dOsQf/vAH3nnnHZ555pla11bxx/5ytVXUVeFq/hjPmDGDzMxM3n33Xe65554qazp9+jRKKafpGRkZlJaWOmoKDg6mtLS0UqhKT093eh4YGIjJZGLGjBns2LHjko+K4FJbq1evZuLEiQwbNozly5djNpuvaDlCNBYSWIRowHx8fOjfvz9ff/210yEeu93OJ598QlRUFO3bt6/0vrCwMGbMmMGUKVM4dOgQBQUFleZp3749f/rTn+jWrRu//vprrWsbOHAgXl5efPLJJ07TU1NTWb9+fY2uAqqpFi1a8MwzzzBhwgSmT59+2flGjhxJXl4eK1ascJr+0UcfOV4HGDFiBACffvqp03yfffaZ03Nvb29GjBjBrl276N69O3369Kn0uLiXpibWrFnDxIkTufbaa1mxYoVcxiwEcpWQEA3C+vXrSUlJqTR9/PjxzJ07l9GjRzNixAhmzZqF2WzmnXfeYd++fSxdutTRc9G/f39uvPFGunfvTmBgIAkJCXz88ccMHDgQb29v9uzZw6OPPsptt91Gu3btMJvNrF+/nj179vDcc8/VuuZmzZrx4osv8vzzzzNt2jSmTJnC2bNnefnll7FarXV29UyFV199tdp5pk2bxttvv8306dNJSUmhW7dubN68mb/97W+MHz+eUaNGATBmzBiGDh3Ks88+S35+Pn369OGnn37i448/rrTMN954g2uvvZYhQ4bw+9//ntjYWHJzczly5Aj//e9/Wb9+fa0+x+bNm5k4cSLh4eE8//zzlS4p79y58xUdOhOiwdP7rF8hxOVVXA1zuUdycrJSSqkff/xRXXfddcrHx0d5eXmpAQMGqP/+979Oy3ruuedUnz59VGBgoLJYLKp169bqD3/4g8rMzFRKKXX69Gk1Y8YM1bFjR+Xj46N8fX1V9+7d1euvv65KS0trVOelrmb697//rbp3767MZrMKCAhQN998s9q/f7/TPNOnT1c+Pj61/l6qunpKqcpXCSml1NmzZ9XDDz+sIiIilIeHh4qJiVGzZ89WRUVFTvNlZWWpe++9VzVr1kx5e3ur0aNHq4MHD1a6Skgp7Qqge++9V7Vo0UJ5enqqkJAQNWjQIPXKK684zUMNrhJ66aWXqvyZb9iwobqvR4hGyaDURQdzhRBCCCHcjJzDIoQQQgi3J4FFCCGEEG5PAosQQggh3J4EFiGEEEK4PQksQgghhHB7EliEEEII4fYazcBxdrudU6dO4efnJ/fbEEIIIRoIpRS5ublERkZiNF6+H6XRBJZTp04RHR2tdxlCCCGEuAInTpwgKirqsq83msDi5+cHaB9Yhq0WQgghGoacnByio6Mdf8cvp9EElorDQP7+/hJYhBBCiAamutM55KRbIYQQQrg9CSxCCCGEcHsSWIQQQgjh9hrNOSxCCCEqU0pRWlpKWVmZ3qWIJspkMuHh4XHVQ45IYBFCiEaquLiYtLQ0CgoK9C5FNHHe3t5ERERgNpuveBkSWIQQohGy2+0kJydjMpmIjIzEbDbLoJrC5ZRSFBcXc+bMGZKTk2nXrl2Vg8NVRQKLEEI0QsXFxdjtdqKjo/H29ta7HNGEeXl54enpybFjxyguLsZqtV7RcuSkWyGEaMSudG9WiLpUF7+H8psshBBCCLcngUUIIYQQbq9WgWXOnDkYDAanR3h4eJXv+fTTT+nRo4fjDOF77rmHs2fPOs2zfPlyOnfujMVioXPnznzzzTe1/yRCCCHEZQwfPpwnn3yyxvOnpKRgMBiIj4+vt5oANm7ciMFgICsrq17X0xjUuoelS5cupKWlOR579+697LybN29m2rRp3Hfffezfv58vv/ySHTt2cP/99zvm2bp1K5MnT2bq1Kns3r2bqVOncvvtt7N9+/Yr+0RCCCEarIt3ii9+zJgx44qW+/XXX/PXv/61xvNHR0eTlpZG165dr2h9ou7V+iohDw+PantVKmzbto3Y2Fgef/xxAFq1asVDDz3E/PnzHfMsXLiQ0aNHM3v2bABmz55NXFwcCxcuZOnSpZddts1mw2azOZ7n5OTU9qMIIYRwM2lpaY7/f/755/z5z3/m0KFDjmleXl5O85eUlODp6VntcoOCgmpVh8lkqvHfOuEate5hSUxMJDIyklatWnHHHXeQlJR02XkHDRpEamoq3333HUopTp8+zVdffcUNN9zgmGfr1q2MGTPG6X1jx45ly5YtVdYxd+5cAgICHI/o6OjafpSa2bIFli2D9PT6Wb4Qokr5+fmOvev8/HxtYmkpbNsGxcX6FtfAKAX5+fo8lKpZjeHh4Y5HQECA49SD8PBwioqKaNasGV988QXDhw/HarXyySefcPbsWaZMmUJUVBTe3t5069at0g7vxYeEYmNj+dvf/sa9996Ln58fLVu25L333nO8fvEhoYpDN+vWraNPnz54e3szaNAgpzAF8MorrxAaGoqfnx/3338/zz33HD179qzVz2n58uV06dIFi8VCbGwsCxYscHr9nXfeoV27dlitVsLCwpg0aZLjta+++opu3brh5eVFcHAwo0aN+q3dNHSqFr777jv11VdfqT179qi1a9eqYcOGqbCwMJWZmXnZ93z55ZfK19dXeXh4KEDddNNNqri42PG6p6en+vTTT53e8+mnnyqz2VxlLUVFRSo7O9vxOHHihAJUdnZ2bT5S9Xr1UgqU+t//6na5QogaycvLU4ACVF5enjbxrbe0dvnqq/oW58YKCwvVgQMHVGFhoWNaXp72tenxqPjR1cbixYtVQECA43lycrICVGxsrFq+fLlKSkpSJ0+eVKmpqervf/+72rVrlzp69Kh68803lclkUtu2bXO8d9iwYeqJJ55wPI+JiVFBQUHq7bffVomJiWru3LnKaDSqhIQEp3Xt2rVLKaXUhg0bFKD69++vNm7cqPbv36+GDBmiBg0a5FjmJ598oqxWq1q0aJE6dOiQevnll5W/v7/q0aPHZT9jxXLPnz+vlFJq586dymg0qr/85S/q0KFDavHixcrLy0stXrxYKaXUjh07lMlkUp999plKSUlRv/76q3rjjTeUUkqdOnVKeXh4qNdee00lJyerPXv2qLffflvl5ubW/suvY5f6fayQnZ1do7/ftQosF8vLy1NhYWFqwYIFl3x9//79KiIiQs2fP1/t3r1bff/996pbt27q3nvvdczj6empPvvsM6f3ffLJJ8pisdSqlpp+4FobN05rbYsW1e1yhRA1csnA8tRTWrucOlXf4txYYw4sCxcurPa948ePV08//bTj+aUCy9133+14brfbVWhoqPrXv/7ltK6LA8sPP/zgeM/KlSsV4PiO+/fvr2bOnOlUx+DBg2sVWO688041evRop3meeeYZ1blzZ6WUUsuXL1f+/v4qJyen0rJ++eUXBaiUlJTLrk8vdRFYrmqkWx8fH7p160ZiYuIlX587dy6DBw/mmWeeAaB79+74+PgwZMgQXnnlFSIiIggPDyf9osMtGRkZhIWFXU1pdSc0VPs3I0PfOoQQvykq0v6Vdlkr3t6Ql6ffuutKnz59nJ6XlZXx6quv8vnnn3Py5EnHOY4+Pj5VLqd79+6O/1ccesqo5nfqwvdEREQA2t+sli1bcujQIR555BGn+fv168f69etr9LkAEhISuPnmm52mDR48mIULF1JWVsbo0aOJiYmhdevWXH/99Vx//fXccssteHt706NHD0aOHEm3bt0YO3YsY8aMYdKkSQQGBtZ4/e7sqsZhsdlsJCQkOH5oFysoKKg0up3JZAK0+wsADBw4kLVr1zrNs2bNGgYNGnQ1pdWZ0mAtsJSclA2jEHowGo0MGzaMYcOG/bY9kcByRQwG8PHR51GXtzG6OIgsWLCA119/nWeffZb169cTHx/P2LFjKa7mHKeLT9Y1GAzY7fYav6fi3kwXvufi+zVV/K2rKaVUlcvw8/Pj119/ZenSpURERPDnP/+ZHj16kJWVhclkYu3ataxatYrOnTvz1ltv0aFDB5KTk2tVg7uqVWCZNWsWcXFxJCcns337diZNmkROTg7Tp08HtCt8pk2b5ph/woQJfP311/zrX/8iKSmJn376iccff5x+/foRGRkJwBNPPMGaNWuYN28eBw8eZN68efzwww+1ul6+Pr3xmRZYMvef1rkSIZomLy8vNm7cyMaNGx1XiJw4ogWWghRplwJ+/PFHbr75Zu6++2569OhB69atL9vzX586dOjAzz//7DRt586dtVpG586d2bx5s9O0LVu20L59e8cOv4eHB6NGjWL+/Pns2bOHlJQURy+OwWBg8ODBvPzyy+zatQuz2dxoxjar1SGh1NRUpkyZQmZmJiEhIQwYMIBt27YRExMDaJejHT9+3DH/jBkzyM3N5Z///CdPP/00zZo147rrrmPevHmOeQYNGsSyZcv405/+xIsvvkibNm34/PPP6d+/fx19xKtTEhQG6aBkT04It5GVVkQ0YM7K0E6RkLsQN2lt27Zl+fLlbNmyhcDAQF577TXS09Pp1KmTS+t47LHHeOCBB+jTpw+DBg3i888/Z8+ePbRu3brGy3j66afp27cvf/3rX5k8eTJbt27ln//8J++88w4A//vf/0hKSmLo0KEEBgby3XffYbfb6dChA9u3b2fdunWMGTOG0NBQtm/fzpkzZ1z+PdSXWgWWZcuWVfn6kiVLKk177LHHeOyxx6p836RJk5wuy3IroaFwADzOSWARwl0Yi7UeFg9VCllZ0EiO0Ysr8+KLL5KcnMzYsWPx9vbmwQcfZOLEiWRnZ7u0jrvuuoukpCRmzZpFUVERt99+OzNmzKjU61KVXr168cUXX/DnP/+Zv/71r0RERPCXv/zFMWBes2bN+Prrr5kzZw5FRUW0a9eOpUuX0qVLFxISEti0aRMLFy4kJyeHmJgYFixYwLhx4+rpE7uWQdX2AJubysnJISAggOzsbPz9/etsuQvu3sXTn/YixycC/7xTdbZcIUTN5OfnExsbC2hjY/j4+HCgxSg6n1qnzXDwIHTooF+BbqqoqIjk5GRatWqF1WrVu5wma/To0YSHh/Pxxx/rXYquqvp9rOnf76u6SqgpsLbUzmHxKcgAux3kVu1CuFxmZqbTc1NJ0W9PTp+WwCLcQkFBAe+++y5jx47FZDKxdOlSfvjhh0oXlogrI4GlGn6tQwAwqTI4fx6Cg3WuSAjhUfpbYFGnM5AzWIQ7MBgMfPfdd7zyyivYbDY6dOjA8uXLGTVqlN6lNQoSWKoR0sLMOQIJ4rx2CaUEFiF0d2FgKTyWQR0O8SHEFfPy8uKHH37Qu4xGS45vVCM0FDKQweOEcCcXBpaCFGmXQjQFEliqERb2W2Cxp8mYD0K4A8+y3wJL8Qlpl0I0BRJYqhEaCqfRbhNQeEz25IRwB2b7b4HFflrapRBNgZzDUg2zGbItoWDTup6rvjOFEKKuGY1Gx71jKobmvzCwGDMlsAjRFEhgqYFCPy2wFMv9hIRwOS8vL3bs2OE07cLAYj4v7VKIpkAOCdVASaB2DouSc1iE0F9pKR6UOZ5650m7FKIpkMBSAypEO4fFeFb25ITQXVGR01Pv4myw2XQqRrir4cOHO91ENzY2loULF1b5HoPBwIoVK6563XW1nKrMmTOHnj171us63I0ElhowRWg9LOYsCSxCuFpBQQGxsbHExsZSUFDgFFhK0e5ey5kzOlUn6tqECRMuO9Da1q1bMRgM/Prrr7Ve7o4dO3jwwQevtjwnlwsNaWlpjeb+Pe5EAksNmKO0wOKdJ4FFCFdTSnHs2DGOHTuGUsoRWIrxlDGSGqH77ruP9evXc+zYsUqvLVq0iJ49e9KrV69aLzckJARvb9cMMRgeHo7FYnHJupoSCSw14NOqPLCU5FTqjhZCuFZpvnb4pwjrb4HltJzHUiNKQX6+Po8a3mf3xhtvJDQ0lCVLljhNLygo4PPPP+e+++7j7NmzTJkyhaioKLy9venWrRtLly6tcrkXHxJKTExk6NChWK1WOnfufMn7/fzxj3+kffv2eHt707p1a1588UVKSkoAWLJkCS+//DK7d+/GYDBgMBgcNV98SGjv3r1cd911eHl5ERwczIMPPkheXp7j9RkzZjBx4kT+8Y9/EBERQXBwMDNnznSsqybsdjt/+ctfiIqKwmKx0LNnT77//nvH68XFxTz66KNERERgtVqJjY1l7ty5jtfnzJlDy5YtsVgsREZG8vjjj9d43a4iVwnVQGBsADbMWCjW9uRattS7JCGarOKcIjzQAkvFGEm2ExnI/mwNFBSAr68+687LA5/qB4bw8PBg2rRpLFmyhD//+c8YDNqdor788kuKi4u56667KCgooHfv3vzxj3/E39+flStXMnXqVFq3bk3//v2rXYfdbud3v/sdzZs3Z9u2beTk5Did71LBz8+PJUuWEBkZyd69e3nggQfw8/Pj2WefZfLkyezbt4/vv//eMRx/QEBApWUUFBRw/fXXM2DAAHbs2EFGRgb3338/jz76qFMo27BhAxEREWzYsIEjR44wefJkevbsyQMPPFDt5wF44403WLBgAf/v//0/rrnmGhYtWsRNN93E/v37adeuHW+++SbffvstX3zxBS1btuTEiROcOHECgK+++orXX3+dZcuW0aVLF9LT09m9e3eN1utSqpHIzs5WgMrOzq7zZW/bptRxopQCpXbsqPPlCyEuLy8vTwEKUHl5eer8DzuVAnWMaPWp8W6lQGX+cb7eZbqdwsJCdeDAAVVYWPjbxLw8bTumxyMvr8a1JyQkKECtX7/eMW3o0KFqypQpl33P+PHj1dNPP+14PmzYMPXEE084nsfExKjXX39dKaXU6tWrlclkUidOnHC8vmrVKgWob7755rLrmD9/vurdu7fj+UsvvaR69OhRab4Ll/Pee++pwMBAlXfB51+5cqUyGo0qPT1dKaXU9OnTVUxMjCotLXXMc9ttt6nJkydftpaL1x0ZGan+7//+z2mevn37qkceeUQppdRjjz2mrrvuOmW32ysta8GCBap9+/aquLj4suu7Wpf8fSxX07/f0sNSAxXD80eTKneGFUJnJbnaYdkirOT7hEKu1sMiasDbW+vp0GvdNdSxY0cGDRrEokWLGDFiBEePHuXHH39kzZo1AJSVlfHqq6/y+eefc/LkSWw2GzabDZ8a9OAAJCQk0LJlS6KiohzTBg4cWGm+r776ioULF3LkyBHy8vIoLS3F39+/xp+jYl09evRwqm3w4MHY7XYOHTpEWJjWS9ilSxdMJpNjnoiICPbu3VujdeTk5HDq1CkGDx7sNH3w4MGOnpIZM2YwevRoOnTowPXXX8+NN97ImDFjALjttttYuHAhrVu35vrrr2f8+PFMmDABDw/3ighyDksNXHg/ocIUOVYuhJ4qAkux0YotQGuXpaekXdaIwaAdltHjYajdrt59993H8uXLycnJYfHixcTExDBy5EgAFixYwOuvv86zzz7L+vXriY+PZ+zYsRQXF9do2eoS59MYLqpv27Zt3HHHHYwbN47//e9/7Nq1ixdeeKHG67hwXRcv+1Lr9PT0rPSa3W6v1bouXs+F6+7VqxfJycn89a9/pbCwkNtvv51JkyYBEB0dzaFDh3j77bfx8vLikUceYejQobU6h8YVJLDUgJcXnPPQUnB+suzJCeFKBoOBzp0707lzZwwGgyOwlBgtlDbX2qVBrhJqdG6//XZMJhOfffYZH374Iffcc4/jj++PP/7IzTffzN13302PHj1o3bo1iYmJNV52586dOX78OKdOnXJM27p1q9M8P/30EzExMbzwwgv06dOHdu3aVbpyyWw2U1ZWRlU6d+5MfHw8+fn5Tss2Go20b9++xjVXxd/fn8jISDZv3uw0fcuWLXTq1MlpvsmTJ/P+++/z+eefs3z5cs6dOwdoI0rfdNNNvPnmm2zcuJGtW7fWuIfHVdyrv8eNFfiGQpZ0PQvhat7e3uzfv9/xvDSvIrBYMYRpPSweMjx/o+Pr68vkyZN5/vnnyc7OZsaMGY7X2rZty/Lly9myZQuBgYG89tprpKenO/1xrsqoUaPo0KED06ZNY8GCBeTk5PDCCy84zdO2bVuOHz/OsmXL6Nu3LytXruSbb75xmic2Npbk5GTi4+OJiorCz8+v0uXMd911Fy+99BLTp09nzpw5nDlzhscee4ypU6c6DgfVhWeeeYaXXnqJNm3a0LNnTxYvXkx8fDyffvopAK+//joRERH07NkTo9HIl19+SXh4OM2aNWPJkiWUlZXRv39/vL29+fjjj/Hy8iImJqbO6qsL0sNSQ8XNtA1jWZpsGIXQkyOwmKx4Rmrt0poj7bIxuu+++zh//jyjRo2i5QVXZ7744ov06tWLsWPHMnz4cMLDw5k4cWKNl2s0Gvnmm2+w2Wz069eP+++/n//7v/9zmufmm2/mD3/4A48++ig9e/Zky5YtvPjii07z3HrrrVx//fWMGDGCkJCQS15a7e3tzerVqzl37hx9+/Zl0qRJjBw5kn/+85+1+zKq8fjjj/P000/z9NNP061bN77//nu+/fZb2rVrB2gBcN68efTp04e+ffuSkpLCd999h9FopFmzZrz//vsMHjyY7t27s27dOv773/8SHBxcpzVeLYO61MG8BignJ4eAgACys7NrfVJUTbzZ5yMe/2U6xzuOpmXCmjpfvhCiZg48/QGdX7ufOL8bSXr6be6ZE0OJwRPPMlutz5NozIqKikhOTqZVq1ZYrVa9yxFNXFW/jzX9+y09LDUVrnXdeUrXsxAuVVBQQJcuXejSpQsFBQWUFWg9LKUeVvzaaD0snqoEsrJ0rFIIUd/kHJYaMrfQNoxeuRJYhHAlpRQHDhxw/N9eEVg8rYREW8nGnwBytEEdAwP1LFUIUY+kh6WGvGO1wOJXmAG1vNRMCFF3VHlgKfO0Og05IPcTEqJxk8BSQ36tQwAwqTI4f17naoRoulShFljsnlZCQ38LLMUnZCwWIRozCSw1FBpl5hzl3c2yJyeEblT5DUjtZiuBgXDGoJ1flpck7VKIxkwCSw1J17MQbqJIu1uzslgxGCDXq3wU6mPSLoVozCSw1NCFgaXomHQ9C6Ebm9bDoizapZFF/lq7LDkpgUWIxkyuEqohX1/INIaBXet6llENhHANg8HgGHHTYDBguCiwlASFQjqo07IjIURjJoGlhgwGyPMOhTwoOi57ckK4ire3NykpKY7nxvLAQvngUyo0DA6Ax1lpl0I0ZnJIqBaKyu8MW3JKNoxC6MVYrAUWg5d2zxaPCK1dWrKlXQrhKgaDgRUrVrh0nRJYaqEsqPykW+l6FkI3vwUWrYfF2lJrl975Elgak/T0dB577DFat26NxWIhOjqaCRMmsG7dOr1Lq5ElS5bQrFmzel/P8uXLGT58OAEBAfj6+tK9e3f+8pe/OO7C3JhIYKkFFapdPildz0K4TmFhIX379qVv374UFhZiKtECi7E8sFQM6uhbkgU2m15lijqUkpJC7969Wb9+PfPnz2fv3r18//33jBgxgpkzZ+pdnkuVlZVhv8xgpS+88AKTJ0+mb9++rFq1in379rFgwQJ2797Nxx9/fMn3lJSU1Ge59Us1EtnZ2QpQ2dnZ9baO96b9qBSo0wFt620dQghneXl5ClCAysvLU4cD+yoF6n8P/1cppdTa1WWqGA+lQKkTJ3Su1n0UFhaqAwcOqMLCwkqv5eXlXfZx8fxVzVtQUFCjeWtr3LhxqkWLFpd87/nz5x3/P3bsmLrpppuUj4+P8vPzU7fddptKT093vP7SSy+pHj16qA8++EBFR0crHx8f9fDDD6vS0lI1b948FRYWpkJCQtQrr7zitA5AvfPOO+r6669XVqtVxcbGqi+++MLx+oYNGxTgVMuuXbsUoJKTkx2vX/h46aWXlFJK2Ww29cwzz6jIyEjl7e2t+vXrpzZs2OBYzuLFi1VAQID673//qzp16qRMJpNKSkqq9D1s375dAWrhwoWX/A4rarvwO2jVqpUyGAzKbrerVatWqcGDB6uAgAAVFBSkbrjhBnXkyBHH+202m5o5c6YKDw9XFotFxcTEqL/97W9O39H777+vJk6cqLy8vFTbtm3Vf/7zn0vWolTVv481/fstPSy1UNH17Ctdz0LoxqNU62Ex+Wg9LGERRs6gjUQtYyTVjK+v72Uft956q9O8oaGhl5133LhxTvPGxsZecr7aOHfuHN9//z0zZ87Ex8en0usVh1mUUkycOJFz584RFxfH2rVrOXr0KJMnT3aa/+jRo6xatYrvv/+epUuXsmjRIm644QZSU1OJi4tj3rx5/OlPf2Lbtm1O73vxxRe59dZb2b17N3fffTdTpkwhISGhRp9h0KBBLFy4EH9/f9LS0khLS2PWrFkA3HPPPfz0008sW7aMPXv2cNttt3H99deTmJjoeH9BQQFz587l3//+N/v37yc0NLTSOj799FN8fX155JFHLlnDhYejjhw5whdffMHy5cuJj48HID8/n6eeeoodO3awbt06jEYjt9xyi6M358033+Tbb7/liy++4NChQ3zyySfExsY6rePll1/m9ttvZ8+ePYwfP5677rqrfg9FVRlnGhBX9LB8vei8thcHSl0iJQoh6t7FPSwnvNopBWrNn39USil1+rRSu+ihFKiS/67St1g3UtUeLRft/V/4GD9+vNO83t7el5132LBhTvM2b978kvPVRkXPwddff13lfGvWrFEmk0kdP37cMW3//v0KUD///LNSSutd8Pb2Vjk5OY55xo4dq2JjY1VZWZljWocOHdTcuXOdvp+HH37YaX39+/dXv//975VS1fewKPVbT8mFjhw5ogwGgzp58qTT9JEjR6rZs2c73geo+Pj4Kj//uHHjVPfu3aucRyntO/D09FQZGRlVzpeRkaEAtXfvXqWUUo899pi67rrrlN1uv+T8gPrTn/7keJ6Xl6cMBoNaterSbbAueljksuZaCGoVgA0zFoq1PbmWLfUuSYgmx7PMuYclOBh2lw/qmHvkNHK/5url5eVd9jWTyeT0PKOKXiuj0bmT/sLLz6+U9rdQuwqlKgkJCURHRxMdHe2Y1rlzZ5o1a0ZCQgJ9+/YFtF4fPz8/xzxhYWGYTCan2sPCwip9zoEDB1Z6XtE7caV+/fVXlFK0b9/eabrNZiM4ONjx3Gw207179yqXpZSq9juqEBMTQ0hIiNO0o0eP8uKLL7Jt2zYyMzMdPSvHjx+na9euzJgxg9GjR9OhQweuv/56brzxRsaMGeO0jAtr9PHxwc/Pr8rfl6slgaUWQsMMZBBKNKkSWITQiaddCywevlpgMZkgyxoGRVBwLEMCSw1c6lCLq+e9nHbt2mEwGEhISGDixImXne9yf7Avnu7p6en0usFguOS0y53YevF88FtQqwhXULOTWe12OyaTiV9++aVSMLzw0JmXl1e1YaR9+/Zs3ryZkpKSSp/nYpf6uUyYMIHo6Gjef/99IiMjsdvtdO3aleLiYgB69epFcnIyq1at4ocffuD2229n1KhRfPXVV45lXOn3eKXkHJZauHB4fhkGXAh9mMsDi6ffb+NNF/pW3LFZ2mVDFxQUxNixY3n77bfJz8+v9HpWVhag9aYcP36cEydOOF47cOAA2dnZdOrU6arruPiclm3bttGxY0cAR29FWlqa4/WLe1/MZjNlZWVO06655hrKysrIyMigbdu2To/w8PBa1XfnnXeSl5fHO++8c8nXK76nSzl79iwJCQn86U9/YuTIkXTq1Inz589Xms/f35/Jkyfz/vvv8/nnn7N8+XJdL5eWwFIL2p1hf+t6FkK4RvPmzWnevDkAZrt26fKFgaW4mdYu7ekSWBqDd955h7KyMvr168fy5ctJTEwkISGBN99803GoZtSoUXTv3p277rqLX3/9lZ9//plp06YxbNgw+vTpc9U1fPnllyxatIjDhw/z0ksv8fPPP/Poo48C0LZtW6Kjo5kzZw6HDx9m5cqVLFiwwOn9sbGx5OXlsW7dOjIzMykoKKB9+/bcddddTJs2ja+//prk5GR27NjBvHnz+O6772pVX//+/Xn22Wd5+umnefbZZ9m6dSvHjh1j3bp13HbbbXz44YeXfW9gYCDBwcG89957HDlyhPXr1/PUU085zfP666+zbNkyDh48yOHDh/nyyy8JDw93ydgylyOBpRYMBsjx0sZiKUiRDaMQruDj48OZM2c4c+YMPl5emNG6rC8MLPbmWmAxnpEdicagVatW/Prrr4wYMYKnn36arl27Mnr0aNatW8e//vUv4LeRVgMDAxk6dCijRo2idevWfP7553VSw8svv8yyZcvo3r07H374IZ9++imdO3cGtEMhS5cu5eDBg/To0YN58+bxyiuvOL1/0KBBPPzww0yePJmQkBDmz58PwOLFi5k2bRpPP/00HTp04KabbmL79u1O5+LU1Lx58/jss8/Yvn07Y8eOpUuXLjz11FN0796d6dOnX/Z9RqORZcuW8csvv9C1a1f+8Ic/8Pe//91pHl9fX+bNm0efPn3o27cvKSkpfPfdd5XOW3Ilg7rwIFwDlpOTQ0BAANnZ2fj7+9fbej4Me5bpGX8naeJTtP5mQfVvEELUncJC8PYGIOHnXDr11Y77v3/Ldzyw4gZSQ68h6vSvelboNoqKikhOTqZVq1ZYrXK71towGAx88803VZ5DI2qnqt/Hmv79lh6WWioJlK5nIXRTVOT4ryXgt42eJbp8eP48aZdCNFYSWGpJhWgbRlOmdD0L4QqFhYUMHz6c4cOHU1h+YmApJqy+v13kWDE8v19hhjZSkhCi0ZHLmmvJGKGdw2LOkj05IVzBbrcTFxcHQHFuIV6ADQsX9ir7t9UCi6cqgaws7Qx5Ia5QIzlTotGRHpZakq5nIfRjy9EOCRVhdQosIdFWsik/9i3D8wvRKElgqSVH17PtDNTjADlCiMqKc7RLmouwYrH8Nv3CMZLk/DJn0lsg3EFd/B5KYKmlgLbagEEeqhQuMdCOEKL+lOT91sNy4UChISG/BZbcoxJY4LdRSAsKCnSuRIjffg+rG5W3KnIOSy2FRpk5RyBBnNe6ni+4/4MQon6V5mqBpdjofFmkpydkeYZCCeQnnSZAj+LcjMlkolmzZo57u3h7e9f43jNC1BWlFAUFBWRkZNCsWbNKtySoDQkstVTR9RzEeUrTMvCogyGghRA1U5KnHRK6OLAA5PmEQRYUHZcelgoVw73X5w3phKiJZs2a1fr2AxeTwFJLwcFwkFA6cojcIxkEXqd3RUI0ft7lg8WV5ms9LCWXCCy2gFDIgtI0+eNcwWAwEBERQWhoaI1uzidEffD09LyqnpUKElhqyWSCHEso2LSuZ7l4Uoj65ePj47gJ3sE5SwEoMVUOLGXBoXAMDNKbUInJZKqTPxhC6ElOur0Ceb7aWCw2uTOsEC5Vlq8dEirxuMRQ86HaSbce52RQRyEaIwksV6A4QNswStezEK5VVqAdEiq9RGDxjNJ2JLxypF0K0RhJYLkCZcFaYDGckQ2jEPWtqKiIG264gRtuuIHCnFwAyi4RWKwttXbpWyDtUojGqFaBZc6cORgMBqdHVWf9zpgxo9L8BoOBLl26OOZZsmTJJecpuuAmZ+7GEKZtGM3S9SxEvSsrK+O7777ju+++o6ygUJvmWTmw+LUpDyylWVBc7MoShRAuUOuTbrt06cIPP/zgeF7ViVxvvPEGr776quN5aWkpPXr04LbbbnOaz9/fn0OHDjlNc+fboVd0PVtzZU9OCFdShdqOTJm58vYhsHUgpZjwoEwbIykqytXlCSHqUa0Di4eHR42vpQ4ICCAg4LchnFasWMH58+e55557nOarrqfG3TjdGVYI4TKqvOdVeVoqvRYWYSSDUCJJQ53OwCCBRYhGpdbnsCQmJhIZGUmrVq244447SEpKqvF7P/jgA0aNGkVMTIzT9Ly8PGJiYoiKiuLGG29k165d1S7LZrORk5Pj9HCViq5nn9IccONDV0I0OkXaVUJ2S+UelgvvJ5SXJDsTQjQ2tQos/fv356OPPmL16tW8//77pKenM2jQIM6ePVvte9PS0li1ahX333+/0/SOHTuyZMkSvv32W5YuXYrVamXw4MEkJiZWuby5c+c6enACAgKIjo6uzUe5KsGtA7Bh1p7ImA9CuI6tvIflEoHFaoVzpor7Ccn5ZUI0NrUKLOPGjePWW2+lW7dujBo1ipUrVwLw4YcfVvveJUuW0KxZMyZOnOg0fcCAAdx999306NGDIUOG8MUXX9C+fXveeuutKpc3e/ZssrOzHY8TJ07U5qNclbBwg9wZVggdGIrLezQvEVgAcr21dll0TNqlEI3NVV3W7OPjQ7du3artDVFKsWjRIqZOnYrZbK66IKORvn37VrtMi8WCv7+/08NV5M6wQujDYNMOCXGZk/IL/bQT4ktOSrsUorG5qsBis9lISEggIiKiyvni4uI4cuQI9913X7XLVEoRHx9f7TL15OkJ5z0lsAjhCj4+PiilUErhU1qqTfS6dGApDdLapTot7VKIxqZWgWXWrFnExcWRnJzM9u3bmTRpEjk5OUyfPh3QDtNMmzat0vs++OAD+vfvT9euXSu99vLLL7N69WqSkpKIj4/nvvvuIz4+nocffvgKP5Jr5Dm6nuVYuRCuYio/JGS4TGBRIVq7NGVKuxSisanVZc2pqalMmTKFzMxMQkJCGDBgANu2bXNc9ZOWlsbx48ed3pOdnc3y5ct54403LrnMrKwsHnzwQdLT0wkICOCaa65h06ZN9OvX7wo/kmsU+YdBNpSekj05IVzFVKIFFuNlAotHpBZYLNnSLoVobGoVWJYtW1bl60uWLKk0LSAggIKCgsu+5/XXX+f111+vTRluoTQoFE5I17MQ9a2oqIipU6cC8OcSbVti9L50YDFHa+eweOdLuxSisan1wHGiXPmdYU3nZMMoRH0qKyvjq6++AuDPft2AywcWn1ZauwywZYBSYDC4pkghRL2Tmx9eoYquZ2u2HCsXwlU8SrWrhDx8Lx1YmrULAcBTlUBWlqvKEkK4gASWK2SN0bqefaTrWQiX8SzTzmG5XGAJaelFDn7aExnUUYhGRQLLFfJtXdH1fAbsdp2rEaJpqC6whIXBabSdiYIUCSxCNCYSWK5QRdezB6XS9SyEi3jaqz4k5OsLmUZtZyLniAQWIRoTCSxXKDTKzHmaAaDS5TwWIVzBorQeFk/fyndrrpBj1QJLQbK0SyEaEwksVyg09LeuZ7kzrBCuYUXrYTH7X7qHBaDAV2uXthPSLoVoTCSwXCGrFc6W3xk2R4bnF6LeeHt7k5eXR97Zs3iXT6sqsBQ3kxuTCtEYSWC5Crle5V3PcnKfEPXGYDDg4+ODj8lExagqVQUWe3OtXRrOSLsUojGRwHIVCn21DWPJCTlWLkR9U4VFjv9b/S9/13djuNYuzeelXQrRmMhIt1ehODAM0qXrWYj6ZLPZeOihh7Dn5PE+YMeK1evyI9h6RmnnsHjlSbsUojGRwHIVVEgoJIAxUzaMQtSX0tJSPvzwQwD+BRRjxXr5I0J4x2o9LP6F0i6FaEzkkNBVMEaUdz1nyYZRCFcpworl8lc1499Wa5d+ZVlQXOyaooQQ9U4Cy1WwRGkbRu88OVYuhKvYsGKsYssV3DaQUkzaExmeX4hGQwLLVfBupR0r9y+SjaIQrmIzVnE8CAiLMJKBtjMhY7EI0XhIYLkKFV3PvmU5UFRUzdxCiLpQXE1gCQiAMwatbWYdlsAiRGMhgeUqNG8TQDGe2pMzZ/QtRogmoqSawGIwQLZZCyx5yRJYhGgsJLBchbBwg6PrWe5bIoRrlJiqDiwAeT7lh4SOS7sUorGQwHIVfH3hjEE7jyU7UfbkhKgP3t7eZGRkEDfzLbyBUlMVlwiVKwrQ2mXpKWmXQjQWEliuUnb5nWHlBohC1A+DwUBISAgByowBKPWovoelLEhrl3KVkBCNhwSWq1TgI1cjCOEK9gLtxPZSz+oDiyFMa5ce56RdCtFYyEi3V8nWLBQyoeyUHCsXoj7YbDaeeuopzmzexcdAWQ0Ci0ekFli8sqVdCtFYSGC5SmXBYXAEkDvDClEvSktLeeeddwBYDNhrEFi8YrVzWHwLpF0K0VjIIaGrVNH17HleNoxCuILdXH1g8WmltctmxRmgVH2XJIRwAQksV8mzRXnXc44EFiFcoSaBJbB9CACelEB2dn2XJIRwAQksV8krpvxGa4VyrFwIV1CW6gNLaIwXOfgBUJIqbVOIxkACy1Xya6sdKw8oPgN2u87VCNEEWKsPLMHBcBoZI0mIxkQCy1X6reu5FLKy9C1GiKagBoHFaITznlrvZ85RCSxCNAYSWK5SWLSZ8zQDZCwWIVyiBoEFIM9LCyyFKdIuhWgMJLBcpYAAOFN+P6HzB+VYuRB1zcvLi+TkZL4LH44XYPCqWWAp9NfapZzDIkTjIIHlKhkMcN6iHSvPleH5hahzRqOR2NhYopURI2DwrllgKQnU2qU6Le1SiMZAAksdqOh6LjomG0Yh6otHiTY0v7GGPSwqRGuXxrPSLoVoDCSw1IGiiq7nk7JhFKKuFRcX88wzz/CPvCMUAybv6u/WDGCK0NqlJUvapRCNgQSWOlBacWfY03KsXIi6VlJSwj/+8Q8+KM6gBDD51KyHxRKttUuffGmXQjQGEljqgArVjpWb5M6wQtQ7D9+aBRaf1uVjJNmkXQrRGEhgqQMVd4a1ZMuGUYj6VtPAEtBOa5f+ZVlQXFyPFQkhXEECSx2wttQ2jL7S9SxEvatpYGneLpBSTACUpcnOhBANnQSWOuDdIRqA0KLjUFKiczVCNG6efjULLCFhRlKJAiAnPqk+SxJCuIAEljoQ1LMlufhiVsXYDyXqXY4QjVpNA4uHBySauwKQ89Pe+ixJCOECEljqQNv2RvYbtA3jmfWyYRSiPpn9axZYADIjugFQ+LO0SyEaOgksdcDDA04FaRvGrB9lwyhEXfLy8mLfzz+zD/ACLAE1DyxlnbV2aT4s7VKIhk4CSx3Jb61tGNknG0Yh6pLRaKRzbCu6oG2wLP41GzgOwHeg1i5DM/aBUvVToBDCJSSw1BFzb23D2OyEBBYh6lppnjYsvw0zVu+ab7Zaju5ACR74luWgjh2vr/KEEC4ggaWOhI7UAktYfjLk5upcjRCNR3FxMS/Nm88cIAcL1pofEaJTDzMH6QhA1qY99VKfEMI1JLDUkU7XBnOKCABsv+zTuRohGo+SkhLm/r+3eBnIxYql5keE8PKCY37azkTmBun9FKIhk8BSR8LC4JCntmFMXysbRiHqQzFmjLXcamXHaO2yLF7apRANmQSWOmIwQEZEdwDyt8uGUYj6UGyoxfGgcsYeWrv0TZF2KURDJoGlDpV2LL+E8pBsGIWoD1cSWIKHl59flnVI7ikkRAMmgaUO+Qwov4Qyfa9cQilEPSgx1eIElnJtR0STRQCelFK672A9VCWEcAUJLHUoekwnSjHhX3oO0tL0LkeIRqfEVPselthWBhKM2kjU6T9I76cQDZUEljrU6RoribQD4Pwm2TAKUddKryCwGI2Q1lzr/cyVewoJ0WBJYKlD3t6Q4qttGOWeQkLUDavVyie3Pc/PgNHD+4qWUdROa5fG/dIuhWioJLDUseyW2oaxZJdsGIWoCyaTiU5+EfQFlNnripZh6aO1y6CT0i6FaKgksNQxQ3dtw+iTJBtGIeqKvdAGQJln7Q8JAUSM1s5hCSk6AVlZdVWWEMKFJLDUscChWmCJOH8ASkt1rkaIhq+4uJiPdq/l74DNw/OKltFpUCAniAIgb5uMRC1EQySBpY61GdWKPHywKBulB4/oXY4QDV5JSQlvHVjNs0Cxp/mKlhEYCIlWbWfitFwpJESDVKvAMmfOHAwGg9MjPDz8svPPmDGj0vwGg4EuXbo4zbd8+XI6d+6MxWKhc+fOfPPNN1f2adxAqzZGEoza55Mh+oWoW8pc+3FYKpyL1AJL4c/SLoVoiGrdw9KlSxfS0tIcj717L9/433jjDad5T5w4QVBQELfddptjnq1btzJ58mSmTp3K7t27mTp1Krfffjvbt2+/sk+kM6MR0oK1DWO2XEIpRJ1S5is7hwWgtLPWLq2J0i6FaIg8av0GD48qe1UuFBAQQEBAgOP5ihUrOH/+PPfcc49j2sKFCxk9ejSzZ88GYPbs2cTFxbFw4UKWLl1a2/LcQkHbbnAGDFWEOSFE7SnLlQcW/0Hd4H8QdqZ8JGqDoQ4rE0LUt1r3sCQmJhIZGUmrVq244447SEpKqvF7P/jgA0aNGkVMTIxj2tatWxkzZozTfGPHjmXLli1VLstms5GTk+P0cBeW3tqeXGCqBBYh6pLBemXnsAC0HNORUkz4lWWjTqTWYVVCCFeoVWDp378/H330EatXr+b9998nPT2dQYMGcfbs2Wrfm5aWxqpVq7j//vudpqenpxMWFuY0LSwsjPT09CqXN3fuXEcPTkBAANHR0bX5KPUqbFT5zdYKkiA/X+dqhGhErqKHpX03C4fpAECGDOwoRINTq8Aybtw4br31Vrp168aoUaNYuXIlAB9++GG1712yZAnNmjVj4sSJlV4zXNQ1q5SqNO1is2fPJjs72/E4ceJEzT9IPes4JIR0wjCiyPt5v97lCNFoGLyuPLCYzXAsQNuZOLtRAosQDc1VXdbs4+NDt27dSExMrHI+pRSLFi1i6tSpmM3OXbrh4eGVelMyMjIq9bpczGKx4O/v7/RwF0FBcNiibRjT18iGUYirYbVa+SCwFxsAs6/fVS0rN1Zrl/Z4aZdCNDRXFVhsNhsJCQlERERUOV9cXBxHjhzhvvvuq/TawIEDWbt2rdO0NWvWMGjQoKspTXdnI+QSSiHqgslkYoDRm+GAp8+V3UuogkdPrV36pUi7FKKhqVVgmTVrFnFxcSQnJ7N9+3YmTZpETk4O06dPB7TDNNOmTav0vg8++ID+/fvTtWvXSq898cQTrFmzhnnz5nHw4EHmzZvHDz/8wJNPPnlln8hNlHbSNozmQ7JhFOJqeZQWAWDyufJDQgBBw7R2GZmdACUlV12XEMJ1ahVYUlNTmTJlCh06dOB3v/sdZrOZbdu2Oa76SUtL4/jx407vyc7OZvny5ZfsXQEYNGgQy5YtY/HixXTv3p0lS5bw+eef079//yv8SO7Bd6C2YQzNkMAixNUoKSlhaeFJ3gaUtdYjMThpNyqGXHzxpATb3sN1U6AQwiUMSimldxF1IScnh4CAALKzs93ifJa92wvoMsAXIwqVlo4hvOpzcoQQl5afn4+vry8A389Zy9iXRl3xspSCnZ4D6Vu2jeS5S2n13B11VaYQ4grV9O+33EuonnS4xpsjtAXkEkoh6orHVR4SMhggPVTr/czdIu1SiIZEAks9MZvhmH/5JZTr9+hcjRCNg4fvld9LqEJRW61deuyXdilEQyKBpR7ltNQ2jKW7ZE9OiLrg6Xt1PSwAXv20dhl0StqlEA2JBJZ6ZOyhbRh95RJKIeqEp//VB5bw0Vq7DC86Bm50Sw8hRNUksNSjwGHdAYg8vx/KynSuRogG6oK241kHh4Q6Dg7mJJEAnP9x31UvTwjhGhJY6lHbMa0pwAurKqI44aje5QjRMNlsjv9a/K4+sPj6wlEvrZfl9A/S+ylEQyGBpR61aGnioKkLAKdWy4ZRiCthtiv+B/wP8AsJqJNlnmuhBZaindIuhWgoJLDUI4MB0ptrG8bszbJhFOJKqMJSbgDGYsK32dWfwwJg76K1S68j0i6FaCgksNSzwvJLKI37ZcMoxJWwZWvD8hdhxVo3eQX/weUn3p7Zq40mJ4RwexJY6pm1r7ZhDDwpgUWIK5F/LpclwGLAaKyb+//EXN+JUkwElJ2n7MSpOlmmEKJ+SWCpZ2Gjym+2VnAECgp0rkaIhif/XB73AI+TT0lJcZ0ss3VnK0cM7QBIWyM7E0I0BBJY6lmHoWFkEIIRRdaWA3qXI0SDU5JbVOfLNJngRDNtZ+JcnAQWIRoCCSz1zM8Pjli1DWP6WtkwClFbJXl1H1gA8lpp7dK+R9qlEA2BBBYXOBupbRgLf5YNoxC1VZpfN4eBLubRU2uX/sekXQrREEhgcYGyztqG0XJYNoxC1FZpfv30sDQfobXLFjkJUFpaL+sQQtQdCSwu4DtI2zCGnZHAIkRtldVTYGk7uhV5+GBRNvLjE+tlHUKIuiOBxQVaXt8FOwaCS05jP31G73KEaFDsBbbqZ7oCIWFGDntqI1Gf/F52JoRwdxJYXKBNdx+SDa0BuYRSiNoy2Ur5Apjj1x+L5ervJXSh0yFa72feNmmXQrg7CSwuYDLB8QBtw5i5QTaMQtSGsaiE24Ahvm3x8PCo02XbOmjt0uOAtEsh3J0EFhfJidE2jGXxsmEUojZUoXYOi92zbntXALzKR6IOTpN2KYS7k8DiIqYe2obRN0U2jELURkl+AV8CG4pTKa3jq3kix5ZfKVSUhMrNq9NlCyHqlgQWFwkeXr5hzNoPdrvO1QjRcJQU5nE78Lf0NdhsdXsCbrtBIaQTBsCZjfvrdNlCiLolgcVF2oxtSxEWfFQ+BfuT9S5HiIajqH6uEgKwWiHJR9uZOP2D9H4K4c4ksLhIaKQHhz06A5Aql1AKUWMGW/2Mw1LhfAstsNh2SrsUwp1JYHGh9JDuAORv3KlzJUI0HIbi+g0s9q5au/Q/LO1SCHcmgcWFzvcaCUDQlv/qXIkQDYexuP4OCQH43XwdAG0yt6FOZ9TruoQQV04CiwvFzryBUkzEZO2h5FCS3uUI0SAYSuo3sAy4vSXxxmswYSfl7f/V67qEEFdOAosL9RkTxFbPoQAkL/yPztUI0TCY6vmQkNUKBztOBKBombRLIdyVBBYXMpkgtfdEAAz/lQ2jEDVhKStmMfD00Ecwm831sg7fuyYC0OrIGsjPr5d1CCGujgQWFwu5/2YAWp/8EXUmU+dqhHB/XqXFzACuv+Z6PD0962Udgx7qRjKxWFUR6R+tqZd1CCGujgQWFxt8Zwy7DT0xYeeYHC8XoloeZdohIZO3td7WERRs4JfoiQCcXbSi3tYjhLhyElhczMsLEsqPlxcuW6FrLUI0BIaSQlYCO1J31fnQ/E7rmTgRgKj4/0E9rkcIcWUksOjA586JAMQeXgMFBfoWI4SbU/ZCbgT++PEf63xo/gv1eWIwmQQTUHqOrP9trrf1CCGujAQWHQx6uDspxOClCjn9yVq9yxHCrXmW1e9lzRVi2niwJehGAE6+vcIl6xRC1JwEFh0ENzews8VEADI/WKFrLUK4O7Oq38uaL1QweiIAzbf8B5Ry2XqFENWTwKKXiuPlv/5XjpcLUQVXBpbOT46hAC/CClIo3L7HZesVQlRPAotO+jx5LWcJIqD0LNmrtuhdjhDuSSmsFLtsdd36e7PZawwAKW+scNl6hRDVk8Cik9i2HmwJ1I6Xp8rxciEurR5Psr0UgwEyB2tjJXmvWeHSdQshqiaBRUf55cfLg39cIcfLhbgEVei6w0EVYmbeSBlGYs7FU3r0mMvXL4S4NAksOur0xBgKsRJekEzRzn16lyOE2ynJs2EG3gL+Pv+tehua/0L9bwxhm8e1AKS8IbfQEMJdSGDRUfeBPvxYcbx84Qp9ixHCDdmyi/AE7sWLxx5/tN6G5r+Qhwcc6zkRALViRb2vTwhRMxJYdGQwQOYg7Xi5dfUKfYsRwg0V52iHhIqw4oLOFYege7R22erEJtTZc65bsRDisiSw6Cz6kQmUYST27K+UJR/Xuxwh3EpxThFlwAYMxMVtpKyszCXrHTK9NXsN3fCgjNT/t9Il6xRCVE0Ci84GTAhhu8dgAFLe/FbnaoRwLyW5RRQBkzjHiBEjKCpyzUm4Pj6wv+1EAPI+XeGSdQohqiaBRWeenpDcfSIAZV+v0LUWIdxNSa7rrxKqYL1jIgCxB7+HwkLd6hBCaCSwuIHge7Xj5a2Pb0SdO69zNUK4j9I8/QLLoJnXcJxovOwFnFm2Trc6hBAaCSxu4Nrpbdhn6IoHZZx8T46XC1FBz8ASGmbg5whtZ+LM+yt0q0MIoZHA4gZ8fWFfm4kA5H4i4z4IUUHPwAJgnzARgIid34KLTvgVQlyaBBY3YZk8EYCYhFXgohMLhXB39gJ928I1TwzlPM0ILDlD7tptutYiRFMngcVNDJzZixNE4W3PJ/MLOV4uBEBZvr6BpV1nTzYH3ADAiX+u0LUWIZo6CSxuIjzCwM/h2vHyjPdW6FuMEG7CXqCNdPuQXw/mz5/vkpFuL5Z73UQAgjZ+I/f8EkJHEljcSOmNEwGI2CHHy4UAsBcWYQZuC7yGZ555xiX3ErpY+8fGUoSF8PyjFMcfcPn6hRAaCSxu5Jonh5FFAIHFGeSt2653OULoThXaALCbrbrV0GuYHz9ZRgKQLDdDFEI3EljcSPsunvzoX368/K0V+hYjhDso0obmTyjLYseOHS4bmv9CRiOk9Z8IgOW7FS5fvxBCI4HFzeSUHy9vtnGFHC8Xokgbmv+J5GX069fPZUPzX6zFwxOwYyD2zA7sJ07qUoMQTZ0EFjfT/rHrKcJCRF4itg1b9C5HCF0ZbO5xif/gW8PZbhoEwIlXluhbjBBNVK0Cy5w5czAYDE6P8PDwKt9js9l44YUXiImJwWKx0KZNGxYtWuR4fcmSJZWWaTAYdNuT0lvv4X6s8L0bgLSn/q5zNULoy10Ci9kMuwf+HoCAj96SsZKE0IFHbd/QpUsXfvjhB8dzk8lU5fy33347p0+f5oMPPqBt27ZkZGRQWlrqNI+/vz+HDh1ymma16neSnZ6MRrA9Ogte/YCWu7+lbP9BTF066l2WELowFrtPMBj4+u0c6/s8MUXHyXz9Y5rPfkDvkoRoUmodWDw8PKrtVanw/fffExcXR1JSEkFBQQDExsZWmq8mPTVNya0vdOS7BTcxvuRbUp5cQOza9/UuSQhduFNg6dHHk3c7/IGHD/2Bsvn/gD/ep+1hCCFcotatLTExkcjISFq1asUdd9xBUlLSZef99ttv6dOnD/Pnz6dFixa0b9+eWbNmUXjRrdrz8vKIiYkhKiqKG2+8kV27dlVbh81mIycnx+nRWPj6wok7ngUgct1HqLR0nSsSQh/GEvcJLAAd/n4/5wgkLOswOZ9+q3c5QjQptQos/fv356OPPmL16tW8//77pKenM2jQIM6ePXvJ+ZOSkti8eTP79u3jm2++YeHChXz11VfMnDnTMU/Hjh1ZsmQJ3377LUuXLsVqtTJ48GASExOrrGXu3LkEBAQ4HtHR0bX5KG7v5vmD2WoYiFkVk/rsm3qXI4QuPNwssAy/0ZdvwrRzWbJfmK9zNUI0LQalrvza2fz8fNq0acOzzz7LU089Ven1MWPG8OOPP5Kenk5AQAAAX3/9NZMmTSI/Px8vL69K77Hb7fTq1YuhQ4fy5puX/0Nts9mw2WyO5zk5OURHR5OdnY2/v/+VfiS38s6YFTyy9hbyPJvhe/Y4+PnpXZIQLnUgcBBts7byQL/JtBrXkeeff16X0W4v9M2/0hn3SCxWbNjWbcZy3WBd6xGiocvJySEgIKDav99XdQDWx8eHbt26XbY3JCIighYtWjjCCkCnTp1QSpGamnrpgoxG+vbtW20Pi8Viwd/f3+nR2Ix84yYO0R7fkizSX/m33uUI4XIepdrQ/PdeN4M5c+boHlYAJjwQzte+0wA49QfpZRHCVa4qsNhsNhISEoiIiLjk64MHD+bUqVPk5eU5ph0+fBij0UhUVNQl36OUIj4+/rLLbEo6dDKyrucsADzffh1KSnSuSAjX8izTDgmZfNznqkEPDyh74mnsGGi1R7uSTwhR/2oVWGbNmkVcXBzJycls376dSZMmkZOTw/Tp0wGYPXs206ZNc8x/5513EhwczD333MOBAwfYtGkTzzzzDPfee6/jcNDLL7/M6tWrSUpKIj4+nvvuu4/4+HgefvjhOvyYDVev16eSThjB+Sc4/+7nepcjhEt5lhVhB47nnmL//v3Y7Xa9SwLgd7M7sMpTu7v68Sf+oXM1QjQNtQosqampTJkyhQ4dOvC73/0Os9nMtm3biImJASAtLY3jx4875vf19WXt2rVkZWXRp08f7rrrLiZMmOB0bkpWVhYPPvggnTp1YsyYMZw8eZJNmzbRr1+/OvqIDduA4Va+jXkcgKK//l2G6xdNitleRCFw1/y76Nq1a6UrDPXi4wOn7tau5Gux/mPUqTSdKxKi8buqk27dSU1P2mmIVn12niF3ReNLPvnLv8fnd2P1LkkIlzhvCsZsP4dv+fO8vDx8fHx0ralCRgYcCb+WQeonjt/1HC0/mat3SUI0SC456Va4xtg7Avk68H4AMp+T4fpF02FR7nVZ84VCQ2H3mGcACPr8X5Cbq3NFQjRuElgaAKMRLM/9gVJMxCSuo2T7r3qXJET9U8qtAwvAqDcmcJAO+JZmk/5XGZFaiPokgaWBmPhEDN9aJwOQ+oT0sogmoLQUE+5xku3ltOtgZEMv7Uo+s1zJJ0S9ksDSQFgskP2g1v3ccvsXqKRknSsSon6pQvfuXanQ+/W7SSOcoIJUzr2zTO9yhGi0JLA0ILe83JN1ptGYsHPsydf1LkeIelWc0zACS7+hVv4bq13JZ3tFruQTor5IYGlAmjWDI7dol1KGrfwALnMPJyEaA1u2FljKMPOHP8xi1qxZeHp66lzVpcW8+jC5+BKRuZf8b9boXY4QjZIElgZm/IKRxNMTL3sBqc+/o3c5QtSbih4WhZUFC/7O3//+d7cYmv9SRt8WyNdBDwCQ+UcZrl+I+iCBpYGJbmlg6xCtl8Xvw7fATQbSEqKuVQSWIqwYDDoXUw2jEbyff5ISPIg5sp6Sbb/oXZIQjY4Elgbo2jduI4UYAmxnyJi/RO9yhKgXJbkVgcVCSkoKKSkpbjM0/6Xc/FhLvvW6A4DUx6WXRYi6JoGlAep2jQffd3oKAMurc+D8eX0LEqIeVASWbKOFVq1a0apVK7cZmv9SzGbIfVi7kq/Vji8o2fiTzhUJ0bhIYGmgur/9EAl0JKAog/QZz+ldjhB1riKwlBjd507N1bnlpe58YrkPgKzJD0Fxsc4VCdF4SGBpoAaNsPDt+P8HQPi371ESt0XnioSoW2X55YHFZNG5kpoLCADP1+eTQQghGfs5+8JrepckRKMhgaUBe+DjoXxmvQeA85MfklE2RaNSmm/T/jU1nB4WgNsfDmJR5wUA+L72Mupoks4VCdE4SGBpwIKCwPP1v3OG5oSe3kfm8wv0LkmIOuPoYfFoOD0sAAYDTFpxNxsM12GxF5F2yyMymJwQdUACSwM36aFgFnep2Jv7i+zNiUbDXlA+cJxHw+phAWjbzsDBJ/5FERYi964m94Mv9C5JiAZPAksDZzDArSumstE4Aqu9kFO/myl7c6JRqAgspQ0wsADcN689/w55HgD7409AVpa+BQnRwElgaQTatDVw6Il/YcNMiz3fk/PBl3qXJMRVqwgsBrMXjzzyCI888ggeHh46V1VzZjP0+vyPHKQDAYWnOTnjeb1LEqJBk8DSSNw7rwP/DpW9OdF4qCItsHhYfHj77bd5++23sVga1vksg0ZYWDlBu5ov4j/vUhy3VeeKhGi4JLA0Ep6e0PuL5zhEe5oVpnNyuuzNiQauUAssdnPDPCRU4b6PhrHMawZGFOdvl6v5hLhSElgakQHDLKy66V0AIr59F1vcNp0rEuIqFFUEFgtnzpzhzJkzqAZ4flazZmB98+9kEkxYxl4ynl+od0lCNEgSWBqZez4awRfWadrenIzNIhowg00LLDZPI6GhoYSGhlJQUKBzVVfm5vua83H3fwDg/9pL2JNS9C1IiAZIAksjExAAln/+g7MEEX56D6dnL9S7JCGuSEVgUdaGfUgItKv5blkxnU3GYVjthZycKFfzCVFbElgaoZvuDfltb+71ObI3JxokQ3H5VUIN7ETby4ltZeDI0+9iw0z03u/IXrRc75KEaFAksDRCBgPc8p8Z/Ggcipe9gNRbHpW9OdHgGMsDC14Nv4elwrS/dWRJmHazUvtjj0N2ts4VCdFwSGBppGJiDRyd9S7FeNJyz0rOz39P75KEqBVTSXkPSyMKLB4e0Pur2RymHYGFaZy66WHZmRCihiSwNGJ3/18n3o38KwB+sx/FtnaTzhUJUXPGEu3mhwavxnFIqEKfa62smrSIEjyI3LSMzGfm6V2SEA2CBJZGzMMDbtz0LF+bJ+OhSrFNuBWVckzvsoSoEY9SrYfF5N14elgqPPTxtbwe+yYAQQueJ//z/+lckRDuTwJLI9e6jYHgFYv4lWvwt2WSMehmyM/XuywhqlURWDx9fZg+fTrTp09vUEPzV8Vqhalbfs/H3g9hRGG4607K9iXoXZYQbk0CSxMwbJw3+/66gtOEEpa2m1Nj75Hj5sLteZYHFu8AP5YsWcKSJUsa3ND8VYmIgC7r3mSzYQjeZbmcG3ITnD+vd1lCuC0JLE3EtD+15OObl1OMJ5E/fcnpJ/5P75KEqJJnWfkhIZ/Gd0ioQq8BZjLf/YpjtCQk6winht4BpaV6lyWEW5LA0oQ88eW1vNn+bQDC3nqRnI//o3NFQlyep7385oe+FvLz88nPz2+QQ/NXZ+KDofz3vv+QjzeR+9Zwatof9S5JCLckgaUJ8fSEGT89wEd+MwHwuOduSnbt07kqIS7NXB5YSjwVvr6++Pr6Ntih+avzyHs9ebvvhwBELn2Ncws/0rkiIdyPBJYmpnlz6BX3OnGmEXiX5ZE1/GY4e1bvsoSoxKK0wGL2b7yHhCoYjfDI+km8G/oiAD5PPUhR3HadqxLCvUhgaYK6XuNJ4ZIvSCaWkJwkUgdPluPmwu04Aotv4znRtiq+vnD91jmsNE/EomwUXH8LKvWk3mUJ4TYksDRR19/dnHWPf0sePkQdWkfq5Kf1LkmI35SW4okWos3+TSOwAMS2NtLsPx+xjy4EFaWRNuAWKCzUuywh3IIElibsvoXdeO/ajwGI+vpNMuYv1rkiITSqyOb4v6df4z8kdKHB1/ux95VvOUsQkSd3cHz8Q3qXJIRbkMDShBkM8Ps1t/D/IucAYHzxBRmfRbiF4pwix/+tTaiHpcKUF1rz2cQvKcVEy40fk7PzsN4lCaE7CSxNnJcXjFn7LMV40rw4jZw9KXqXJARFWeVXCOGB1bdxjG5bW7//8jp+tQ4GIOnjn3SuRgj9SWARtOrsxQFLLwCSP92iczVC/NbDUoQVq9XEpEmTmDRpEiaTSefKXMfDAzI7DAKgeKO0SyGa5q6LqCS9zSA4sJ3C9VuAu/QuRzRxxbnaOSxFWAnxsvLll1/qXJE+rMMHwm4ISZTAIoT0sAgAPIcMBCD4kGwYhf4qeliKDU3rhNuLtbpTa5etCg9QeEruMySaNgksAoDYKdqGsXXeHmyZuTpXI5q6klwtsNiMTTuwxPYN4aipHQBHPtmmczVC6EsCiwCg9dAoThhbYsLOkaU79C5HNHGleeUn3Rqt5OfnYzAYMBgM5Ofn61yZaxkMcCJKO48le5X0foqmTQKLALQNY0qEtmE8t1I2jEJfFYGl2NS0e1gAygZo7dJn71adKxFCXxJYhENJX23D6LVLAovQV1m+FlhKJbAQ8TutXbY9u50ym9xCQzRdEliEQ8jE8g3jma3YS+06VyOasorAUuIhgaX9xM5k448feRz9j9xdXTRdEliEQ8fbupOPN81UFknfHdS7HNGEVQSWMgkseJiNJAYPACDta+n9FE2XBBbh4OntSWKzvgCc/Eo2jEI/9sKKwNL0huW/lPzuWu+naZu0S9F0SWARTnK6aBtGwxbZMAr9qILywGKWHhaAgHFau4xO3SK3+xJNlgQW4cR3jLZhjDwuVyQI/ajyHha7pxWTycT48eMZP358kxqa/0Lt7u6PHQMxZcmc3JmmdzlC6EICi3DSbqp2rLxtyUHS9p3VuRrRZBWVBxazFavVysqVK1m5ciVWa9PscfGJ8OeoVzcAkj6VnQnRNElgEU78WjUnxdIBgMSPZWRNoZPywKIsTTOgXMqZdlrvp22jBBbRNElgEZWkt9E2jIXr5DwWoRObdvNDCSy/MQ/Tbp/R/LC0S9E0SWARlXgM0QJLkNwIUejEYNN6WLBqQ/P7+Pjg4+PT5Ibmv1Cru7R22blwJ+fSbDpXI4TrSWARlbS8o3zDmPcz2WdlZE3heobi3wILQEFBAQUFBTpWpL/gfm04awrBQjEHPv1V73KEcDkJLKKS0KEdyTY2w4cC9i/do3c5ogkyVgQWLzkk5GAwcLxF+Y0Qv5PeT9H01CqwzJkzx3HX1IpHeHh4le+x2Wy88MILxMTEYLFYaNOmDYsWLXKaZ/ny5XTu3BmLxULnzp355ptvav9JRN0xGjkWoV0tdPa/smEUrmcqDyxGCSxOyvqX3whxj7RL0fTUuoelS5cupKWlOR579+6tcv7bb7+ddevW8cEHH3Do0CGWLl1Kx44dHa9v3bqVyZMnM3XqVHbv3s3UqVO5/fbb2b59e+0/jagzxX3Kb4QYLxtG4XqmEi2wGCSwOAkvvxFih7NbKCqUEeRE0+JR6zd4eFTbq1Lh+++/Jy4ujqSkJIKCggCIjY11mmfhwoWMHj2a2bNnAzB79mzi4uJYuHAhS5curW15oo6E3jwI/gPtMrZQXAxms94ViabEVFrew+IjgeVCLW7qTTGeRJDOz/89Rr/bY/UuSQiXqXUPS2JiIpGRkbRq1Yo77riDpKSky8777bff0qdPH+bPn0+LFi1o3749s2bNorCw0DHP1q1bGTNmjNP7xo4dy5Zqhoa32Wzk5OQ4PUTdib61H2UYieEYe78/qXc5oonxKA8sJm8JLBcyeHtxLPAaANKWS++naFpqFVj69+/PRx99xOrVq3n//fdJT09n0KBBnD176RFRk5KS2Lx5M/v27eObb75h4cKFfPXVV8ycOdMxT3p6OmFhYU7vCwsLIz09vcpa5s6dS0BAgOMRHR1dm48iqmHw9+NYQHcAUr+UgaqEa3mWaYHFw8eC0Whk2LBhDBs2DKNRrhPIK78RonG7BBbRtNSq9Y8bN45bb72Vbt26MWrUKFauXAnAhx9+eMn57XY7BoOBTz/9lH79+jF+/Hhee+01lixZ4tTLYjAYnN6nlKo07WKzZ88mOzvb8Thx4kRtPoqogYobIaotEliEa1UEFpOPFS8vLzZu3MjGjRvx8vLSuTL9+ZffCLHliS3Y7ToXI4QLXdXuio+PD926dSMxMfGSr0dERNCiRQsCAgIc0zp16oRSitTUVADCw8Mr9aZkZGRU6nW5mMViwd/f3+kh6lbFjRBbHJcNo3AtRw+LrxwSuljMZG3E26723STsyNO5GiFc56oCi81mIyEhgYiIiEu+PnjwYE6dOkVe3m+N6vDhwxiNRqKiogAYOHAga9eudXrfmjVrGDRo0NWUJupAy/INY4/SXzi8p0jnakRTYrZrv2+efhJYLuYRG8Vpa0tM2Dny2c96lyOEy9QqsMyaNYu4uDiSk5PZvn07kyZNIicnh+nTpwPaYZpp06Y55r/zzjsJDg7mnnvu4cCBA2zatIlnnnmGe++919G1+8QTT7BmzRrmzZvHwYMHmTdvHj/88ANPPvlk3X1KcUXMHVpxzjMMMyUcXvqL3uWIJsSifgss+fn5hISEEBIS0qSH5r9QRtvyGyFukPNYRNNRq8CSmprKlClT6NChA7/73e8wm81s27aNmJgYANLS0jh+/Lhjfl9fX9auXUtWVhZ9+vThrrvuYsKECbz55puOeQYNGsSyZctYvHgx3bt3Z8mSJXz++ef079+/jj6iuGIGA+mttQ1jwQ+yYRSuY1bavXIsAVoPS2ZmJpmZmXqW5FYs5TdCDE6U88tE02FQSjWK0YdycnIICAggOztbzmepQ4ce+Acd/v0Mq70nMjZfRiAWLqAUlF8NdHJXBs3aeePr6wtAXl4ePj4+elbnFgp/3InX0L6cI5C85ExaxsrVU6Lhqunfb/ktF1WKul3rYelRsJWTqY0i2wo3p4p+uxNxRQ+LcOY1oAdFRi+COM+eLw/pXY4QLiGBRVTJZ0gvig1mwjlN/DfJepcjmgBb9m8neEtguQxPT1Ij+wGQJTdCFE2EBBZRNauVk2G9AbkRonCNisBShhGrb63vHtJklPXTej+95X5foomQwCKqVdxbO8HP+qtsGEX9K87RAksRVjw8qx5AsikLu0ULLJ2ytnD+vM7FCOECElhEtUJu1jaM7c9uITtb52JEo3dhYDEYwGg00qdPH/r06SND81+g2fUDAOjEQXasPqdzNULUP2n9olpBN2g9LN3Yy471uTpXIxq7isBSbNDOX/Hy8mLHjh3s2LFDhua/UPPmpPu3B+DU19t0LkaI+ieBRVQvMpIzvrGYsHNiuYysKepXSa4WWGxGOeG2OrndtN5PwzY5XCsaPwksokayO5ffCPEn2TCK+lWapwWWEqNF50rcn//15TdCTN1Ckdw9QzRyElhEjVx4I8TiYp2LEY2aI7CYtB6WgoICYmNjiY2NpaCgQM/S3E7oRK1d9lPb2bmtVOdqhKhfElhEjVRckdDfvpUf4+TWzaL+XBxYlFIcO3aMY8eO0UgG5q4zhs6dyPcMwIcC9n66B/l6RGMmgUXUiKF7NwpNPjQjm8fHJNCrF/z5z7B9O9glv4g6VJavBZZSk5zDUi2jkTNttJPi9/97C7GxMHMmfPcdFBbqW5oQdU0Ci6gZDw9Kemoja67kBu7f9Qh7/rqCMQOyiYiAGTPgq68gJ0ffMkXDZy8oDyyeElhqIuRmLbC8wp+Yd/wOCt9ZxIM3pBIcDBMmwP/7f5CaqnORQtQBCSyixvyfuh88PYnlGI/wL1ZwC2cJZnnGtbT88C/847ZthASVMXIkLFgAu3dL74uovbIC7V5CZR4SWGrCZ/ptEBJCM7K5g89ZxH2kEs3Ows5c978/sOLhVXSIzqdnT3jhBdi4EWy26pYqhPuRuzWL2snNhbg4WL0a1qyBw4edXj5PM35gFD8wijiGca55B64baWDUKBg1CmJj9SlbNBw/3f4Gg798kh+jpjDkxGfk5+fL3ZqrU1ICP/+stck1a1A//4zhgr0FG2Y2cy1rGc1GhnPA2puBQz0d7bJHD8cNsoVwuZr+/ZbAIq5OSgqsXattKH/4AbKynF4+TSibGEocw4hjGEWtu3DdKCOjRsGIEdC8uS5VCze2ecI8rv3fc2xsfQ/Djy6SwHIlzp+H9eu1HYvVq+H4caeX8/BhC4OIYxibGMrRoH5cO9LiCDCtW+tUt2iSJLAI1ysrg507tQ3khg2obdswXDQ4RCbB/MgQ4hjGjwzF3rU7g4Z6MGQIDBkCLVroVLtwGz+Oepkh6+awoePvGZHwDgUFBfTt2xeAHTt24O3trXOFDYxSkJio7VSsW4fatAnDOeeh/Auxso0BjgBzMmoAfYd5O9plp05gkNs6iXoigUXoz2bTuqnj4mDTJtRPP2G4aByNPHz4mX5sZSBbGUh6zAC6Dm/OkCEwdCi0bSsbyqbmx2tnM+SnV1nX4w+MjH9N73IaH7sd9u/X2mVcnBZgMjKcZinBg930cLTLQ4EDaTk0liFDDQwZAtdcAx5yI21RRySwCPdTUgK//OLYUNp/3Iwxr/K9iQ7TzrGhPBw0kObDu9J/kIn+/aF3b5DbyTRum/v+gWt3LuSHfrMZtf1vepfT+CkFhw791i43xmFMO1VptnTC2MYAtjKQeOtAPAf2ofcQb/r3h/79IThYh9pFoyCBRbi/sjI4cAC2boWtWyn7aSumxEOVZsvDh11cw076sMvYh/xOvYkY2p7+A40MGCC9MI3NT91/z+C97/LDkJcZtenPepfT9CilnfNS3i7tW7bCrl0Yy5xH0i3FxD66spM+7KQPZ6J7439td/oMtjBgAHTvDp6eOn0G0aBIYBEN07lzsG0bbNtG2U9bUdu241FQuRcmBz9+pRe/0JuDPn1QvfsQNawNvfoY6d0bIiMlxDRUWzrcw6DDS/hh9DxGrXlWzmFxB4WF8OuvsHUrastWSjdvxfNMWqXZivFkL934hd7s9uhDYZc+BA/rSs9+Znr3hvbt5WokUZkEFtE4lJVp3dU7d6J2/kLxlp2Y9u7Co7jyMJ65+LKbHuymB8l+PSjt2pOgoV3pPsCb3r0hKkpCTEOwrdUUBqQsY91NCxn5nyfkKiF3pBScOKEd4v3lF4q37oSdOzHnnK00azGeHKAzu+nBQXMPCjv0xHtgDzpdG0zv3tChA5hMOnwG4TYksIjGq7QUDh6EnTsp+/kXCjfvxHowHo+SyrerLcNIIu2IpydHfHpQ3LEHXv26EjM4mh49DXToICcPupufo26h38kVrJ/8Ltcte0gCS0NRcSipfOei4MedeMTvxJJ//pKzp9KCeHqS4NmDnNY9MffqSsTQdnTv5UHXriAdaU2HBBbRtJSWaoPYxcdTsnM3+VviMR+Ixzs345KzZ+PPATpz0NiFs5FdoXMXAgZ1oe2QCHr0NBAU5OL6hcMvYePonfE9G2csYfji6RJYGrKKELN7N/Zdu8nbHA97duOfcfSSs9swc4gO7KcL6cFdKe3QBZ9+XYge1poevUxER0svaWMkgUUIgPR02L2bkh3x5Py4G8Pe3QScPozJXnrJ2c/TjH105Zh3Z/KiO2Ls3ImA/h2JGdKSzl2NyK9W/dsdNIIe5zey6ZFlDH17sgSWxignB/buxf5rPNmbdlP26278TuzHUpJ/ydkLsZJAJxI9u5Ad0RF7h0749ulIi2Ft6dzTTGioBJmGTAKLEJdTXAyJidj37ifrp30U7tiP+ch+gs4mYuLSNz8qwIvDtOe4d0dyIjtBx4749dU2mO16+hAQ4OLP0Ijt9x9Il9xt/PTMCgbPv1kCS1Nht2u9Mfv3k7t1Hznb9mM6uJ+g9AOYyyof7gXtSqUkWpPk2ZHzYR0padcJn14dCRvSnrb9gwkLkyDTEEhgEaK2iorg0CEKduzn7OYESvYexOtYAsHnEjGr4su+7SSRHDO343xwO2wx7fHs3I7Afu2IHt6G6HZWuSqilg769KJjwS62zfmeAS+NlcDS1JWVQXIyJfH7yYzbT+HuQ5iPJBB05iDepZWvIKxwjkCSTO3IbNaOwqh2GDu0w793OyKHtaPVNc0wm134GUSVJLAIUVdKSyElhdyfE8jcfBDb7oOYkw7SPDMB/9JLn1AIYMdAqiGaNJ+25Ia0oTS2DZZOrQns3Yaooa0JbtNM9v4u4ai1M21sCez8x0b6PD2MgoICOnfuDMCBAwfksmahUQrS0iiKP0jGpgQKfjmIKfEggacTaF50ssq3nqE5J6ztyApuQ0l0Gzzat8b/mjZEDG5NZK9wjCZpmK4kgUUIVzh7lvz4RE5vTiRvVyIqMRGfU4mE5STiZ8+p8q3nDEGke7Umu3kbSlu21jaa3WMJ6x9LcM9oDFaLiz6Eezlhbk10STLx726j50P99S5HNET5+RQnHCV9cyLZOxMpS0jEmppI83OJNC9Nr/qteHPS0pqswNYUtWiDR7tW+HRtRfM+sYT3j8HUzM9FH6LpkMAihJ6UoiQtk7RNiZzdfoSi/UcxpiThm3GUkNwkQu2nq3y7HQNnPCI55x9LQVgsxMRi6RBLYM8YQvrEYG4T3WjvUXDaI5KwsjT2fRJP17t66F2OaGRUTi4ZW45w+qdECvYmoY4m4Z12lObZRwkvPXHZ89gqnDcGc8Y3lvzmsZRFx2JuH4t/91hC+7TEu1MMckJb7UlgEcKNFZ7JI3VTEmd3JFG47ygkHcXr9DGCclOIKknGm8oD413svGcIWf4tKQyNQUW3xNKuJQFdWxLUsyWm2GgIDW2Qw4qeNwYRqM5z6D8H6XBTB73LEU1IaUExp7YeI2NbEnm7j2JPPIolLYVmWSmE21II5ly1y8gz+XPOtyX5wS2xt2iJZ5uW+HZpSfA1MVjaRmvDcMs9C5xIYBGigbIVKU78eobT21PI3ZtCSWIKphMp+J5NISQ/hWh1HF8uffnnhUoMnmT5tKAgMIrS8ChMMVF4tY+iWddoLG2itKF/w8LcbpjRAoM33hSSvCGFVsNjKCwsZOjQoQBs2rQJr0basyTcW1kZnDqYQ9rWFLLiU7AdSsFwPAXvjBSCc1NoUXac5lQe6fdidgxkWcPJaxZFSVgUhugovNpG4d8lGp8O5e0yMhIsTeeQsAQWIRohpSDjtOL47vNk/nqcvITjlB49junkcbzPHic4/zgt1TEiSMNI9U27DCO5PuEUBrWgLCwSU3QLvNpE4texBaboSG3DGRkJgYGuuT5UKexGE0YUJ39Jp0WvMLlKSDQIWVlwPCGfjF9OkL3vOMWJxzGeOIb1zHECc47Twn6caE5gpqRGy8uzBpMf0IKS0EiMUS2wtI7Er0MLzLGR0KKF1i5DQtxuh+NKSGARogkqK4NTp+BEUgkZe9LJOZCK7cgJSE3FnJGKX3Yq4WWpRJFKJKfwoKxGyy0xWSjwD6ckOAIiIvBsGYF32wg8o7XnRERAeLi2Ab2Kex3Yi4oxeml7lplHsmjeJkACi2jwlIKzZyElyc7p/Zlk7Uul8PAJ7CdS8UxPxScrlRCb1i6jSMWLS487c7Eyg4kC31BsQRGo8Ag8oiLwah2BJTYCQ0T4b20zLAys1nr+lFdOAosQohKl4Px5bXyuY0llnE3IIP/wSYpTTsHJk5gzT+GXc5JwdYoWnCSSUzU6bl/BjgGbb3NKgsNR4eF4tgjDGhuOMTJc22iGh2vn1oSFQXBwpXBTeDoHr3DtpMXcM0X4NbdIYBFNQn5+ebtMUZw5fJ6chJMUJZ9CnTiJ+cxJvLNOEVqqtckWnCSM0zXqRa1Q5NWM4sAwVGg4pqhwrC3D8IgK19pkWJj2CA3VHi4+HCWBRQhxRex2yMjQbsZ74gSkJReRm5hOUXIaZalpmDLS8MpKo3lpGhH89gjhTLVXWFxIGQwU+wVTFhSKITwMz6hQyrz8sHz8bwBKbHY8zQYJLEKg7WxkZWltMjUVUlNKyU7MoDApjdITaRjS07CcTyOwKI1w0h3tMpx0LFx+4MtLKfYOoDRICy+eLcLwiAzFEFYeZiZO1A5J1SEJLEKIelOx8UxNhZMntX/TT5aRk5RJ0bHTqLR0jGfSsWadJlSlE046YZwmnHRCyaA5mVXuHWbjj789G4MBCSxC1EJ+vtYmK9rlqZOKrJQsClNOU3YyHcPpdMznTxNcWrldhpJR7Tk2B/+9mY73Da7Tmmv69/vKDzYLIZosg0E7DzcwELp1q5hqAsLKH90B7ZyazEztvJpTp2DrKe1+lKdPlZF//CwlqachIwPT2QwCS04TSgYhnGF/xGgWymCjQtSajw+0b689NAYgsPzREfhth+PUKUhLg33l/6anKXKOZ1GcmoE9PQNT5ml8CzMcYSaM07QLj9blc4EEFiFEPTKZfjs8fs01Tq8AoeUPbQOak6OFmfR0uKWL83KaN2/uooqFaPwu3OHo4tTWLgw32hhIBQW/tcu0NGg11PX1OqqTQ0JCCCGE0EtN/343vGEwhRBCCNHkSGARQgghhNuTwCKEcGuFhYUMHz6c4cOHU1hY/T2WhBCNk5x0K4Rwa3a7nbi4OMf/hRBNk/SwCCGEEMLtSWARQgghhNuTwCKEEEIItyeBRQghhBBuTwKLEEIIIdyeXCUkhHB73t7eepcghNCZBBYhhFvz8fEhPz9f7zKEEDqTQ0JCCCGEcHsSWIQQQgjh9iSwCCHcWlFRETfccAM33HADRUVFepcjhNCJnMMihHBrZWVlfPfdd47/CyGaJulhEUIIIYTbk8AihBBCCLcngUUIIYQQbq9WgWXOnDkYDAanR3h4+GXn37hxY6X5DQYDBw8edMyzZMmSS84jJ9cJIYQQokKtT7rt0qULP/zwg+O5yWSq9j2HDh3C39/f8TwkJMTpdX9/fw4dOuQ0zWq11rY0IYQQQjRStQ4sHh4eVfaqXEpoaCjNmjW77OvV9dRcis1mw2azOZ7n5OTU6v1CCCGEaDhqfQ5LYmIikZGRtGrVijvuuIOkpKRq33PNNdcQERHByJEj2bBhQ6XX8/LyiImJISoqihtvvJFdu3ZVu8y5c+cSEBDgeERHR9f2owghGgAfHx+UUiil8PHx0bscIYRODEopVdOZV61aRUFBAe3bt+f06dO88sorHDx4kP379xMcHFxp/kOHDrFp0yZ69+6NzWbj448/5t1332Xjxo0MHToUgG3btnHkyBG6detGTk4Ob7zxBt999x27d++mXbt2l63lUj0s0dHRZGdnOx1+EkIIIYT7ysnJISAgoNq/37UKLBfLz8+nTZs2PPvsszz11FM1es+ECRMwGAx8++23l3zdbrfTq1cvhg4dyptvvlnjWmr6gYUQQgjhPmr69/uqLmv28fGhW7duJCYm1vg9AwYMqHJ+o9FI3759a7VMIYQQQjRuVxVYbDYbCQkJRERE1Pg9u3btqnJ+pRTx8fG1WqYQQgghGrdaXSU0a9YsJkyYQMuWLcnIyOCVV14hJyeH6dOnAzB79mxOnjzJRx99BMDChQuJjY2lS5cuFBcX88knn7B8+XKWL1/uWObLL7/MgAEDaNeuHTk5Obz55pvEx8fz9ttv1+HHFEIIIURDVqvAkpqaypQpU8jMzCQkJIQBAwawbds2YmJiAEhLS+P48eOO+YuLi5k1axYnT57Ey8uLLl26sHLlSsaPH++YJysriwcffJD09HQCAgK45ppr2LRpE/369aujjyiEEEKIhu6qTrp1J3LSrRBCCNHwuOSkWyGEEEIIV5DAIoQQQgi3J4FFCCGEEG5PAosQQggh3J4EFiGEEEK4PQksQgghhHB7EliEEEII4fYksAghhBDC7dVqpFt3VjH+XU5Ojs6VCCGEEKKmKv5uVzeObaMJLLm5uQBER0frXIkQQgghais3N5eAgIDLvt5ohua32+2cOnUKPz8/DAZDnS03JyeH6OhoTpw4IUP+60C+f33J968v+f71Jd+/ayilyM3NJTIyEqPx8meqNJoeFqPRSFRUVL0t39/fX35hdSTfv77k+9eXfP/6ku+//lXVs1JBTroVQgghhNuTwCKEEEIItyeBpRoWi4WXXnoJi8WidylNknz/+pLvX1/y/etLvn/30mhOuhVCCCFE4yU9LEIIIYRwexJYhBBCCOH2JLAIIYQQwu1JYBFCCCGE25PAIoQQQgi3J4GlGu+88w6tWrXCarXSu3dvfvzxR71LapQ2bdrEhAkTiIyMxGAwsGLFCqfXlVLMmTOHyMhIvLy8GD58OPv379en2EZm7ty59O3bFz8/P0JDQ5k4cSKHDh1ymke+//rzr3/9i+7duztGUx04cCCrVq1yvC7fvWvNnTsXg8HAk08+6ZgmPwP3IIGlCp9//jlPPvkkL7zwArt27WLIkCGMGzeO48eP611ao5Ofn0+PHj345z//ecnX58+fz2uvvcY///lPduzYQXh4OKNHj3bc9FJcubi4OGbOnMm2bdtYu3YtpaWljBkzhvz8fMc88v3Xn6ioKF599VV27tzJzp07ue6667j55psdfxDlu3edHTt28N5779G9e3en6fIzcBNKXFa/fv3Uww8/7DStY8eO6rnnntOpoqYBUN98843jud1uV+Hh4erVV191TCsqKlIBAQHq3Xff1aHCxi0jI0MBKi4uTikl378eAgMD1b///W/57l0oNzdXtWvXTq1du1YNGzZMPfHEE0op+f13J9LDchnFxcX88ssvjBkzxmn6mDFj2LJli05VNU3Jycmkp6c7/SwsFgvDhg2Tn0U9yM7OBiAoKAiQ79+VysrKWLZsGfn5+QwcOFC+exeaOXMmN9xwA6NGjXKaLj8D99Fo7tZc1zIzMykrKyMsLMxpelhYGOnp6TpV1TRVfN+X+lkcO3ZMj5IaLaUUTz31FNdeey1du3YF5Pt3hb179zJw4ECKiorw9fXlm2++oXPnzo4/iPLd169ly5bx66+/smPHjkqvye+/+5DAUg2DweD0XClVaZpwDflZ1L9HH32UPXv2sHnz5kqvyfdffzp06EB8fDxZWVksX76c6dOnExcX53hdvvv6c+LECZ544gnWrFmD1Wq97HzyM9CfHBK6jObNm2MymSr1pmRkZFRK2qJ+hYeHA8jPop499thjfPvtt2zYsIGoqCjHdPn+65/ZbKZt27b06dOHuXPn0qNHD9544w357l3gl19+ISMjg969e+Ph4YGHhwdxcXG8+eabeHh4OL5n+RnoTwLLZZjNZnr37s3atWudpq9du5ZBgwbpVFXT1KpVK8LDw51+FsXFxcTFxcnPog4opXj00Uf5+uuvWb9+Pa1atXJ6Xb5/11NKYbPZ5Lt3gZEjR7J3717i4+Mdjz59+nDXXXcRHx9P69at5WfgJuSQUBWeeuoppk6dSp8+fRg4cCDvvfcex48f5+GHH9a7tEYnLy+PI0eOOJ4nJycTHx9PUFAQLVu25Mknn+Rvf/sb7dq1o127dvztb3/D29ubO++8U8eqG4eZM2fy2Wef8Z///Ac/Pz/HnmRAQABeXl6OMSnk+68fzz//POPGjSM6Oprc3FyWLVvGxo0b+f777+W7dwE/Pz/H+VoVfHx8CA4OdkyXn4Gb0O8CpYbh7bffVjExMcpsNqtevXo5LvUUdWvDhg0KqPSYPn26Ukq7tPCll15S4eHhymKxqKFDh6q9e/fqW3QjcanvHVCLFy92zCPff/259957HduYkJAQNXLkSLVmzRrH6/Ldu96FlzUrJT8Dd2FQSimdspIQQgghRI3IOSxCCCGEcHsSWIQQQgjh9iSwCCGEEMLtSWARQgghhNuTwCKEEEIItyeBRQghhBBuTwKLEEIIIdyeBBYhhBBCuD0JLEIIIYRwexJYhBBCCOH2JLAIIYQQwu39f3uS9wCdrFeVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 5.568030221395926\n",
      "Final validation loss: 5.568508148193359\n"
     ]
    }
   ],
   "source": [
    "train_loss_3 = [5.795922756195068, 5.716060638427734, 5.675954341888428, 5.652993679046631, 5.637449741363525, 5.626087188720703, 5.617416858673096, 5.5870568954752615, 5.5849507165688195, 5.58306241227453, 5.581355651859658, 5.57980357082138, 5.5783844819704225, 5.577080461326141, 5.575876704246722, 5.574760838448565, 5.573722521347672, 5.572752980676263, 5.571844811146291, 5.570991626421145, 5.570187989824164, 5.569429223394324, 5.568711155330105, 5.568030221395926, 5.795922756195068, 5.716060638427734, 5.675954341888428, 5.652993679046631, 5.637449741363525, 5.626087188720703, 5.617416858673096, 5.5870568954752615, 5.5849507165688195, 5.58306241227453, 5.581355651859658, 5.57980357082138, 5.5783844819704225, 5.577080461326141, 5.575876704246722, 5.574760838448565, 5.573722521347672, 5.572752980676263, 5.571844811146291, 5.570991626421145, 5.570187989824164, 5.569429223394324, 5.568711155330105, 5.568030221395926]\n",
    "valid_loss_3 = [5.799070835113525, 5.7169508934021, 5.676379680633545, 5.653461933135986, 5.637903690338135, 5.626455783843994, 5.617713451385498, 5.586125373840332, 5.584168910980225, 5.58241605758667, 5.58082914352417, 5.579389572143555, 5.578066825866699, 5.576859474182129, 5.575737476348877, 5.574705123901367, 5.573742389678955, 5.572847843170166, 5.572007656097412, 5.571223735809326, 5.570484638214111, 5.569790363311768, 5.569129943847656, 5.56850814819336, 5.799070835113525, 5.7169508934021, 5.676379680633545, 5.653461933135986, 5.637903690338135, 5.626455783843994, 5.617713451385498, 5.586125373840332, 5.584168910980225, 5.58241605758667, 5.58082914352417, 5.579389572143555, 5.578066825866699, 5.576859474182129, 5.575737476348877, 5.574705123901367, 5.573742389678955, 5.572847843170166, 5.572007656097412, 5.571223735809326, 5.570484638214111, 5.569790363311768, 5.569129943847656, 5.568508148193359]\n",
    "\n",
    "plt.plot(train_loss_3, label='Training loss', color='blue')\n",
    "plt.plot(valid_loss_3, label=\"Validation loss\", color='red')\n",
    "plt.vlines(23.5,5.55,5.80, color='black', label='Computer Crash', linestyle='--')\n",
    "\n",
    "plt.title('Loss for Model 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Final training loss:\", train_loss_3[-1])\n",
    "print(\"Final validation loss:\", valid_loss_3[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76faad-799f-419e-a8d8-186c3c368528",
   "metadata": {},
   "source": [
    "Okay! So this makes me a bit sad, since it's clear from the figure I messed up training somehow when my computer crashed. Ignoring the obvious issue around epoch 23, we can see that the model is improving! It learns quickly then slowly tapers off. However, the loss function is pretty high overall, ranging between `5.80` and `5.50` over all epochs. I'm not sure how much more training will help, but this code ran for 2 days as-is and I can't spare any more time to run it. What makes me happy, however, is that the training and validation loss are nearly identical! This shows that the simple head I have is pretty good at its job. Let's see how it performs on some data! I replace the `<pad>` values with `_` so that the RoBERTa decoder doesn't throw errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "39e2cfd7-bc6d-4af0-861d-b09c1ece4620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset point #0\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "Which NFL team represented the AFC at Super Bowl 50?Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50._____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([49])\n",
      "True end: tensor([51])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Denver Broncos\n",
      "\n",
      "Model answer:\n",
      "Denver Broncos\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #32\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What year was the Carolina Panthers franchise founded?The Panthers finished the regular season with a 15–1 record, and quarterback Cam Newton was named the NFL Most Valuable Player (MVP). They defeated the Arizona Cardinals 49–15 in the NFC Championship Game and advanced to their second Super Bowl appearance since the franchise was founded in 1995. The Broncos finished the regular season with a 12–4 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 20–18 in the AFC Championship Game. They joined the Patriots, Dallas Cowboys, and Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([69])\n",
      "True end: tensor([70])\n",
      "\n",
      "\n",
      "True answer:\n",
      "1995\n",
      "\n",
      "Model answer:\n",
      " in the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #64\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "How many fumbles did Von Miller force?The Broncos took an early lead in Super Bowl 50 and never trailed. Newton was limited by Denver's defense, which sacked him seven times and forced him into three turnovers, including a fumble which they recovered for a touchdown. Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2½ sacks, and two forced fumbles._____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([77])\n",
      "True end: tensor([78])\n",
      "\n",
      "\n",
      "True answer:\n",
      "two\n",
      "\n",
      "Model answer:\n",
      " which they\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #96\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What band headlined half-time during Super Bowl 50?CBS broadcast Super Bowl 50 in the U.S., and charged an average of $5 million for a 30-second commercial during the game. The Super Bowl 50 halftime show was headlined by the British rock group Coldplay with special guest performers Beyoncé and Bruno Mars, who headlined the Super Bowl XLVII and Super Bowl XLVIII halftime shows, respectively. It was the third-most watched U.S. broadcast ever.__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([57])\n",
      "True end: tensor([59])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Coldplay\n",
      "\n",
      "Model answer:\n",
      " was headlined\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #128\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What is the name of the stadium in Miami that was considered?The league eventually narrowed the bids to three sites: New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium, and the San Francisco Bay Area's Levi's Stadium._________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([39])\n",
      "True end: tensor([42])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Sun Life Stadium\n",
      "\n",
      "Model answer:\n",
      "'s Levi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #160\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "Where did the spring meetings of the NFL owners take place?On May 21, 2013, NFL owners at their spring meetings in Boston voted and awarded the game to Levi's Stadium. The $1.2 billion stadium opened in 2014. It is the first Super Bowl held in the San Francisco Bay Area since Super Bowl XIX in 1985, and the first in California since Super Bowl XXXVII took place in San Diego in 2003._____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([29])\n",
      "True end: tensor([30])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Boston\n",
      "\n",
      "Model answer:\n",
      " 2014.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #192\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "How many teams have had a 15-1 record for the regular season?For the third straight season, the number one seeds from both conferences met in the Super Bowl. The Carolina Panthers became one of only ten teams to have completed a regular season with only one loss, and one of only six teams to have acquired a 15–1 record, while the Denver Broncos became one of four teams to have made eight appearances in the Super Bowl. The Broncos made their second Super Bowl appearance in three years, having reached Super Bowl XLVIII, while the Panthers made their second Super Bowl appearance in franchise history, their other appearance being Super Bowl XXXVIII. Coincidentally, both teams were coached by John Fox in their last Super Bowl appearance prior to Super Bowl 50._________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([62])\n",
      "True end: tensor([63])\n",
      "\n",
      "\n",
      "True answer:\n",
      "six\n",
      "\n",
      "Model answer:\n",
      " a regular\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #224\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What position does Jerricho Cotchery play?The Panthers offense, which led the NFL in scoring (500 points), was loaded with talent, boasting six Pro Bowl selections. Pro Bowl quarterback Cam Newton had one of his best seasons, throwing for 3,837 yards and rushing for 636, while recording a career-high and league-leading 45 total touchdowns (35 passing, 10 rushing), a career-low 10 interceptions, and a career-best quarterback rating of 99.4. Newton's leading receivers were tight end Greg Olsen, who caught a career-high 77 passes for 1,104 yards and seven touchdowns, and wide receiver Ted Ginn, Jr., who caught 44 passes for 739 yards and 10 touchdowns; Ginn also rushed for 60 yards and returned 27 punts for 277 yards. Other key receivers included veteran Jerricho Cotchery (39 receptions for 485 yards), rookie Devin Funchess (31 receptions for 473 yards and five touchdowns), and second-year receiver Corey Brown (31 receptions for 447 yards). The Panthers backfield featured Pro Bowl running back Jonathan Stewart, who led the team with 989 rushing yards and six touchdowns in 13 games, along with Pro Bowl fullback Mike Tolbert, who rushed for 256 yards and caught 18 passes for another 154 yards. Carolina's offensive line also featured two Pro Bowl selections: center Ryan Kalil and guard Trai Turner.______________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([106])\n",
      "True end: tensor([109])\n",
      "\n",
      "\n",
      "True answer:\n",
      "receivers\n",
      "\n",
      "Model answer:\n",
      ", throwing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #256\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What is the name of the Bronco's head coach, who was hired after John Fox? Following their loss in the divisional round of the previous season's playoffs, the Denver Broncos underwent numerous coaching changes, including a mutual parting with head coach John Fox (who had won four divisional championships in his four years as Broncos head coach), and the hiring of Gary Kubiak as the new head coach. Under Kubiak, the Broncos planned to install a run-oriented offense with zone blocking to blend in with quarterback Peyton Manning's shotgun passing skills, but struggled with numerous changes and injuries to the offensive line, as well as Manning having his worst statistical season since his rookie year with the Indianapolis Colts in 1998, due to a plantar fasciitis injury in his heel that he had suffered since the summer, and the simple fact that Manning was getting old, as he turned 39 in the 2015 off-season. Although the team had a 7–0 start, Manning led the NFL in interceptions. In week 10, Manning suffered a partial tear of the plantar fasciitis in his left foot. He set the NFL's all-time record for career passing yards in this game, but was benched after throwing four interceptions in favor of backup quarterback Brock Osweiler, who took over as the starter for most of the remainder of the regular season. Osweiler was injured, however, leading to Manning's return during the Week 17 regular season finale, where the Broncos were losing 13–7 against the 4–11 San Diego Chargers, resulting in Manning re-claiming the starting quarterback position for the playoffs by leading the team to a key 27–20 win that enabled the team to clinch the number one overall AFC seed. Under defensive coordinator Wade Phillips, the Broncos' defense ranked number one in total yards allowed, passing yards allowed and sacks, and like the previous three seasons, the team has continued to set numerous individual, league and franchise records. With the defense carrying the team despite the issues with the offense, the Broncos finished the regular season with a 12–4 record and earned home-field advantage throughout the AFC playoffs.___________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([77])\n",
      "True end: tensor([80])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Gary Kubiak\n",
      "\n",
      "Model answer:\n",
      " parting with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #288\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What was Ronnie Hillman's average yards per carry in 2015?Manning finished the year with a career-low 67.9 passer rating, throwing for 2,249 yards and nine touchdowns, with 17 interceptions. In contrast, Osweiler threw for 1,967 yards, 10 touchdowns and six interceptions for a rating of 86.4. Veteran receiver Demaryius Thomas led the team with 105 receptions for 1,304 yards and six touchdowns, while Emmanuel Sanders caught 76 passes for 1,135 yards and six scores, while adding another 106 yards returning punts. Tight end Owen Daniels was also a big element of the passing game with 46 receptions for 517 yards. Running back C. J. Anderson was the team's leading rusher 863 yards and seven touchdowns, while also catching 25 passes for 183 yards. Running back Ronnie Hillman also made a big impact with 720 yards, five touchdowns, 24 receptions, and a 4.7 yards per carry average. Overall, the offense ranked 19th in scoring with 355 points and did not have any Pro Bowl selections.___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([191])\n",
      "True end: tensor([194])\n",
      "\n",
      "\n",
      "True answer:\n",
      "4.7\n",
      "\n",
      "Model answer:\n",
      " Oswe\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #320\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "Who won Super Bowl XLIX?The Broncos defeated the Pittsburgh Steelers in the divisional round, 23–16, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20–18, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games._____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([50])\n",
      "True end: tensor([53])\n",
      "\n",
      "\n",
      "True answer:\n",
      "New England Patriots\n",
      "\n",
      "Model answer:\n",
      " New\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #352\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "Who is the General Manager for the Broncos?Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager.______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([56])\n",
      "True end: tensor([59])\n",
      "\n",
      "\n",
      "True answer:\n",
      "John Elway\n",
      "\n",
      "Model answer:\n",
      " The past\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #384\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "Who did Kubiak take the place of after Super Bowl XXIV?With Rivera having been a linebacker with the Chicago Bears in Super Bowl XX, and Kubiak replacing Elway at the end of the Broncos' defeats in Super Bowls XXI and XXIV, this will be the first Super Bowl in which both head coaches played in the game themselves.____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([37])\n",
      "True end: tensor([39])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Elway\n",
      "\n",
      "Model answer:\n",
      " Bowls\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #416\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "At what university's facility did the Panthers practice?The Panthers used the San Jose State practice facility and stayed at the San Jose Marriott. The Broncos practiced at Stanford University and stayed at the Santa Clara Marriott.__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([18])\n",
      "True end: tensor([21])\n",
      "\n",
      "\n",
      "True answer:\n",
      "San Jose State\n",
      "\n",
      "Model answer:\n",
      "__\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #448\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What color was featured in promotions related to Super Bowl 50?Various gold-themed promotions and initiatives were held throughout the 2015 NFL season to tie into the \"Golden Super Bowl\"; gold-tinted logos were implemented across the NFL's properties and painted on fields, the numbering of the 50-yard line on fields was colored gold, and beginning on week 7, all sideline jackets and hats featured gold-trimmed logos. Gold footballs were given to each high school that has had a player or coach appear in the Super Bowl, and \"homecoming\" events were also held by Super Bowl-winning teams at games._____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([17])\n",
      "True end: tensor([18])\n",
      "\n",
      "\n",
      "True answer:\n",
      "gold\n",
      "\n",
      "Model answer:\n",
      " NFL's\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #480\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "Where was a beer, wine and food festival held at prior to the Super Bowl?In addition, there are $2 million worth of other ancillary events, including a week-long event at the Santa Clara Convention Center, a beer, wine and food festival at Bellomy Field at Santa Clara University, and a pep rally. A professional fundraiser will aid in finding business sponsors and individual donors, but still may need the city council to help fund the event. Additional funding will be provided by the city council, which has announced plans to set aside seed funding for the event.______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([58])\n",
      "True end: tensor([61])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Bellomy Field\n",
      "\n",
      "Model answer:\n",
      " a beer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #512\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What is the Super Bowl program called that gives local companies business opportunities for the Super Bowl?For the first time, the Super Bowl 50 Host Committee and the NFL have openly sought disabled veteran and lesbian, gay, bisexual and transgender-owned businesses in Business Connect, the Super Bowl program that provides local companies with contracting opportunities in and around the Super Bowl. The host committee has already raised over $40 million through sponsors including Apple, Google, Yahoo!, Intel, Gap, Chevron, and Dignity Health._______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([53])\n",
      "True end: tensor([55])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Business Connect\n",
      "\n",
      "Model answer:\n",
      "owned businesses\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #544\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "Which television network aired the Super Bowl?In the United States, the game was televised by CBS, as part of a cycle between the three main broadcast television partners of the NFL. The network's lead broadcast team of Jim Nantz and Phil Simms called the contest, with Tracy Wolfson and Evan Washburn on the sidelines. CBS introduced new features during the telecast, including pylon cameras and microphones along with EyeVision 360—an array of 36 cameras along the upper deck that can be used to provide a 360-degree view of plays and \"bullet time\" effects. (An earlier version of EyeVision was last used in Super Bowl XXXV; for Super Bowl 50, the cameras were upgraded to 5K resolution.)________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([22])\n",
      "True end: tensor([23])\n",
      "\n",
      "\n",
      "True answer:\n",
      "CBS\n",
      "\n",
      "Model answer:\n",
      "antz and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #576\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What talk show followed immediately after Super Bowl 50 on CBS?As opposed to broadcasts of primetime series, CBS broadcast special episodes of its late night talk shows as its lead-out programs for Super Bowl 50, beginning with a special episode of The Late Show with Stephen Colbert following the game. Following a break for late local programming, CBS also aired a special episode of The Late Late Show with James Corden._________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([52])\n",
      "True end: tensor([58])\n",
      "\n",
      "\n",
      "True answer:\n",
      "The Late Show with Stephen Colbert\n",
      "\n",
      "Model answer:\n",
      " episode of\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #608\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What Universal trailer debuted during the Super Bowl?20th Century Fox, Lionsgate, Paramount Pictures, Universal Studios and Walt Disney Studios paid for movie trailers to be aired during the Super Bowl. Fox paid for Deadpool, X-Men: Apocalypse, Independence Day: Resurgence and Eddie the Eagle, Lionsgate paid for Gods of Egypt, Paramount paid for Teenage Mutant Ninja Turtles: Out of the Shadows and 10 Cloverfield Lane, Universal paid for The Secret Life of Pets and the debut trailer for Jason Bourne and Disney paid for Captain America: Civil War, The Jungle Book and Alice Through the Looking Glass.[citation needed]____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([104])\n",
      "True end: tensor([107])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Jason Bourne\n",
      "\n",
      "Model answer:\n",
      ": Apocalypse\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #640\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "Who handled the color commentary for Denver's radio stations?The flagship stations of each station in the markets of each team will carry their local play-by-play calls. In Denver, KOA (850 AM) and KRFX (103.5 FM) will carry the game, with Dave Logan on play-by-play and Ed McCaffrey on color commentary. In North Carolina, WBT (1110 AM) will carry the game, with Mick Mixon on play-by-play and Eugene Robinson and Jim Szoke on color commentary. WBT will also simulcast the game on its sister station WBT-FM (99.3 FM), which is based in Chester, South Carolina. As KOA and WBT are both clear-channel stations, the local broadcasts will be audible over much of the western United States after sunset (for Denver) and the eastern United States throughout the game (for Carolina). In accordance with contractual rules, the rest of the stations in the Broncos and Panthers radio networks will either carry the Westwood One feed or not carry the game at all.______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([71])\n",
      "True end: tensor([75])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Ed McCaffrey\n",
      "\n",
      "Model answer:\n",
      " (103\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #672\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "Who did the Super Bowl 50 National Anthem?Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation.________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([22])\n",
      "True end: tensor([24])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Lady Gaga\n",
      "\n",
      "Model answer:\n",
      "__\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #704\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "Which player recovered possession of the ball in the end zone?After each team punted, Panthers quarterback Cam Newton appeared to complete a 24-yard pass Jerricho Cotchery, but the call was ruled an incompletion and upheld after a replay challenge. CBS analyst and retired referee Mike Carey stated he disagreed with the call and felt the review clearly showed the pass was complete. A few plays later, on 3rd-and-10 from the 15-yard line, linebacker Von Miller knocked the ball out of Newton's hands while sacking him, and Malik Jackson recovered it in the end zone for a Broncos touchdown, giving the team a 10–0 lead. This was the first fumble return touchdown in a Super Bowl since Super Bowl XXVIII at the end of the 1993 season.____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([115])\n",
      "True end: tensor([118])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Malik Jackson\n",
      "\n",
      "Model answer:\n",
      " after a\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #736\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "Which fullback fumbled the ball after a Darian Stewart tackle?On Carolina's next possession fullback Mike Tolbert lost a fumble while being tackled by safety Darian Stewart, which linebacker Danny Trevathan recovered on the Broncos 40-yard line. However, the Panthers soon took the ball back when defensive end Kony Ealy tipped a Manning pass to himself and then intercepted it, returning the ball 19 yards to the Panthers 39-yard line with 1:55 left on the clock. The Panthers could not gain any yards with their possession and had to punt. After a Denver punt, Carolina drove to the Broncos 45-yard line. But with 11 seconds left, Newton was sacked by DeMarcus Ware as time expired in the half.________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([23])\n",
      "True end: tensor([26])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Mike Tolbert\n",
      "\n",
      "Model answer:\n",
      "-yard\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #768\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What Panther defender was called for holding on third down?With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24–10 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one.________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([150])\n",
      "True end: tensor([152])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Josh Norman\n",
      "\n",
      "Model answer:\n",
      "-and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #800\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "Who is the only quarterback to win a Super Bowl with two teams?Super Bowl 50 featured numerous records from individuals and teams. Denver won despite being massively outgained in total yards (315 to 194) and first downs (21 to 11). Their 194 yards and 11 first downs were both the lowest totals ever by a Super Bowl winning team. The previous record was 244 yards by the Baltimore Ravens in Super Bowl XXXV. Only seven other teams had ever gained less than 200 yards in a Super Bowl, and all of them had lost. The Broncos' seven sacks tied a Super Bowl record set by the Chicago Bears in Super Bowl XX. Kony Ealy tied a Super Bowl record with three sacks. Jordan Norwood's 61-yard punt return set a new record, surpassing the old record of 45 yards set by John Taylor in Super Bowl XXIII. Denver was just 1-of-14 on third down, while Carolina was barely better at 3-of-15. The two teams' combined third down conversion percentage of 13.8 was a Super Bowl low. Manning and Newton had quarterback passer ratings of 56.6 and 55.4, respectively, and their added total of 112 is a record lowest aggregate passer rating for a Super Bowl. Manning became the oldest quarterback ever to win a Super Bowl at age 39, and the first quarterback ever to win a Super Bowl with two different teams, while Gary Kubiak became the first head coach to win a Super Bowl with the same franchise he went to the Super Bowl with as a player.___________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([220])\n",
      "True end: tensor([222])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Manning\n",
      "\n",
      "Model answer:\n",
      " to 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #832\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What is the largest medical school in Poland?The University of Warsaw was established in 1816, when the partitions of Poland separated Warsaw from the oldest and most influential Polish academic center, in Kraków. Warsaw University of Technology is the second academic school of technology in the country, and one of the largest in East-Central Europe, employing 2,000 professors. Other institutions for higher education include the Medical University of Warsaw, the largest medical school in Poland and one of the most prestigious, the National Defence University, highest military academic institution in Poland, the Fryderyk Chopin University of Music the oldest and largest music school in Poland, and one of the largest in Europe, the Warsaw School of Economics, the oldest and most renowned economic university in the country, and the Warsaw University of Life Sciences the largest agricultural university founded in 1818._________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([85])\n",
      "True end: tensor([89])\n",
      "\n",
      "\n",
      "True answer:\n",
      "Medical University of Warsaw\n",
      "\n",
      "Model answer:\n",
      " is the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #864\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What does the world's first Museum of Posters have one of the largest collections of in the world?As interesting examples of expositions the most notable are: the world's first Museum of Posters boasting one of the largest collections of art posters in the world, Museum of Hunting and Riding and the Railway Museum. From among Warsaw's 60 museums, the most prestigious ones are National Museum with a collection of works whose origin ranges in time from antiquity till the present epoch as well as one of the best collections of paintings in the country including some paintings from Adolf Hitler's private collection, and Museum of the Polish Army whose set portrays the history of arms.________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([52])\n",
      "True end: tensor([54])\n",
      "\n",
      "\n",
      "True answer:\n",
      "art posters\n",
      "\n",
      "Model answer:\n",
      " collections of\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #896\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What did Lempicka represent better than anyone else?Tamara de Lempicka was a famous artist born in Warsaw. She was born Maria Górska in Warsaw to wealthy parents and in 1916 married a Polish lawyer Tadeusz Łempicki. Better than anyone else she represented the Art Deco style in painting and art. Nathan Alterman, the Israeli poet, was born in Warsaw, as was Moshe Vilenski, the Israeli composer, lyricist, and pianist, who studied music at the Warsaw Conservatory. Warsaw was the beloved city of Isaac Bashevis Singer, which he described in many of his novels: Warsaw has just now been destroyed. No one will ever see the Warsaw I knew. Let me just write about it. Let this Warsaw not disappear forever, he commented.____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([65])\n",
      "True end: tensor([70])\n",
      "\n",
      "\n",
      "True answer:\n",
      "the Art Deco style\n",
      "\n",
      "Model answer:\n",
      "adeus\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset point #928\n",
      "single element size:      torch.Size([1, 32, 512])\n",
      "\n",
      "From BPE encoding:\n",
      "What year did King Sigismund III Vasa move his court to Warsaw?In 1529, Warsaw for the first time became the seat of the General Sejm, permanent from 1569. In 1573 the city gave its name to the Warsaw Confederation, formally establishing religious freedom in the Polish–Lithuanian Commonwealth. Due to its central location between the Commonwealth's capitals of Kraków and Vilnius, Warsaw became the capital of the Commonwealth and the Crown of the Kingdom of Poland when King Sigismund III Vasa moved his court from Kraków to Warsaw in 1596. In the following years the town expanded towards the suburbs. Several private independent districts were established, the property of aristocrats and the gentry, which were ruled by their own laws. Three times between 1655–1658 the city was under siege and three times it was taken and pillaged by the Swedish, Brandenburgian and Transylvanian forces._______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Model start: tensor(49)\n",
      "Model end: tensor(51)\n",
      "\n",
      "True start: tensor([125])\n",
      "True end: tensor([127])\n",
      "\n",
      "\n",
      "True answer:\n",
      "1596\n",
      "\n",
      "Model answer:\n",
      " its name\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_model():\n",
    "    for batch_idx in range(30):\n",
    "        print(f\"Dataset point #{batch_idx*32}\")\n",
    "        squad_bpes = load_object(f\"Validation_32/validation_squad_bpes\")\n",
    "        validation_ans_idxs = load_object(f\"Validation_32/validation_ans_idxs\")\n",
    "        validation_ans_lens = load_object(f\"Validation_32/validation_ans_lens\")\n",
    "\n",
    "        squad_bpe = squad_bpes[batch_idx].unsqueeze(0)\n",
    "        ans_idx = validation_ans_idxs[batch_idx][0].unsqueeze(0)\n",
    "        ans_len = validation_ans_lens[batch_idx][0].unsqueeze(0)\n",
    "        print(\"single element size:     \", squad_bpe.shape)\n",
    "\n",
    "        print(\"\\nFrom BPE encoding:\")\n",
    "        for i in range(len(squad_bpe[0][0])):\n",
    "            if squad_bpe[0][0][i] == 1:\n",
    "                squad_bpe[0][0][i] = 1215\n",
    "        print(roberta.decode(squad_bpe[0][0]))\n",
    "        print()\n",
    "\n",
    "        model = SquadModel(512, 768) # N=512, H=768\n",
    "        model.to('cuda') # Use GPU\n",
    "        model.load_state_dict(torch.load('models/saved_model'))\n",
    "\n",
    "        out = model.forward(squad_bpes[0])\n",
    "        model_start_idx = torch.argmax(out[0][0].to('cpu'))\n",
    "        model_end_idx = torch.argmax(out[1][0].to('cpu'))\n",
    "\n",
    "        print(\"Model start:\", model_start_idx)\n",
    "        print(\"Model end:\", model_end_idx)\n",
    "        print()\n",
    "        print(\"True start:\", ans_idx)\n",
    "        print(\"True end:\", ans_idx + ans_len)\n",
    "        print()\n",
    "\n",
    "        print(\"\\nTrue answer:\")\n",
    "        print(roberta.decode(squad_bpe[0][0][ans_idx:ans_idx+ans_len]))\n",
    "        print()\n",
    "        print(\"Model answer:\")\n",
    "        print(roberta.decode(squad_bpe[0][0][model_start_idx:model_end_idx]))\n",
    "        print()\n",
    "        print(\"\\n\\n\\n\\n\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5366ffa-ed1f-48e1-91e8-a2e7ff949160",
   "metadata": {},
   "source": [
    "Cool! So the model works how we expect it to, giving us a start and end index to the tokens which we can decode to give us our answer. Unfortunately, the model isn't very accurate, which explains the high loss we saw before. I wonder if it's a coincidence if the first answer was correct? I'm not sure... Either way, I'm super glad that the model training and inference works!\n",
    "\n",
    "Now we can try to improve the model. One way to do this is to add a densely connected layer at the head of the model. My vision is to turn the 768\\*512 output to a single vector and use that as the first layer of neurons. Then, I'll turn it into a larger layer of 768\\*512\\*2 and then back down to 768\\*512. Unfortunately, my computer doesn't have the RAM to do this, as seen below. It's asking for over 618 GB of RAM. So, I think the next best thing would be a scaled down MLP, but if we're reducing the dimensions of the output of RoBERTa, then we're effectively trekking into bottleneck territory, which we showed earlier is not always good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "986a7539-8b28-4c12-9c9b-dcf457d32081",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1236950581248 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [143], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m         end \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(end,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m start, end\n\u001b[0;32m---> 35\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSquadModel2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# N=512, H=768\u001b[39;00m\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Use GPU\u001b[39;00m\n\u001b[1;32m     37\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n",
      "Cell \u001b[0;32mIn [143], line 11\u001b[0m, in \u001b[0;36mSquadModel2.__init__\u001b[0;34m(self, input_len, hidden_dim)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mE \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m768\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m512\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m768\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m512\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1236950581248 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "# Our custom RoBERTa head\n",
    "class SquadModel2(nn.Module):\n",
    "    def __init__(self, input_len, hidden_dim):\n",
    "        self.input_len = input_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        super(SquadModel2, self).__init__()\n",
    "        \n",
    "        self.S = nn.Parameter(torch.ones(self.hidden_dim, requires_grad=True)/self.hidden_dim)\n",
    "        self.E = nn.Parameter(torch.ones(self.hidden_dim, requires_grad=True)/self.hidden_dim)\n",
    "        \n",
    "        self.fc1 = nn.Linear(768*512, 768*512*2)\n",
    "        self.fc2 = nn.Linear(768*512*2, 768*512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad(): # Saves GPU memory since we aren't keeping track of gradients\n",
    "            x = roberta.extract_features(x) # BPE Squad input\n",
    "        x = torch.swapdims(x, 1, 2) # Correct dimensions so that matrix multiplication works\n",
    "        \n",
    "        # Densely connected layers\n",
    "        with torch.no_grad():\n",
    "            saved_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], 1, -1)\n",
    "        x = F.ReLU(self.fc1(x))\n",
    "        x = F.ReLU(self.fc2(x))\n",
    "        x = x.reshape(saved_shape)\n",
    "        \n",
    "        # Classicfication\n",
    "        start = self.S @ x\n",
    "        start = F.softmax(start,dim=0)\n",
    "        \n",
    "        end = self.E @ x\n",
    "        end = F.softmax(end,dim=0)\n",
    "        return start, end\n",
    "\n",
    "model = SquadModel2(512, 768) # N=512, H=768\n",
    "model.to('cuda') # Use GPU\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae05ed-7895-4a2b-8426-d6c05d2047a6",
   "metadata": {},
   "source": [
    "So, I'll call off the report here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a7b4c4-479e-4c2f-9f5a-bfe8f4d75534",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Overall, I'm really happy with this report. I learned a lot about a lot lol. I learned about the details and mathematical operations of the encoder. I did some dataset pre-processing to speed up my model training time. I also did some model pre-training using some provided code which I modified the hyper-parameters of. I compared the effects of a bottleneck and inverted bottleneck for RoBERTa and concluded the inverted bottleneck was better. I also added a question-answering head to BERT.\n",
    "\n",
    "Future explorations would be analyzing the GPT model and how a decoder works in detail (although it's very similar to the encoder). Trying out different pre-training regimes. Fine-tuning the model would be interesting to explore as well. Also, modifying the SQuAD head to perform better (with maybe a MLP) would be better. Further, different heads for different problems could be explored.\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9cde43-5578-4044-9efb-2c4d08393650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-thesis] *",
   "language": "python",
   "name": "conda-env-.conda-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
