{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c601e08-1f95-4d0e-a8f9-7256f79a0e3c",
   "metadata": {},
   "source": [
    "# APPM X720 Biweekly Report\n",
    "\n",
    "### *Alexey Yermakov*\n",
    "### *February 15 2022*\n",
    "\n",
    "# Summary\n",
    "\n",
    "\n",
    "\n",
    "# Main Content\n",
    "\n",
    "For this report I wanted to explore GloVe. I first re-read the [paper](https://nlp.stanford.edu/pubs/glove.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce9851c-892c-4b1e-b2d4-3439649ffeb5",
   "metadata": {},
   "source": [
    "Reading the paper, I learned that one of the main goals of the GloVe (which stands for Global Vectors) model is to take into account the \"global corpus statistics\" instead of what \"window-based\" methods provide. The paper implies that the latter gives a shallow view of the total statistics of word relationships. The former, however, gives a much more broad sense of which terms appear near one another by providing a value for any pair of two words. Thus, GloVe utilizes the co-occurrence matrix which has a non-negative integer for each pair of words in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975b4ab-da67-4e73-998e-b314f1841f7a",
   "metadata": {},
   "source": [
    "Lets now dive into the results for [GloVe](https://github.com/stanfordnlp/GloVe). One of the things that caught my attention was the \"pre-trained\" word vectors. Lets analyze what properties these word-vectors have. First, I have my standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0b47e12c-bf9a-4fbf-81a0-35737165b600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available? True\n"
     ]
    }
   ],
   "source": [
    "# Make sure PyTorch is installed and our GPU is available\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, permute, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"GPU Available?\",torch.cuda.is_available())\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815e813-ba53-4ed7-ad53-0cc69647ca4d",
   "metadata": {},
   "source": [
    "I then downloaded the word-vector representations that came out of GloVe by training on the [Wikipedia 2014 + Gigaword 5](https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip) database. Below is a helper function to parse the files and extract them to what I need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dd83e6c-4934-4a0d-ab6f-284a58dbf304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of vectors and a list of words from the file, as well as the dimension of the vectors\n",
    "def get_vecs(file_name):\n",
    "    # Initialize return variables\n",
    "    vec_list = list()\n",
    "    words_list = list()\n",
    "    vec_dim = 0\n",
    "    # Open file\n",
    "    with open(file_name) as f:\n",
    "        # Read line by line\n",
    "        for line in f.readlines():\n",
    "            # Split by white space\n",
    "            line = line.split()\n",
    "            # Add word to words list\n",
    "            words_list.append(line[0])\n",
    "            # Convert vector represenation from strings to floats\n",
    "            vec = list(map(lambda x: float(x), line[1:]))\n",
    "            # Add vector representation to return list\n",
    "            vec_list.append(vec)\n",
    "        return words_list, vec_list, len(vec_list[0])\n",
    "    raise Exception(\"get_vecs: File cound not be opened\")\n",
    "\n",
    "words, vecs, vec_dim = get_vecs('/home/alexey/School/APPM 4720/Biweekly/Feb15/Report/word_vecs/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc720cbf-2ad1-47ca-860d-e14a625555d6",
   "metadata": {},
   "source": [
    "According to the paper, we should see that the `words` array should contain 400,000 words, consisting of the 400,000 most frequent words from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32f445f9-161f-4662-af28-d59251388cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  400001\n",
      "Number of vectors: 400001\n",
      "Vector dimensions: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words: \", len(words))\n",
    "print(\"Number of vectors:\", len(vecs))\n",
    "print(\"Vector dimensions:\", vec_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8200ef11-6d02-48f1-af0e-e17a6d4cb803",
   "metadata": {},
   "source": [
    "Looks like we're one over?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7148d75b-52d5-41b6-bc30-5cf3d9c40f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "[0.072617, -0.51393, 0.4728, -0.52202, -0.35534, 0.34629, 0.23211, 0.23096, 0.26694, 0.41028, 0.28031, 0.14107, -0.30212, -0.21095, -0.10875, -0.33659, -0.46313, -0.40999, 0.32764, 0.47401, -0.43449, 0.19959, -0.55808, -0.34077, 0.078477, 0.62823, 0.17161, -0.34454, -0.2066, 0.1323, -1.8076, -0.38851, 0.37654, -0.50422, -0.012446, 0.046182, 0.70028, -0.010573, -0.83629, -0.24698, 0.6888, -0.17986, -0.066569, -0.48044, -0.55946, -0.27594, 0.056072, -0.18907, -0.59021, 0.55559]\n"
     ]
    }
   ],
   "source": [
    "print(words[-1])\n",
    "print(vecs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7390767-f5f3-44ae-8a50-52cd60521397",
   "metadata": {},
   "source": [
    "Scrolling down to the file I parsed, it's clear that there are 400,001 lines, which is what we see, though I'm not sure what `<unk>` is. Maybe it means `unknown`? Or maybe it's a grouping of all of the remaining words that didn't make the top 400,000. In either case, I'll just ignore this \"word\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d444b98d-0b1a-4912-8110-753fca671405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  400000\n",
      "Number of vectors: 400000\n",
      "Vector dimensions: 50\n"
     ]
    }
   ],
   "source": [
    "words = words[:-1]\n",
    "vecs = vecs[:-1]\n",
    "\n",
    "print(\"Number of words: \", len(words))\n",
    "print(\"Number of vectors:\", len(vecs))\n",
    "print(\"Vector dimensions:\", vec_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f49748-e21c-4a6e-9b69-89be68e6222e",
   "metadata": {},
   "source": [
    "Lets now print the top 10 and bottom 10 most frequent words. Note that no where does it say the input file is in terms of frequency, but as we can see below it can be implied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca088933-f8dc-42f2-8b99-a1568c12f295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1      :the\n",
      "#2      :,\n",
      "#3      :.\n",
      "#4      :of\n",
      "#5      :to\n",
      "#6      :and\n",
      "#7      :in\n",
      "#8      :a\n",
      "#9      :\"\n",
      "#10     :'s\n",
      "       ...\n",
      "#399991 :sigarms\n",
      "#399992 :katuna\n",
      "#399993 :aqm\n",
      "#399994 :1.3775\n",
      "#399995 :corythosaurus\n",
      "#399996 :chanty\n",
      "#399997 :kronik\n",
      "#399998 :rolonda\n",
      "#399999 :zsombor\n",
      "#400000 :sandberger\n"
     ]
    }
   ],
   "source": [
    "# Prints first and last n words of the words vector\n",
    "def print_top_bottom_n(n, words):\n",
    "    for i in range(n):\n",
    "        print(f\"#{i+1:<7}:{words[i]}\")\n",
    "    print(\"       ...\")\n",
    "    for i in range(len(words)-n,len(words)):\n",
    "        print(f\"#{i+1:<7}:{words[i]}\")\n",
    "\n",
    "print_top_bottom_n(10, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3537b106-f5ab-4afd-8483-47d525bb601d",
   "metadata": {},
   "source": [
    "Some immediate observations:\n",
    "1) It seems almost certainly that the list is in terms of frequency, since \"the\" is first, along with \"a\", \"of\", and \"and\" in the top 10.\n",
    "2) Special characters like \",\" and \".\" aren't exactly words but are included in this list\n",
    "3) Numbers appear as well, such as \"1.3775\"\n",
    "\n",
    "So, lets dig deeper and count up the number of words that contain just numbers (including decimals), just letters, and both. Below I define some helper functions using [regex](https://docs.python.org/3/howto/regex.html) to find fun words included in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1d8d7520-cb97-41fa-b6a6-c36fcac772fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for Alphabetical\n",
      "Test        : True\n",
      "Test1       : False\n",
      "123         : False\n",
      "1.23        : False\n",
      "\"           : False\n",
      "<bruh>      : False\n",
      "<1bro>      : False\n",
      "<123>       : False\n",
      "big-sleep   : True\n",
      "1,200       : False\n",
      "1/2         : False\n",
      "day_care    : True\n",
      "\n",
      "Checking for Numeric\n",
      "Test        : False\n",
      "Test1       : False\n",
      "123         : True\n",
      "1.23        : True\n",
      "\"           : False\n",
      "<bruh>      : False\n",
      "<1bro>      : False\n",
      "<123>       : False\n",
      "big-sleep   : False\n",
      "1,200       : True\n",
      "1/2         : True\n",
      "day_care    : False\n",
      "\n",
      "Checking for Alphanumeric\n",
      "Test        : True\n",
      "Test1       : True\n",
      "123         : True\n",
      "1.23        : True\n",
      "\"           : False\n",
      "<bruh>      : False\n",
      "<1bro>      : False\n",
      "<123>       : False\n",
      "big-sleep   : True\n",
      "1,200       : True\n",
      "1/2         : True\n",
      "day_care    : True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define helper functions\n",
    "def is_alpha(word):\n",
    "    p = re.compile('^[a-zA-Z-_]+$')\n",
    "    return p.match(word) is not None\n",
    "\n",
    "def is_num(word):\n",
    "    p = re.compile('^[0-9.,/]+$')\n",
    "    return p.match(word) is not None\n",
    "\n",
    "def is_alpha_num(word):\n",
    "    p = re.compile('^[a-zA-Z0-9.,-/_]+$')\n",
    "    return p.match(word) is not None\n",
    "\n",
    "# Run some basic tests\n",
    "for func, name in [(is_alpha, \"Alphabetical\"), (is_num, \"Numeric\"), (is_alpha_num, \"Alphanumeric\")]:\n",
    "    print(f\"Checking for {name}\")\n",
    "    for test_word in [\"Test\", \"Test1\", \"123\", \"1.23\", \"\\\"\", \"<bruh>\", \"<1bro>\", \"<123>\", \"big-sleep\", \"1,200\", \"1/2\", \"day_care\"]:\n",
    "        print(f\"{test_word:<12}:\", func(test_word))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad3bf5-9bfd-4647-84da-869926f12053",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now lets loop through our list of words and print anything that stands out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0713c863-795a-4cec-9105-85accd40a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define counters\n",
    "\n",
    "def find_cool_words(words):\n",
    "    # Counters\n",
    "    alpha = 0\n",
    "    num = 0\n",
    "    alpha_num = 0\n",
    "    # Store cool words\n",
    "    cool_words = list()\n",
    "    for word in words:\n",
    "        hit = 0 # Flag to see if the word fit in any category\n",
    "        if is_alpha(word):\n",
    "            alpha += 1\n",
    "            hit = 1\n",
    "        if is_num(word):\n",
    "            alpha += 1\n",
    "            hit = 1\n",
    "        if is_alpha_num(word):\n",
    "            alpha_num += 1\n",
    "            hit = 1\n",
    "\n",
    "        if hit == 0:\n",
    "            # Found a cool word\n",
    "            cool_words.append(word)\n",
    "    return alpha, num, alpha_num, cool_words\n",
    "\n",
    "alpha, num, alpha_num, cool_words = find_cool_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f9c48e9-7ac2-4493-9a58-45cb94e2bd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13650\n",
      "['6:15', \"'20s\", '3:05', \"shi'a\", 'fenerbahçe', 'seán', 'olyā', \"o'dowd\", 'μm', 'héctor', 'coupé', '4:45', \"d'alene\", 'bolesław', 'niño', '7:10', \"d'epargne\", '2:45', 'a&e', 'göring', '+44', '14:00', 'chrétien', 'franche-comté', 'tromsø', 'lászló', 'asunción', 'édouard', 'rzeszów', 'cádiz', 'supérieure', 'fútbol', 'étienne', 'köln', 'métis', '5:45', '‘’', 'françoise', 'soflá', 'agustín', 'björk', '8:20', \"d'souza\", 'encyclopædia', 'http://spaceflight.nasa.gov', 'http://nytsyn.com', '2:20', 'tomé', '11:45', \"o'briant\", 'poincaré', \"o'higgins\", \"d'alessandro\", 'fiancé', 'víctor', 'condé', 'rhône', 'björn', '1:35', 'milošević', '¡', 'bālā', \"o'dell\", '15:00', \"d'onofrio\", '3:45', '1:1', '1:40', 'ρ', 'd&d', 'medellín', 'núñez', 'épée', \"ma'am\", 'istván', '1:20', 'première', 'cafés', \"o'donoghue\", 'münchen', '(717)', '19:00', '12:15', 'lwów', 'īn', 'văn', 'bình', 'vélez', 'i̇zmir', 'atatürk', '18:00', \"c'est\", 'île-de-france', '20:00', 'jean-françois', 'rubén', 'việt', \"d'art\", 'español', 'dalí', 'dušan', 'goiás', 'schrödinger', 'naïve', 'sơn', 'jános', \"'93\", '10:10', 'vitória', 'über', '8:50', 'márquez', 'española', '(852)', 'bibliothèque', 'doña', \"ba'ath\", 'cristóbal', 'potosí', \"d'amico\", '2:05', 'mérida', '10:20', 'i̇stanbul', \"o'donovan\", \"o'kelly\", 'diệm', \"o'brian\", '7:40', \"d'azur\", 'príncipe', 'áh', '12:45', 'velázquez', 'niccolò', \"d'oeuvres\", \"l'amour\", 'shōnen', '17:00', '09:00', 'saarbrücken', 'coruña', '8:10', 'nestlé', 'cárdenas', 'plzeň', 'perú', '9:40', \"d'oyly\", 'simón', '7:35', 'räikkönen', '11:35', 'hk$', '3:10', 'ramí', '¢', 'göteborg', 'señora', 'košice', 'beşiktaş', 'jiří', 'václav', 'tucumán', 'père', '+66', 'straße', \"nor'easter\", 'ávila', '13:00', 'györgy', 'å', '2:50', \"d'oro\", 'jérôme', 'františek', 'rivière', '10:40', 'hernán', '8:40', 'niš', 'renée', '2:10', 'métro', 'música', 'antônio', 'tomáš', 'bartók', 'b&o', 'vázquez', 'brasília', 'płock', 'lüneburg', 'régime', 'dvořák', 'gödel', 'süd', 'armée', 'ψ', '3:20']\n"
     ]
    }
   ],
   "source": [
    "print(len(cool_words))\n",
    "print(cool_words[400:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add5526-daa5-4ecf-bc2f-3ca349b0514e",
   "metadata": {},
   "source": [
    "Turns out there are a lot of words that probably shouldn't have been in the word list! In fact, I found over 10,000 words that evaded my regex. As you can see from the sub-sample above, part of the reason is that characters which aren't alpha-numeric were included. Such as, \"ř\" and \"ψ\". Furthermore, things like \"9:40\" (probably a time) and \"+66\" (the phone number country code for Thailand) were included as well. So, to me this means that NLP is probably a very messy practice, since the vocabularies dealt with aren't always composed of what we would typically call \"words\". I would never use \"http://spaceflight.nasa.gov\" in a normal conversation lol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff5e08-843e-4fae-b334-71cf337eabdc",
   "metadata": {},
   "source": [
    "I now want to compute some cosine similarity distances between words we typically deal with in NLP models when representing them as vectors. Recall the definition of [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity):\n",
    "\n",
    "$\\text{cosine similarity} = S_C(A,B):=cos(\\theta)=\\frac{A \\cdot B}{||A||\\ ||B||}$\n",
    "\n",
    "Thus, we are comparing the \"angles\" between words. Words with similar angles ($\\theta \\approx 0$) will result in a similarity score close to $1$ and words with different angles will result in smaller values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "743b8b20-6d1a-4941-99da-451d1e8dc13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between \"man\" and \"man\" 1.0000000000000002\n",
      "cosine similarity between \"man\" and \"woman\" 0.886033771849582\n",
      "cosine similarity between \"king\" and \"queen\" 0.7839043010964117\n",
      "cosine similarity between \"man\" and \"king\" 0.5309376939717351\n",
      "cosine similarity between \"woman\" and \"queen\" 0.6003105805751625\n",
      "cosine similarity between \"serf\" and \"peasant\" 0.37457241755984644\n",
      "cosine similarity between \"rich\" and \"poor\" 0.5721133053516305\n",
      "cosine similarity between \"brick\" and \"running\" 0.3030795426034597\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity function (note: input are vectors)\n",
    "def cos_sim(A,B):\n",
    "    return np.dot(A,B)/(np.linalg.norm(A)*np.linalg.norm(B))\n",
    "\n",
    "# Wrapper to find cosine similarity between two words (note: input are strings)\n",
    "def print_cos_sim(A,B):\n",
    "    print(f\"cosine similarity between \\\"{A}\\\" and \\\"{B}\\\"\", cos_sim(vecs[words.index(A)],vecs[words.index(B)]))\n",
    "\n",
    "print_cos_sim(\"man\", \"man\")\n",
    "print_cos_sim(\"man\", \"woman\")\n",
    "print_cos_sim(\"king\", \"queen\")\n",
    "print_cos_sim(\"man\", \"king\")\n",
    "print_cos_sim(\"woman\", \"queen\")\n",
    "print_cos_sim(\"serf\", \"peasant\")\n",
    "print_cos_sim(\"rich\", \"poor\")\n",
    "print_cos_sim(\"brick\", \"running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51acd62-3aaf-4246-9d15-88526e9c6c9e",
   "metadata": {},
   "source": [
    "From the above, it it clear why \"brick\" and \"running\" are the least similar, however, I am surprised \"serf\" and \"peasant\" had the second lowest similarity scores. Similarly, I am not 100% sure why \"man\" and \"woman\" were more similar than \"woman\" and \"queen\". Perhaps the model is learning more about the context in which the word is used than the definition. As such, \"man\" and \"woman\" are more likely interchangeable than \"woman\" and \"queen\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384d2b4-e7e0-4cc4-b0e3-a68c1c33853f",
   "metadata": {},
   "source": [
    "The next thing I want to do is write some code to find the closest vector from a semantic relationship. For example, I want to find \"queen\" as the answer to: man is to king as woman is to ______. I have a naive implementation below where I loop through each vector and find the closest vector by minimizing the L2 norm of their difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "67ea6979-77f2-4d14-b0fa-cf5ff6dabdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest vector given three words  (note: input are strings)\n",
    "def find_nearest(A,B,C, words, vecs):\n",
    "    # Calculate approximate location of the word we're looking for\n",
    "    A_vec = np.asarray(vecs[words.index(A)])\n",
    "    B_vec = np.asarray(vecs[words.index(B)])\n",
    "    C_vec = np.asarray(vecs[words.index(C)])\n",
    "    D_vec = A_vec-B_vec+C_vec\n",
    "    \n",
    "    # Find the closest vector\n",
    "    best_vec = np.asarray(vecs[0])\n",
    "    best_idx = 0\n",
    "    min_dist = np.linalg.norm(D_vec - best_vec)\n",
    "    for i in range(1,len(vecs)):\n",
    "        if words[i] in [A,B,C]:\n",
    "            continue\n",
    "        dist = np.linalg.norm(D_vec - vecs[i])\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_idx = i\n",
    "            best_vec = np.asarray(vecs[i])\n",
    "    \n",
    "    return best_idx, words[best_idx], list(best_vec)\n",
    "\n",
    "best_idx, best_word, best_vec = find_nearest(\"man\", \"king\", \"woman\", words, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e73a6b2b-2f72-4012-9548-2852a5765d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best (girl) distance 5.863896597375144\n",
      "Expected (queen) distance 9.562123438372925\n",
      "Woman distance 5.143856989705974\n"
     ]
    }
   ],
   "source": [
    "# Print distances of interest\n",
    "A_vec = np.asarray(vecs[words.index(\"man\")])\n",
    "B_vec = np.asarray(vecs[words.index(\"king\")])\n",
    "C_vec = np.asarray(vecs[words.index(\"woman\")])\n",
    "D_vec = A_vec-B_vec+C_vec\n",
    "\n",
    "print(f\"Best ({best_word}) distance\", np.linalg.norm(D_vec - best_vec))\n",
    "print(\"Expected (queen) distance\", np.linalg.norm(D_vec - vecs[words.index(\"queen\")]))\n",
    "print(\"Woman distance\", np.linalg.norm(D_vec - vecs[words.index(\"woman\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652cad96-9a74-4826-9df0-1e70e5143707",
   "metadata": {},
   "source": [
    "From the above, it's clear that this did not work as expected. Instead of finding \"queen\", I found \"girl\". In fact, \"woman\" was found as the best, but I added an \"if\" statement to not include inputs as the result. I wonder if this is because I'm working in 50 dimensions? Lets see what happens if I increase the dimensions to 300, where the GloVe paper found diminishing returns began."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "124592fe-8368-4483-a201-6b1a1bcb0039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best (girl) distance 9.292324809354577\n",
      "Expected (queen) distance 13.679234155934203\n",
      "Woman distance 7.516539943914799\n"
     ]
    }
   ],
   "source": [
    "# Load 300d file\n",
    "words, vecs, vec_dim = get_vecs('/home/alexey/School/APPM 4720/Biweekly/Feb15/Report/word_vecs/glove.6B.300d.txt')\n",
    "\n",
    "# Find nearest vector\n",
    "best_idx, best_word, best_vec = find_nearest(\"man\", \"king\", \"woman\", words, vecs)\n",
    "\n",
    "# Print distances of interest\n",
    "A_vec = np.asarray(vecs[words.index(\"man\")])\n",
    "B_vec = np.asarray(vecs[words.index(\"king\")])\n",
    "C_vec = np.asarray(vecs[words.index(\"woman\")])\n",
    "D_vec = A_vec-B_vec+C_vec\n",
    "\n",
    "print(f\"Best ({best_word}) distance\", np.linalg.norm(D_vec - best_vec))\n",
    "print(\"Expected (queen) distance\", np.linalg.norm(D_vec - vecs[words.index(\"queen\")]))\n",
    "print(\"Woman distance\", np.linalg.norm(D_vec - vecs[words.index(\"woman\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2f8fc9-b6ec-4093-885e-47649d5343e2",
   "metadata": {},
   "source": [
    "Okay, so we got similar results! This means that the dimensions of the vectors are less likely to be the problem. This also means that the model isn't perfect. Oh well. One last thing I want to do in this notebook is to get t-SNE working so I can further look into the vector representations of the words and see what is \"close\" to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51072907-0b26-4238-abee-ea5826ecf077",
   "metadata": {},
   "source": [
    "I read [this guide](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1) to understand t-SNE. (if you want a screenshot of the page, feel free to email me!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886fb933-d78c-400e-9343-3392996f1232",
   "metadata": {},
   "source": [
    "So, t-SNE stands for t-Distributed Stochastic Neighbor Embedding. It is used to visualize high-dimensional data. I want to use it to bring our 300-dimensional vectors down into 2 dimensions so I can plot them and see which vectors are \"near\" one another. It was developed in 2008, whereas its ancestor PCA (Principal Component Analysis) was developed in 1933."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98abc6b8-0bbe-4de9-b06a-1c13fe6e7cc4",
   "metadata": {},
   "source": [
    "There is an [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) implementation of t-SNE which I'll be using below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "49a8385f-c457-45fb-b615-6a54f1694f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 300)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "perplexity must be less than n_samples",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39masarray(random_vecs)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     13\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([random_vecs])\n\u001b[1;32m     14\u001b[0m X_embedded \u001b[38;5;241m=\u001b[39m \u001b[43mTSNE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 15\u001b[0m \u001b[43m                  \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperplexity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_embedded)\n\u001b[1;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m8\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1122\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \n\u001b[1;32m   1105\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03m        Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:793\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 793\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity must be less than n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: perplexity must be less than n_samples"
     ]
    }
   ],
   "source": [
    "n_words = 4\n",
    "idxs = np.random.choice(400000,n_words, replace=False)\n",
    "\n",
    "random_vecs = list()\n",
    "random_words = list()\n",
    "\n",
    "for idx in idxs:\n",
    "    random_vecs.append(vecs[idx])\n",
    "    random_words.append(words[idx])\n",
    "\n",
    "print(np.asarray(random_vecs).shape)\n",
    "\n",
    "X = np.array([random_vecs])\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=3).fit_transform(X)\n",
    "\n",
    "print(X_embedded)\n",
    "\n",
    "plt.figure(figsize=[12,8])\n",
    "\n",
    "# Plot our scatter plot\n",
    "for i in range(X_embedded):\n",
    "    vec = X_embedded[i]\n",
    "    wrd = X_embedded[i]\n",
    "    plt.scatter(vec[0], vec[1])\n",
    "    plt.text(vec[0],vec[1]-5,wrd,fontsize=12, ha='center')\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.ylabel(\"Y\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.title(f\"t-SNE of some 300-D words\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4fa681-7893-4f3c-a734-ad6524b1afae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-thesis] *",
   "language": "python",
   "name": "conda-env-.conda-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
