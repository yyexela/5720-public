{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c601e08-1f95-4d0e-a8f9-7256f79a0e3c",
   "metadata": {},
   "source": [
    "# APPM X720 Biweekly Report\n",
    "\n",
    "### *Alexey Yermakov*\n",
    "### *April 19, 2022*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357d1a8-78da-4363-987c-7190b9460399",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "For this report, I focus on different sampling methods. This report was inspired by Nucleus Sampling from the paper [The Curious Case of Neural Text Degeneration](https://arxiv.org/pdf/1904.09751.pdf) covered in class.\n",
    "\n",
    "# Main Content\n",
    "\n",
    "I started with using [GPT2 from HuggingFace](https://huggingface.co/gpt2?text=My+name+is+Alexey%2C+which+rhymes+with+%22We%27re+the+One%21%22+%28which%2C+to+be+honest%2C+I+don%27t+understand%29.%0A%0AI+have+always+been+a+huge+fan+of+Alexey%27s+personality.+We+often+played+together+at+the+same) since I was under the impression it would be easy to use and would lend itself nicely to testing through text generation. This intuition was correct, as we'll see.\n",
    "\n",
    "## GPT-2 Text Generation\n",
    "\n",
    "First, I have my typical imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8550cf6-f963-4b5a-bc39-7d9a26930c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available? True\n"
     ]
    }
   ],
   "source": [
    "# Import everything\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model, pipeline, set_seed, GenerationConfig\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"GPU Available?\",torch.cuda.is_available())\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85afc786-dfb3-44a8-b50e-291dfdd08c6f",
   "metadata": {},
   "source": [
    "Then, I load [GPT2](https://huggingface.co/gpt2) from HuggingFace. Note that \"this is the smallest version of GPT-2, with 124M parameters\". There are ways to load the larger models, but I don't really care about that for this particular report. I loaded the smaller model since it runs quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0a6576-996f-4db9-8aaa-cfb9aecb80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace's GPT2 small model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21881ca-6223-46a1-aaae-e3664150a0b3",
   "metadata": {},
   "source": [
    "Fortunately, text generation is wicked easy to use, the code below is from HuggingFace's [GPT2](https://huggingface.co/gpt2?text=My+name+is+Alexey%2C+which+rhymes+with+%22We%27re+the+One%21%22+%28which%2C+to+be+honest%2C+I+don%27t+understand%29.%0A%0AI+have+always+been+a+huge+fan+of+Alexey%27s+personality.+We+often+played+together+at+the+same) page. What it does is generate words based on the initial text, which is `Hello, I'm a language model,`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6150ded7-6ef3-4cfe-84ed-8007fa436555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexey/.conda/envs/thesis/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output:\n",
      "Hello, I'm a language model, though I'm more interested in semantics.\n",
      "\n",
      "How would you describe Python?\n",
      "\n",
      "Python is a framework\n",
      "\n",
      "Word length:\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# Generate output from GPT2 with 30 length\n",
    "set_seed(42)\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "output = generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=1)\n",
    "\n",
    "print('Generated output:')\n",
    "print(output[0][\"generated_text\"])\n",
    "print()\n",
    "print(\"Word length:\")\n",
    "print(len(output[0]['generated_text'].split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce88f7-587e-4a1d-b77c-42eea24bba11",
   "metadata": {},
   "source": [
    "Great! So the text generation is *really easy to do*. First, I want to point out that the word length does *not* equal the *max_length* parameter, the latter represents the number of tokens, which are of size less than or equal to one word. Second, there is some error about generation configuration files. Visiting the [provided link](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/text_generation) took me to [another page](https://huggingface.co/docs/transformers/generation_strategies) explaining how to use generation configuration files. This is excellent because it is exactly what we need to be able to test out nucleus sampling! The latter page shows us how to get the configuration file of a model, let's see what it is for GPT2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49e2bae-ba9c-4b36-86c1-aa43d955ce55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print GPT2 configurations\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11f48e-bbc4-45e5-a015-ea5b48e4a012",
   "metadata": {},
   "source": [
    "Huh. Turns out, that this is normal! `Printing out the model.generation_config reveals only the values that are different from the default generation configuration, and does not list any of the default values.` So, this is expected since we didn't modify the defaults of the downloaded GPT2 model, so we'd expect nothing to be changed. Now let's see which parameters are interesting in modifying in the generation strategy.\n",
    "\n",
    "From [this page](https://huggingface.co/docs/transformers/generation_strategies) we see that the default strategy is greedy search, we also see how to do beam search, and lastly see how to include top-k and top-p/nucleus sampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40998e-548e-4ed3-84ce-dd5bba3c469b",
   "metadata": {},
   "source": [
    "```\n",
    "The default decoding strategy is greedy search, which is the simplest decoding strategy that picks a token with the highest probability as the next token.\n",
    "\n",
    "num_beams: by specifying a number of beams higher than 1, you are effectively switching from greedy search to beam search. This strategy evaluates several hypotheses at each time step and eventually chooses the hypothesis that has the overall highest probability for the entire sequence. This has the advantage of identifying high-probability sequences that start with a lower probability initial tokens and would’ve been ignored by the greedy search.\n",
    "\n",
    "do_sample: if set to True, this parameter enables decoding strategies such as multinomial sampling, beam-search multinomial sampling, Top-K sampling and Top-p sampling. All these strategies select the next token from the probability distribution over the entire vocabulary with various strategy-specific adjustments.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d5b8a-bc4a-4d23-87eb-0230a8053781",
   "metadata": {},
   "source": [
    "The actual parameters are listed [here](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/text_generation#transformers.GenerationConfig), for the [GenerationConfig](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/text_generation#transformers.GenerationConfig) class. We see see exact parameters that we want:\n",
    "\n",
    "```\n",
    "do_sample (bool, optional, defaults to False) — Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "\n",
    "num_beams (int, optional, defaults to 1) — Number of beams for beam search. 1 means no beam search.\n",
    "\n",
    "temperature (float, optional, defaults to 1.0) — The value used to modulate the next token probabilities.\n",
    "\n",
    "top_k (int, optional, defaults to 50) — The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "\n",
    "top_p (float, optional, defaults to 1.0) — If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "```\n",
    "\n",
    "Of course, there are many, many more parameters, but these are the ones that will let us try out the configurations in the paper [The Curious Case of Neural Text Degeneration](https://arxiv.org/pdf/1904.09751.pdf).\n",
    "\n",
    "Below is a sample configuration, just to see how to use it. I reduce the number of max_new_tokens to see if the output size changes from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3950c43-d4fd-443f-b553-ede6a661f4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, not a programming language. I'm a language model\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate output from GPT2 with 10 tokens\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=10, eos_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "generator(\"Hello, I'm a language model,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d697d900-8e87-47c0-a097-db3b1fd4237e",
   "metadata": {},
   "source": [
    "Okay, so two things worth noting here: First, the `config` did reduce the output size, proving to me that this is working. Second, the output is different than before. My guess is that the default generation configuration for GPT2 is not the same as the defaults in the `GenerationConfig` class, so using a separate config messed up the generation from earlier. This doesn't matter too much for this report since I'll be messing with the parameters anyways.\n",
    "\n",
    "To get rid of the next error, I modified the config slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "041994b2-f8cb-4142-a7f1-fa8b63d21c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, not a programming language. I'm a language model\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate GPT2 output without errors\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=10, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "generator(\"Hello, I'm a language model,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c2646-3f41-494a-a514-f4ee9c12d370",
   "metadata": {},
   "source": [
    "Excellent! I think we're now ready to test the different parameters for next text generation. From the paper, it's stated that *degeneration* leads to *output text that is bland, incoherent, or gets stuck in repetitive loops*. Lets try their example input\n",
    "\n",
    "`In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley,\n",
    "in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.`\n",
    "\n",
    "in different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdddf514-09d6-4b5d-b309-f305ae54a3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent.\"\n",
      "\n",
      "The researchers found that the unicorns were able to communicate with each other through their tongues.\n",
      "\n",
      "\"They were able to communicate with each other through their tongues,\" Siegel said. \"They were able to communicate with each other through their tongues.\"\n",
      "\n",
      "The researchers also found that the unicorns were able to communicate with each other through their eyes.\n",
      "\n",
      "\"They were able to communicate with each other through their eyes,\" Siegel said. \"They were\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with input from paper\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf221f-9705-4057-94f3-cead22a561c5",
   "metadata": {},
   "source": [
    "Haha, so here we can see a few things mentioned in the paper:\n",
    "- repeating text: \"They were very intelligent, and they were very intelligent, and they were very intelligent.\"\n",
    "- bland text: The researchers found that the unicorns were able to communicate with each other through their tongues. \"They were able to communicate with each other through their tongues,\" Siegel said.\n",
    "- incoherent text: the unicorns were able to communicate with each other through their eyes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798a320-7a80-472d-8184-9a6eb68c079b",
   "metadata": {},
   "source": [
    "Note that my output here isn't exactly what's from the paper since they used GPT2-large, at 774M parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8959ec-ecfb-415a-9c81-e3a96d9034c8",
   "metadata": {},
   "source": [
    "## Top-k Sampling\n",
    "\n",
    "To explain top-k sampling, I'll start by explaining random sampling. Random sampling takes the final probabilities for each token at the head of the model (after softmax) and determines the next token by randomly choosing a word based on the probability it has from softmax. Top-k sampling basically does the same thing, but only allows the top k (k is some integer) words to be available to be selected randomly. So it effectively just cuts off the tail of the distribution at each inference step. Let's see how this affects our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "667bf4ec-ae45-44ec-bf4c-1d756fe76349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\"They also were completely literate on language and grammar; they were very happy with their food and had even taken to eating human food,\" said the study's lead author, David E. Fuchs, Ph.D., an assistant professor of anthropology at the University of California, Santa Barbara and co-author of the paper. Fuchs was not immediately available for comment on the findings, but the discovery is consistent with a similar study conducted in the same region in 2012. It was published in Science in January, the same month that the first U.S. unicorns were sighted in the mountains.\n",
      "\n",
      "The discovery also raises some fundamental questions about how the U.S. government's efforts to help indigenous peoples in Mexico —\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with top_k=20\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, do_sample=True, top_k=20, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa68e5-b9a1-4e62-8b08-62ab901537af",
   "metadata": {},
   "source": [
    "Uh-huh, so this seems a lot more understandable to me! Intuitively, what's happening is we're introducing variability in the text generated by sampling from a random distribution, and since we're controlling this random distribution to be the k most probable words, we aren't getting gibberish. The issues with top-k mentioned in the paper are that if k is small enough, it's possible to get bland text. Let's try to force that by choosing a smaller k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f273ca9-c0c1-46cb-87bb-ec09f0fa65ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\"They were very, very clever and were very intelligent,\" said Dr. David B. Hargrave, a professor of botany at the University of Arizona. The researchers believe their findings are the first to show that a language has a unique ability to speak. They say they have found evidence of this ability in other animals, and it's the result of an evolutionary process known as \"genetic engineering.\"\n",
      "\n",
      "The researchers say that their findings may be the first to demonstrate that language can be used in the human condition.\n",
      "\n",
      "\"The fact that we have found a way to communicate with these animals is an exciting and very exciting development, and it's a big step toward the discovery of a way to communicate with these animals,\"\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with top_k=5\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, do_sample=True, top_k=5, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c7bf7-dbc2-4be8-8d5a-3f75a98f5ed2",
   "metadata": {},
   "source": [
    "Aha! So we have blandness as described in the paper and overall nonsense. I found `their findings are the first to show that a language has a unique ability to speak` and that the study was done by `a professor of botany at the University of Arizona` to be particularly tickling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea19c23-e1c8-4acf-a938-33c9fb617bc7",
   "metadata": {},
   "source": [
    "The paper also mentions that a large k has the chance to include too many tokens, effectively resulting in random sampling. Let's see this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acf57b0f-72bb-4ae9-bb07-559a30cd8610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\"At the time, there was a remote tribe on the island of Thirrevy that used to live where the Donse people are.\" Things have changed since then.\n",
      "\n",
      "Recently, 124 unicorn are reported to have arrived in a remote region of Thailand across from the Andes Mountains to coexist in life.\n",
      "\n",
      "This developing condition leaves a good chance of befriending scarce animals. According to Dezeen, Research invents books on unicorn culture that give different inside ideas about animals and living in the original It's not just that the results seem hot, but that they lent sway\"\n",
      "\n",
      "Chicago's Dezeen also expressed, \"As a researcher in this field, I believe that the unicorns embody a perfect balance\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with top_k=5000000\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, do_sample=True, top_k=5000000, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f69fb-0c1c-46ac-8899-27224afe3a5e",
   "metadata": {},
   "source": [
    "So here, top-k sampling includes the tails of our softmaxxed token distribution, and we can see those getting sampled frequently. Why do tokens with low probability get sampled, you might ask? Because individually, they have a small probability, but collectively the probability of anything from the tail being chosen is quite large.\n",
    "\n",
    "## Beam Search\n",
    "\n",
    "Let's move on to beam search. Recall that beam search is like greedy search, except at each inference step it's keeping track of the most probably `n` tokens to generate. So, if the beam width is 1, it's greedy search. If the beam width is 2, it's greedy search except it's keeping track of the `n` most probable paths of generating tokens. If I didn't explain it well, which is likely, please read [this article](https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc) or [this one](https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f). Or you can take a look at what GPT2 thinks beam search is below. The below output was generated with a beam with of 10 and the initial tokens being `\"What is beam search? Beam search is \"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fbed52d-b2ca-4355-9ff5-350f722a69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is beam search? Beam search is  a term used to describe the ability of a computer to search for patterns of light in a given area of the electromagnetic spectrum. It is used to describe the ability of a computer to search for patterns of light in a given area of the electromagnetic spectrum. It is used to describe the ability of a computer to search for patterns of light in a given area of the electromagnetic spectrum. It is used to describe the ability of a computer to search for patterns of light in a given area of the electromagnetic spectrum. It is used to describe the ability of a computer to search for patterns of light in a given area of the electromagnetic spectrum. It is used to describe the ability of a computer to search for patterns of light in a given area of the electromagnetic spectrum\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with num_beams=10\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, num_beams=10, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"What is beam search? Beam search is \")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffdaa19-438e-4435-aa26-94fb6c97600b",
   "metadata": {},
   "source": [
    "Going back to the task, let's do beam search on the original input string we've beam using (yes, the typo was intentional). The beam width is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ac1cd66-6141-4e2d-9a35-ea50d9fcbc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "The study, published in the journal Proceedings of the National Academy of Sciences, is the first to show that unicorns are able to communicate with each other.\n",
      "\n",
      "\"This is the first time that unicorns have been able to communicate with each other, and it's the first time that unicorns have been able to communicate with each other, and it's the first time that unicorns have been able to communicate with each other, and it's the first time that unicorns have been able to communicate with each other, and it's the first time that unicorns have been able to communicate with each other, and it's the first time that unicorns have been able to communicate with each other, and it's the first time that\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with num_beams=10\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, num_beams=10, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7c7ee-dead-4180-9c57-c5dbe3191cdc",
   "metadata": {},
   "source": [
    "Ooookay, so this is not very good. There is lots of repetition and we don't like that. In fact, it seems even more repetitive than greedy search! What if we increase the number of beams to 50?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea7b973f-349d-4cbe-b228-9b0979e810fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with num_beams=50\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, num_beams=50, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2eb5a-62d1-4fe7-ad12-4b68eeed8c71",
   "metadata": {},
   "source": [
    "Hmm, so I tried it with both 100 and 50 and neither seems to be working. It appears that something is timing out? I'm not sure. The model is stored locally so I'm not really sure why something would be timing out when I'm not doing inference over the cloud. Maybe it's finding that the most probably result with a beam width of 50 is to just end the sentence? No clue. Let me know what y'all think if you have an idea.\n",
    "\n",
    "Let's move onto nucleus sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c95a9-df44-4261-9a13-77d75ab016df",
   "metadata": {},
   "source": [
    "## Nucleus Sampling\n",
    "\n",
    "Nucleus sampling aims to work off of top-k sampling by also removing the tail of the distribution. However, their insight was that top-k sampling has to guess what an appropriate value of k has to be so that the tail of the distribution is appropriately removed. Let me ask you: what should k be to remove the tail of the distribution? Does this make sense when the distribution is changing at each step in the token generation sequence? Well, top-p sampling chooses a probability and the the top p tokens (chosen by summing the top most probable tokens) are kept and the rest are discarded. This is a better method for removing the tail of the distribution.\n",
    "\n",
    "Let's try our example from the paper with top-p equal to 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8fa318e-1a0f-4501-a58e-66e7aa02b0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\"They also were completely uninhibited in their behaviors, and even though they never came across a human, their eyes were very bright.\n",
      "\n",
      "\"The results were surprising, since humans often talk so fast, which would have caused them to learn a lot,\" explained Peter Vollrath, co-author of the paper and a researcher in the National Science Foundation's Center for Scientific Computing. \"This is consistent with a common view that unicorns often learn languages because they are familiar with a language that is not their native language.\"\n",
      "\n",
      "The results will be used to study how humans learn the language of other species such as chimpanzees and other apes. The results may also allow scientists to look for similarities between unicorns and other primates such\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with top_p=0.9\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, do_sample=True, top_p=0.9, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3206331c-3068-478c-9339-6ec28b126d70",
   "metadata": {},
   "source": [
    "Huh, so to my surprise this output was pretty nonsensical. In fact, I think it's keeping too much of the tail when it shouldn't be! Let's try decreasing the top-p value to 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "563fca23-9900-450e-aa5d-5136d1c50d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\"They also were very nice to humans,\" says Michael R. Anderson, a University of California, Davis professor of linguistics and language studies and the study's lead author.\n",
      "\n",
      "The unicorns are among a handful of rare species that are believed to have lived on the Andes Mountains for thousands of years.\n",
      "\n",
      "In 2012, researchers found a group of seven rare unicorns living in the remote and isolated valley of Cauco de la Hoya, Bolivia. It was found that they were all from the same family — the civets.\n",
      "\n",
      "\"We have not seen the civets since their discovery, but they are quite close to us in this valley,\" Anderson says.\n",
      "\n",
      "The civets live\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with top_p=0.75\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, do_sample=True, top_p=0.75, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc194f-7bae-408f-9bb1-a54baf933f1e",
   "metadata": {},
   "source": [
    "This does seem to be performing better! I was impressed that it got Bolivia and the Andes to be geographically related, though \"Cauco de la Hoya\" is not a real place. What if we do top_p=0.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ac399f0-0d61-47b5-9b2e-791c88614f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\"They were very, very clever and very intelligent,\" said the researcher, who has no prior knowledge of unicorns. \"I think that's why they are so well-known in the region.\"\n",
      "\n",
      "In a paper published in the journal Nature Communications, the researchers describe the study as a breakthrough in the field of unicorns.\n",
      "\n",
      "\"The discovery of this rare, rare, and unique species is a step forward in the search for the true origins of the language,\" said lead author and professor of genetics and evolutionary biology Dr. James A. Waddell, who is also a member of the team. \"This is a great example of how a species can be studied in isolation and used to help guide new discoveries in evolutionary\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with top_p=0.5\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, do_sample=True, top_p=0.5, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf179de-bea5-49b3-91e5-55213506fbed",
   "metadata": {},
   "source": [
    "So now we're entering the regime of repetition and likely we're cutting off too much of the distribution, way past the tail, and approaching greedy search territory. This makes sense hypothetically and we're seeing those results here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce9b358-7b90-49b9-8585-d84cd8b07c1d",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "Temperature is used to reshape the logits output of the model. The limit of temperature being set to 0 results in the probability of the most-likely token to be 1 and everything else to be zero, which is basically just greedy search. The limit of temperature being infinity sets the logits to be uniformly distributed. Finally, a temperature of 1 results in the regular softmax distribution. The paper describes low temperature as being useful in top-k where the distribution is relatively flat (see the paper for a nice figure) and a higher temperature where the distribution is relatively peaked (really, the paper has a nice figure, [figure 5](https://arxiv.org/pdf/1904.09751.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff4679-8bf6-4969-ba97-5560c9ce8aa7",
   "metadata": {},
   "source": [
    " Let's use temperature with top-k to demonstrate how it can have a positive effect on the generation process. In the next code cell I redo top-k with $k=20$ just so y'all don't have to scroll up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5cb297c-d819-4f74-b0fa-5f8d3a194a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\"They also were completely literate on language and grammar; they were very happy with their food and had even taken to eating human food,\" said the study's lead author, David E. Fuchs, Ph.D., an assistant professor of anthropology at the University of California, Santa Barbara and co-author of the paper. Fuchs was not immediately available for comment on the findings, but the discovery is consistent with a similar study conducted in the same region in 2012. It was published in Science in January, the same month that the first U.S. unicorns were sighted in the mountains.\n",
      "\n",
      "The discovery also raises some fundamental questions about how the U.S. government's efforts to help indigenous peoples in Mexico —\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with top_k=20\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, do_sample=True, top_k=20, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac3687-70a7-48bf-bde5-dfe3c375558b",
   "metadata": {},
   "source": [
    "Then, I set the temperature to be $0.9$, to see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92c37581-2f21-421f-9ef9-51d3abc5a066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\"They were a bit overindicated,\" says Michael R. Feltman, an expert in language cognition and linguistics at UC San Diego. \"Some were very hard to read. Others were very well-behaved at times. They spoke very intelligible English.\"\n",
      "\n",
      "In fact, the unicorns were able to read a lot of things, including English, according to the study, published today (December 7) in the Proceedings of the National Academy of Sciences. It all began with a call from the researcher's wife who, she says, was so overwhelmed with her own language that she could not speak it to her daughter in a regular language training class.\n",
      "\n",
      "Feltman says the unicorns were extremely receptive to\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with top_k=20 and temperature=0.9\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, do_sample=True, top_k=20, temperature=0.9, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd8f3b-19ee-433f-bb4b-6d63a1e2eb81",
   "metadata": {},
   "source": [
    "Okay! So this is a completely different set of generated sentences and I found it to be slightly less coherent. I do not know what overindicated means (it doesn't appear to be a real word). So, my intuition is telling me that the original probability distribution of the top 20 words was more peaked than flat, meaning a larger temperature than 1.0 should help with generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b94532a5-5b6f-4851-808f-6f605ab1bd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\"They also were completely literate on language and grammar; in fact, most of them were speaking perfect English while they studied,\" says Richard J. Doss, a professor of botany at the University of Arizona who was not involved in the study.\n",
      "\n",
      "When the researchers looked through more than 20,000 letters, the unicorns were most closely related to the English speaking unicorns. For example, three of the most commonly used terms were from the common name \"Carnivore,\" while \"Dromer\" from the name Carnivore was used to describe wild, small wild animals.\n",
      "\n",
      "The scientists also discovered that, as in previous research in this region, the unicorns also had a very high level of\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output with top_k=20 and temperature=0.9\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=150, do_sample=True, top_k=20, temperature=1.1, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d372fbe-f39e-4508-bdbc-4728c9b6918d",
   "metadata": {},
   "source": [
    "Once again, the temperature didn't seem to help that much in reducing the nonsense in the output. However, it is clear that temperature *does* have an effect on output and may have its use cases where it benefits the output.\n",
    "\n",
    "## Miscellaneous\n",
    "\n",
    "I also wanted to spend some time playing around with GPT2! So here's some cell blocks that are just for fun. You don't have to read this section if you don't want to, the main ideas were above.\n",
    "\n",
    "Here, I try to get GPT2 to give me a nickname that rhymes with Alexey. I start out with top_p=0.95. and generate 10 independently generated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b570514b-340d-4473-b9d7-420182a48da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0:\n",
      "My name is Alexey and my nickname is Lillian. I've been playing video games since I was 2 years old\n",
      "---------------------------\n",
      "Output 1:\n",
      "My name is Alexey and my nickname is C-L. In this game, we've got a lot of fun\n",
      "---------------------------\n",
      "Output 2:\n",
      "My name is Alexey and my nickname is C3VIC,\" he says in the video, which he uploaded as\n",
      "---------------------------\n",
      "Output 3:\n",
      "My name is Alexey and my nickname is Alexey,\" he said.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "Alexey grew up\n",
      "---------------------------\n",
      "Output 4:\n",
      "My name is Alexey and my nickname is 'D' here. So you are going to die to see who gets\n",
      "---------------------------\n",
      "Output 5:\n",
      "My name is Alexey and my nickname is Alexei\" is now the highest selling and most watched TV show in the\n",
      "---------------------------\n",
      "Output 6:\n",
      "My name is Alexey and my nickname is \"The Kitten.\" You know what I mean. You know what I\n",
      "---------------------------\n",
      "Output 7:\n",
      "My name is Alexey and my nickname is \"Pegasus.\" In a few minutes, I'm gonna start to\n",
      "---------------------------\n",
      "Output 8:\n",
      "My name is Alexey and my nickname is Pawn. I can do a lot better than what I do. I\n",
      "---------------------------\n",
      "Output 9:\n",
      "My name is Alexey and my nickname is the Blacktop. I have been writing these jokes in English since college.\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=15, do_sample=True, top_p=0.95, num_return_sequences=10, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"My name is Alexey and my nickname is\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(output[i][\"generated_text\"])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831b1b7-d2c7-4807-87e9-c50381b4a11b",
   "metadata": {},
   "source": [
    "This isn't very helpful LOL, let's try giving it some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d1669325-ee55-4512-aa85-303eddb34565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0:\n",
      "My name is Jack and my nickname is Jack Black. My name is Fred and my nickname is Fred the bed head. My name is Alexey and my nickname is Alexey the bard.\"\n",
      "\n",
      "He was born and raised in Michigan\n",
      "---------------------------\n",
      "Output 1:\n",
      "My name is Jack and my nickname is Jack Black. My name is Fred and my nickname is Fred the bed head. My name is Alexey and my nickname is Alexey the baby cat. My name is Joe and my nickname is Joe\n",
      "---------------------------\n",
      "Output 2:\n",
      "My name is Jack and my nickname is Jack Black. My name is Fred and my nickname is Fred the bed head. My name is Alexey and my nickname is Alexey the cat head. My name is Michael and my nickname is Michael\n",
      "---------------------------\n",
      "Output 3:\n",
      "My name is Jack and my nickname is Jack Black. My name is Fred and my nickname is Fred the bed head. My name is Alexey and my nickname is Alexey the bed head.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "3. There is\n",
      "---------------------------\n",
      "Output 4:\n",
      "My name is Jack and my nickname is Jack Black. My name is Fred and my nickname is Fred the bed head. My name is Alexey and my nickname is Alexey the baby. My name is James and my nickname is James the\n",
      "---------------------------\n",
      "Output 5:\n",
      "My name is Jack and my nickname is Jack Black. My name is Fred and my nickname is Fred the bed head. My name is Alexey and my nickname is Alexey the nurse. My name is Edmond and my nickname is Ed\n",
      "---------------------------\n",
      "Output 6:\n",
      "My name is Jack and my nickname is Jack Black. My name is Fred and my nickname is Fred the bed head. My name is Alexey and my nickname is Alexey the dragon. My name is Johnny, my nickname is Johnny the\n",
      "---------------------------\n",
      "Output 7:\n",
      "My name is Jack and my nickname is Jack Black. My name is Fred and my nickname is Fred the bed head. My name is Alexey and my nickname is Alexey the bedhead. My name is Bem and my nickname is\n",
      "---------------------------\n",
      "Output 8:\n",
      "My name is Jack and my nickname is Jack Black. My name is Fred and my nickname is Fred the bed head. My name is Alexey and my nickname is Alexey the bed head. My name is David and my nickname is David\n",
      "---------------------------\n",
      "Output 9:\n",
      "My name is Jack and my nickname is Jack Black. My name is Fred and my nickname is Fred the bed head. My name is Alexey and my nickname is Alexey the couch head. My name is Al in the basement and my\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=15, do_sample=True, top_p=0.95, num_return_sequences=10, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"My name is Jack and my nickname is Jack Black. My name is Fred and my nickname is Fred the bed head. My name is Alexey and my nickname is\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(output[i][\"generated_text\"])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc4f84-e2d2-4bc8-9cb1-6dad6daa8db2",
   "metadata": {},
   "source": [
    "I like this more, but I wonder if I can get it to rhyme. Apparently the previous examples really influence what it gave me, seeing as how lots of the previous outputs use `head` in the nickname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7be74785-b68b-4436-a108-39f7f0f45235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0:\n",
      "Jack is short for Jack Black. Fred is short for Fred the bed head. Alexey is short for Alexei.\n",
      "---------------------------\n",
      "Output 1:\n",
      "Jack is short for Jack Black. Fred is short for Fred the bed head. Alexey is short for Alexey the headdress.\n",
      "\n",
      "Funny and a bit creepy,\n",
      "---------------------------\n",
      "Output 2:\n",
      "Jack is short for Jack Black. Fred is short for Fred the bed head. Alexey is short for Alexey the cat head. Mikey is short for Mikey the cat\n",
      "---------------------------\n",
      "Output 3:\n",
      "Jack is short for Jack Black. Fred is short for Fred the bed head. Alexey is short for Alexey the head.\n",
      "\n",
      "Hilariously awkward for a comic book\n",
      "---------------------------\n",
      "Output 4:\n",
      "Jack is short for Jack Black. Fred is short for Fred the bed head. Alexey is short for Alexey the house head. He also has a number of nicknames and\n",
      "---------------------------\n",
      "Output 5:\n",
      "Jack is short for Jack Black. Fred is short for Fred the bed head. Alexey is short for Alexei the doll head. Ben is short for Ben the cat head.\n",
      "---------------------------\n",
      "Output 6:\n",
      "Jack is short for Jack Black. Fred is short for Fred the bed head. Alexey is short for Alexey the dragon. Ronny is short for Ronny the red dragon\n",
      "---------------------------\n",
      "Output 7:\n",
      "Jack is short for Jack Black. Fred is short for Fred the bed head. Alexey is short for Alexey the owl. In a few other places there are names like Alex\n",
      "---------------------------\n",
      "Output 8:\n",
      "Jack is short for Jack Black. Fred is short for Fred the bed head. Alexey is short for Alexey. Nick is short for Nicky the couch head.\n",
      "\n",
      "\n",
      "---------------------------\n",
      "Output 9:\n",
      "Jack is short for Jack Black. Fred is short for Fred the bed head. Alexey is short for Alexey the couch and the bed. Alexey is short for Alexey\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=15, do_sample=True, top_p=0.95, num_return_sequences=10, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"Jack is short for Jack Black. Fred is short for Fred the bed head. Alexey is short for\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(output[i][\"generated_text\"])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bd837-a3fc-4394-8509-a67171f8295a",
   "metadata": {},
   "source": [
    "Maybe I need to give it many more examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "51404297-d38d-4fa9-b223-f158602e6f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexei the bard. Charlie is short for Charlie the chimp.\n",
      "---------------------------\n",
      "Output 1:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey the monkey. Lillian is short for Lillian the horse.\n",
      "---------------------------\n",
      "Output 2:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey the cat. Kelly is short for Kelly the cat. Sam is\n",
      "---------------------------\n",
      "Output 3:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey the Dog. Jason is short for Jason the fawn. Ken\n",
      "---------------------------\n",
      "Output 4:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey spitting fax. Chris is short for Chris spitting fax. Brian is\n",
      "---------------------------\n",
      "Output 5:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexei the eidolon. Javi is short for Javi the\n",
      "---------------------------\n",
      "Output 6:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey the troll. Paul is short for Paul the pimp. Mike\n",
      "---------------------------\n",
      "Output 7:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey shooting and taking photos. Alexey and Max have been together for\n",
      "---------------------------\n",
      "Output 8:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey screaming at a lady at a local strip club. Adam is short\n",
      "---------------------------\n",
      "Output 9:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey the wolfhound.\n",
      "\n",
      "1.1m: \"H\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=15, do_sample=True, top_p=0.95, num_return_sequences=10, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"Jack is short for Jack the Jacked. Fred is short for Fred the bed head. \" +\\\n",
    "                   \"Keith is short for Keith the Thief. John is short for John the fawn. \" +\\\n",
    "                   \"Claire is short for Claire the Bear. Blake is short for Blake the drake. \" +\\\n",
    "                   \"Greg is short for Greggy peggy. Max is short for Max spitting fax. \" +\\\n",
    "                   \"Alexey is short for\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(output[i][\"generated_text\"])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404783c5-4d4c-4ae6-b622-b289893a1255",
   "metadata": {},
   "source": [
    "Hmm, not quite. Though `Alexey screaming at a lady at a local strip club` is funny. Let's try changing the sampling method. to top_k=20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7701cf27-a32b-47b6-9353-af14d41cdbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexei the bard.\n",
      "\n",
      "The following characters were added to the\n",
      "---------------------------\n",
      "Output 1:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey the dork. Mike is short for Mike the hottie\n",
      "---------------------------\n",
      "Output 2:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey the cat. Mike is short for Mike the cat. Sam is\n",
      "---------------------------\n",
      "Output 3:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey the Dog.\n",
      "\n",
      "(This list doesn't include the ones\n",
      "---------------------------\n",
      "Output 4:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey spitting fax. Chris is short for Chris spitting fax. Joe is\n",
      "---------------------------\n",
      "Output 5:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexei the Fawn. Ben is short for Ben the Fawn.\n",
      "---------------------------\n",
      "Output 6:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey the pig. Sam has short for Sam playing the piano. Mike\n",
      "---------------------------\n",
      "Output 7:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey the Fawn. Jack the Jack. Max was short for Max\n",
      "---------------------------\n",
      "Output 8:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey screaming at a boy at school. The Doctor is short for the\n",
      "---------------------------\n",
      "Output 9:\n",
      "Jack is short for Jack the Jacked. Fred is short for Fred the bed head. Keith is short for Keith the Thief. John is short for John the fawn. Claire is short for Claire the Bear. Blake is short for Blake the drake. Greg is short for Greggy peggy. Max is short for Max spitting fax. Alexey is short for Alexey the wolfhound.\n",
      "\n",
      "The name of the game is simple\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=15, do_sample=True, top_k=20, num_return_sequences=10, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"Jack is short for Jack the Jacked. Fred is short for Fred the bed head. \" +\\\n",
    "                   \"Keith is short for Keith the Thief. John is short for John the fawn. \" +\\\n",
    "                   \"Claire is short for Claire the Bear. Blake is short for Blake the drake. \" +\\\n",
    "                   \"Greg is short for Greggy peggy. Max is short for Max spitting fax. \" +\\\n",
    "                   \"Alexey is short for\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(output[i][\"generated_text\"])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee5a77e-c536-4f7b-8234-8391cb9e088e",
   "metadata": {},
   "source": [
    "I like `Alexey the dork` since it seems like GPT2 is making fun of me lol. Let's try changing the input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77a0ba92-9f2a-4c9d-b45e-2c885d793345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with a big ol' grin.\n",
      "\n",
      "I was born and raised in the\n",
      "---------------------------\n",
      "Output 1:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with jock.\n",
      "\n",
      "[1:14]\n",
      "\n",
      "[3:\n",
      "---------------------------\n",
      "Output 2:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with cuddly. Michael rhymes with jingle ball. Dax rh\n",
      "---------------------------\n",
      "Output 3:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with jaded. Bob rhymes with jingle bells. Nickle rhymes\n",
      "---------------------------\n",
      "Output 4:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with pimp. The Beatles rhyme with drowsy. Dandy rh\n",
      "---------------------------\n",
      "Output 5:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with cuckoo. John rhymes with kennel. Max rhymes\n",
      "---------------------------\n",
      "Output 6:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with blueberry.\n",
      "\n",
      "And in the final, final, final, final\n",
      "---------------------------\n",
      "Output 7:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with kennan and berry. Alexey rhymes with jacked.\n",
      "---------------------------\n",
      "Output 8:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with dog.\n",
      "\n",
      "The following lyrics are taken from \"The Art of the\n",
      "---------------------------\n",
      "Output 9:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with jolly. James rhymes with jackson. Jodie rhymes\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=15, do_sample=True, top_k=20, num_return_sequences=10, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"Jack rhymes with jacked. Fred rhymes with bed head. \" +\\\n",
    "                   \"Keith rhymes with thief. John rhymes with fawn. \" +\\\n",
    "                   \"Claire rhymes with bear. Blake rhymes with drake. \" +\\\n",
    "                   \"Greggy rhymes with peggy. Max rhymes with facts. \" +\\\n",
    "                   \"Alexey rhymes with\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(output[i][\"generated_text\"])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7621d0-2004-4975-b263-2ba27aea67b1",
   "metadata": {},
   "source": [
    "We're close! I see `Alexey rhymes with kennan and berry` and `Alexey rhymes with blueberry`, which are *actual* rhymes lol. Let's play with temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50a6b155-7348-4fa2-ab14-820587dc8402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with giddy bird. Azzin raps on your forehead! Yup\n",
      "---------------------------\n",
      "Output 1:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with jock.\n",
      "\n",
      "[20:33] So when was it recorded\n",
      "---------------------------\n",
      "Output 2:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with cutesy cat man. Mikey jokingly makes cat noises. J\n",
      "---------------------------\n",
      "Output 3:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with jaded or madder.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "3) There isn\n",
      "---------------------------\n",
      "Output 4:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with pimp!\n",
      "---------------------------\n",
      "Output 5:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with big brother. James sings about a horse and horse hoards with rhymes\n",
      "---------------------------\n",
      "Output 6:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with blueberry pie\n",
      "\n",
      "---------------------------\n",
      "Output 7:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with kennan and benny. Alexa.rhymes with dana\n",
      "---------------------------\n",
      "Output 8:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with dogmatic. Nicko rhymes with hiker. Michael rhymes with\n",
      "---------------------------\n",
      "Output 9:\n",
      "Jack rhymes with jacked. Fred rhymes with bed head. Keith rhymes with thief. John rhymes with fawn. Claire rhymes with bear. Blake rhymes with drake. Greggy rhymes with peggy. Max rhymes with facts. Alexey rhymes with the devil. James rhymes with jackson. Jodyn waffles\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT2 output\n",
    "set_seed(42)\n",
    "config = GenerationConfig(max_new_tokens=15, do_sample=True, top_k=20, temperature=2.0, num_return_sequences=10, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "generator = pipeline('text-generation', model='gpt2', generation_config=config)\n",
    "output = generator(\"Jack rhymes with jacked. Fred rhymes with bed head. \" +\\\n",
    "                   \"Keith rhymes with thief. John rhymes with fawn. \" +\\\n",
    "                   \"Claire rhymes with bear. Blake rhymes with drake. \" +\\\n",
    "                   \"Greggy rhymes with peggy. Max rhymes with facts. \" +\\\n",
    "                   \"Alexey rhymes with\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(output[i][\"generated_text\"])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e19454-0e4a-4866-bce6-128aba45b82d",
   "metadata": {},
   "source": [
    "So changing the temperature to 2.0 caused a lot more adult results, like `Alexey rhymes with the devil` and `Alexey rhymes with pimp!`. Though I did see `Alexey rhymes with cutesy cat man` and `Alexey rhymes with giddy bird`. \n",
    "\n",
    "I'll take `Alexey rhymes with cutesy cat man` as a win.\n",
    "\n",
    "Please only refer to me now as \"Cutesy Cat Man\" (... I'm kidding lol)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdec6d6-ba03-494e-bb39-f0aef500d909",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Overall, I really enjoyed making this report. This type of exploration where I'm playing with the output of the model is in stark contrast to my [previous report](https://github.com/yyexela/APPM5720-Reports/blob/master/Report12/April5.ipynb) where it was much more model development and training heavy, so it was a nice change of pace. Understanding the types of generation methods, like greedy search, beam search, top-k, and top-p is not difficult to understand, so going through these examples to build some intuition on how it actually changes results was nice.\n",
    "\n",
    "Thanks for reading!\n",
    "\n",
    "*- Alexey the cutesy cat man*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267d12b-5ee4-4477-b145-5386296e3a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-thesis] *",
   "language": "python",
   "name": "conda-env-.conda-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
