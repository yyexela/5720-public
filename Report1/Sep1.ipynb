{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPM 5720 Biweekly Report\n",
    "### *Alexey Yermakov*\n",
    "### *September 1, 2022*\n",
    "\n",
    "# Summary\n",
    "I am already very comfortable with using git and GitHub, so I spent the past week and a half with Tensorflow as well as getting my development environment set up for the class.\n",
    "\n",
    "# Main Content\n",
    "\n",
    "## August 24:\n",
    " - This Summer I was messing around with Maziar's [pinned repositories](https://github.com/maziarraissi) in GitHub. In doing so, I was able to figure out which software I need installed locally when running Tensorflow. I made a [pull request](https://github.com/maziarraissi/DeepHPMs/pull/3) in one of his repositories explaining the [software](https://github.com/maziarraissi/DeepHPMs/blob/4cb7a4d33e4315ed20c091b91e2f1298e37133e2/README.md) that needs to be installed and the versions. Note that my pull request has older versions of the software listed since Maziar's code is based on an older version of Tensorflow. I was able to download this older version of the software using [Anaconda](https://www.anaconda.com/). Today, my goal was to get the latest version of Tensorflow working locally. Trying Anaconda again didn't lead to a successful installation, so I used [Docker](https://docs.docker.com/get-docker/) instead, since it is [recommended](https://www.tensorflow.org/install/docker) by Tensorflow as the easiest way to get GPU support.\n",
    "\n",
    " - The installation process went as follows for me on my desktop running Arch Linux with an `Intel I5-6600K` processor and `NVIDIA GeForce GTX 1070` GPU:\n",
    "   - First, I installed Docker by running `sudo pacman -S docker`.\n",
    "   - Then, I started the Docker daemon by running `sudo systemctl start docker.service`. Note that this needs to be run every time I restart my computer, but if I want it to start on boot I would instead run `sudo systemctl enable --now docker.service`.\n",
    "   - I already had Nvidia's proprietary drivers installed on my machine, but instructions can be found [here](https://wiki.archlinux.org/title/NVIDIA). I verified this was the case by running `lsmod | grep nvidia` and observed the `nvidia` kernel module was loaded.\n",
    "   - I installed the [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker) by installing [libnvidia-container-tools](https://aur.archlinux.org/packages/libnvidia-container-tools) and then [nvidia-container-toolkit](https://aur.archlinux.org/packages/nvidia-container-toolkit) from the AUR (Arch User Repository).\n",
    "   - I added myself (`[user]`) to the `docker` group so that I don't need to be root to run docker commands by running `usermod -a -G docker [user]` (note: you will need to either reboot to have the group appear in the `groups` command or you can run `su - [user]`).\n",
    "   - I then pulled a docker image with GPU and Jupyter notebook support by running `docker pull tensorflow/tensorflow:latest-gpu-jupyter`.\n",
    "   - Finally, I had to do a lot of tinkering with the command to start the container so that I could access Jupyter from within the container. The command is:\\\n",
    "   `docker run -u $(id -u):$(id -g) -v '[Host]':'[Guest]' --network host --name [Name] --gpus all tensorflow/tensorflow:latest-gpu-jupyter`\n",
    "   - Where, \n",
    "     - `docker run ... tensorflow:latest-gpu-jupyter` starts the container.\n",
    "     - `-u $(id -u):$(id -g)` runs the container as the current user (to avoid running as root)\n",
    "     - `-v '[Host]':'[Guest]'` mounts the absolute path `[Host]` on my computer to the absolute path `[Guest]` in the container. This allows me to work out of a permanent folder, since restarting the container will reset all files which aren't external and I'll lose all of my progress.\n",
    "     - `--network host` makes the container's networking the same as my host machine, allowing me to access the Jupyter container easily. I think there's a better way to do this but I couldn't figure it out.\n",
    "     - `--gpus all` lets the container access my GPU (this is why the NVIDIA Container Toolkit was installed).\n",
    "     - `--name [Name]` names the container `Name`. This is useful for setting up an alias to join the docker container since you can expect the name to be the same, otherwise the name is randomized.\n",
    "   - This then allows me to access the Jupyter server running in my container in my computer's browser.\n",
    "   - I can access the container via bash by running `docker exec -it [Name] bash` where `[Name]` is the container's name, obtained from inspecting `docker container ls` (or set from using the `--name [Name] option from above`).\n",
    "   - Running one of the tutorial notebooks in the container and observing my graphics card usage with `nvtop` showed me the graphics card was correctly being used in Tensorflow.\n",
    "   - To run on CPU only, remove `--gpus all` and change `tensorflow:latest-gpu-jupyter` to `tensorflow:latest-jupyter`. I followed my own guide to successfully install tensorflow on my laptop.\n",
    "\n",
    "## August 25\n",
    " - The [Tensorflow website](https://www.tensorflow.org/install/docker) I followed contains a link to [Tensorflow tutorials](https://www.tensorflow.org/tutorials). Today, I started working through them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tutorial 1](https://www.tensorflow.org/guide/keras/sequential_model): `sequential_model.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Running the notebook gave me a GPU related error (sorry, forgot to copy the error message `sad face`). Some quick [internet searching](https://www.tensorflow.org/guide/gpu) led me to the following code snippet which resolved my problem:\n",
    "```python\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "```\n",
    " - I also had to `restart` another notebook's kernel which was using a lot of my GPU's memory. Though I can see this setting is limited my GPU usage, it isn't precise... The notebook uses 1439 MiB whereas the limit is 1024 MB (~977 MiB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "## Setup for tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024*4)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The first thing this notebook does is define a \"Sequential Model\", which looks something like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/yyexela/5720-public/master/Report1/Images/SequentialModel.jpg\" alt=\"Diagram of a sequential model\" width=\"700\"/>\n",
    "\n",
    " - Where \"A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\" The image shows a \"dense\" neural network model, where each node in one layer is connected to every other node in neighboring layers. This is the kind of network that is created in this tutorial, implying there are non-dense and non-stack neural networks as well. Note that the exact network in the notebook is 3 layers of sizes 2, 3, and 4 respectively with the first 2 layers having the \"rectified linear unit activation function\" (\"relu\" for short).\n",
    " - An [activation function](https://deepai.org/machine-learning-glossary-and-terms/activation-function) is used to introduce nonlinearity to a neural network, giving them the ability to \"learn\" complex tasks that you wouldn't necessarily be able to learn without it. The link in the previous sentence claims that neural networks without activation functions can be decomposed into a single matrix acting on the input, which is really cool and I believe the proof is simple: every layer is effectively a matrix, so all layers can be matrix-multiplied together to get a single matrix acting on input and resulting in an output.\n",
    " - A diagram showing a \"feed-forward\" neural network is obtained from [here](https://deepai.org/machine-learning-glossary-and-terms/activation-function):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/yyexela/5720-public/d17b2794fdb4d7f4a1f1260765d14804f84ea415/Report1/Images/FeedForward.svg\" alt=\"Diagram of a feed forward neural network\" width=\"700\"/>\n",
    "\n",
    " - The curvy line in a circle is the activation function. Take a look at layer 2. Each neuron is obtaining the values of the previous layer's neurons after being passed in the previous layer's activation function. Layer 2's neurons then take these values and compute their values with the following function:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/yyexela/5720-public/master/Report1/Images/NeuronFunction.jpg\" alt=\"Formula for a neuron\" width=\"300\"/>\n",
    "\n",
    " - Where the function `f` is the activation function and `B` is some bias. So each successive neuron takes the values of the previous layer's neurons to dtermine it's own value. Though it looks like [Keras'](https://keras.io/api/layers/core_layers/dense/) `Dense` layer puts the bias inside the activation function:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/yyexela/5720-public/master/Report1/Images/NeuronFunction2.jpg\" alt=\"Formula for a neuron\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.dense.Dense at 0x7fa6104cc670>,\n",
       " <keras.layers.core.dense.Dense at 0x7fa6104cc4f0>,\n",
       " <keras.layers.core.dense.Dense at 0x7fa6104cc490>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 1 to define the model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2, activation=\"relu\"),\n",
    "        layers.Dense(3, activation=\"relu\"),\n",
    "        layers.Dense(4),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.dense.Dense at 0x7fa5b02dd790>,\n",
       " <keras.layers.core.dense.Dense at 0x7fa6105777f0>,\n",
       " <keras.layers.core.dense.Dense at 0x7fa5b02ddf10>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2 to define the model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(2, activation=\"relu\"))\n",
    "model.add(layers.Dense(3, activation=\"relu\"))\n",
    "model.add(layers.Dense(4))\n",
    "\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# The last layer added can be removed\n",
    "model.pop()\n",
    "print(len(model.layers))  # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers and models can be named\n",
    "model = keras.Sequential(name=\"my_sequential\")\n",
    "model.add(layers.Dense(2, activation=\"relu\", name=\"layer1\"))\n",
    "model.add(layers.Dense(3, activation=\"relu\", name=\"layer2\"))\n",
    "model.add(layers.Dense(4, name=\"layer3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (1, 2)                    10        \n",
      "                                                                 \n",
      " layer2 (Dense)              (1, 3)                    9         \n",
      "                                                                 \n",
      " layer3 (Dense)              (1, 4)                    16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35\n",
      "Trainable params: 35\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summaries about models can also be printed once the model is \"built\"\n",
    "#   (in this case, this means the weights have been defined, which is a result of \n",
    "#    calling the model with input)\n",
    "x = tf.ones((1,4))\n",
    "y = model(x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## August 28\n",
    "  - The Keras API documentation can be found [here](https://keras.io/api/).\n",
    "  - Continuing through the tutorial was mostly straight forward, but I had trouble parsing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_6/kernel:0' shape=(4, 3) dtype=float32, numpy=\n",
       " array([[-0.19412315,  0.025047  , -0.2495017 ],\n",
       "        [ 0.8338394 , -0.693617  , -0.05902171],\n",
       "        [-0.3508405 ,  0.64317787,  0.5348314 ],\n",
       "        [ 0.7090317 , -0.50605094, -0.10464364]], dtype=float32)>,\n",
       " <tf.Variable 'dense_6/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = layers.Dense(3)\n",
    "layer.weights\n",
    "x = tf.ones((1, 4))\n",
    "y = layer(x)\n",
    "layer.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - What this code is doing is first making a `Dense` layer with `3` neurons. This layer initially has no weights associated with it, they are only defined after a tensor is passed into it. Then, a `(1,4)` sized-tensor is passed into the layer, resulting in the weights to be of size `(4,3)`, since the layer now knows the input size and output size, so by basic matrix multiplication we see that `(1,4)x(4,3)=(1,3)`, which is the number of neurons in the layer (and the number of generated weights).\n",
    " - If you were curious like me, you might wonder how the weights are randomized? It turns out that `kernel_initializer` value when creating the `Dense` layer controls that. By default, it is the `GlorotUniform` initializer (see [here for `GlorotUniform`](https://keras.io/api/layers/initializers/#glorotuniform-class) and [here for `Dense` layer initialization](https://keras.io/api/layers/core_layers/dense/)). Biases are randomly selected as well, and the way they are selected can be found by looking at `Dense` layer class as well. The weights can also be manually set with [the `set_weights` method](https://keras.io/api/layers/base_layer/#set_weights-method). The call to `layer(x)` produces random weights once and then the weights are the same from then on unless a new layer is created or the weights are manually changed.\n",
    " - The rest of the tutorial seemed much more technical that I'm comfortable with so far, so I wasn't able to follow along very well despite reading it all. I will revisit it later as I learn more about Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## August 29\n",
    " ### [Tutorial 2](https://www.tensorflow.org/guide/basics): `basics.ipynb`\n",
    "  - The first bit of code threw me for a loop:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
      "(2, 3)\n",
      "<dtype: 'float32'>\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1., 2., 3.],\n",
    "                 [4., 5., 6.]])\n",
    "\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(x.dtype)\n",
    "print(x[0][1]) # We can get individual elements\n",
    "# print(x.op) # But we can't get the entire array (THIS WILL FAIL), but maybe it doesn't matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - This shows us how a constant tensor is created and extracts its parameters. I wanted to extract the tensor's values, not just the parameters. So, I checked [this documentation](https://www.tensorflow.org/api_docs/python/tf/Tensor) and tried `print(x.op)`, but it turns out this is not allowed \"when eager execution is enabled\". What is \"eager execution\"? Well, [this StackOverflow page](https://stackoverflow.com/questions/58112355/what-exactly-is-eager-execution-from-a-programming-point-of-view) explains it pretty well, but it doesn't help me understand *why* I can't just pull the array out of the tensor. Instead, numpy can be used in this case: `np.array(x)` will return a numpy array of the tensor's data. Thanks Google (developers of Tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "\n",
      "Numpy element\n",
      "2.0\n",
      "\n",
      "Tensorflow element\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arr = np.asarray(x)\n",
    "\n",
    "print(\"Numpy array\")\n",
    "print(arr)\n",
    "print()\n",
    "\n",
    "print(\"Numpy element\")\n",
    "print(arr[0][1]) # First row, second element of numpy array\n",
    "print()\n",
    "\n",
    "print(\"Tensorflow element\")\n",
    "print(x[0][1]) # First row, second element of tensorflow tensor\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix addition\n",
    "x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 20.,  40.,  60.],\n",
       "       [ 80., 100., 120.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element-wise multiplication\n",
    "20 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
      "\n",
      "Matrix B:\n",
      "tf.Tensor(\n",
      "[[1. 4.]\n",
      " [2. 5.]\n",
      " [3. 6.]], shape=(3, 2), dtype=float32)\n",
      "\n",
      "A*B:\n",
      "tf.Tensor(\n",
      "[[14. 32.]\n",
      " [32. 77.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Showing the two matrices being multiplied to manually verify matrix multiplication\n",
    "print(\"Matrix A:\")\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "print(\"Matrix B:\")\n",
    "print(tf.transpose(x))\n",
    "print()\n",
    "\n",
    "# Matrix multiplication\n",
    "print(\"A*B:\")\n",
    "print(x @ tf.transpose(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Side by side:\n",
      "tf.Tensor(\n",
      "[[1. 2. 3. 1. 2. 3. 1. 2. 3. 1. 2. 3.]\n",
      " [4. 5. 6. 4. 5. 6. 4. 5. 6. 4. 5. 6.]], shape=(2, 12), dtype=float32)\n",
      "\n",
      "On top of each other:\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Different ways to concatenate matrices\n",
    "\n",
    "# Side by side\n",
    "print(\"Side by side:\")\n",
    "print(tf.concat([x, x, x, x], axis=1))\n",
    "print()\n",
    "\n",
    "# On top of each other\n",
    "print(\"On top of each other:\")\n",
    "print(tf.concat([x, x, x, x], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Here, I am also introduced to the `tf.nn.softmax` function, which [normalizes the input](https://en.wikipedia.org/wiki/Softmax_function) vector by making the output values in (0,1) and sum to 1 based on their exponentials by using the following formula for a `K`-dimensional vector:\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/yyexela/5720-public/blob/master/Report1/Images/Softmax.jpg\" alt=\"Standard softmax formula\" width=\"300\"/>\n",
    "\n",
    " - The base can be changed to any exponential, the larger it is the more weight is put on larger input values, and vice versa. Bases in `(0,1)` make smaller values larger in the output. The Tensorflow function `tf.nn.softmax` just uses `e` as the base, though (verified manually, couldn't find it in the documentation), and for tensors you need to specify the axis for the vectors (for example, in a 2D tensor you either compute `softmax` per row or per column).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax:\n",
      "tf.Tensor(\n",
      "[[0.09003057 0.24472848 0.6652409 ]\n",
      " [0.09003057 0.24472848 0.6652409 ]], shape=(2, 3), dtype=float32)\n",
      "\n",
      "Manual computation of one row:\n",
      "[0.09003057317038046, 0.24472847105479764, 0.6652409557748218]\n"
     ]
    }
   ],
   "source": [
    "# Softmax, base 'e'\n",
    "print(\"Softmax:\")\n",
    "print(tf.nn.softmax(x, axis=1))\n",
    "print()\n",
    "\n",
    "# Inspection that the values are correct:\n",
    "print(\"Manual computation of one row:\")\n",
    "print([math.e**i/(math.e**1+math.e**2+math.e**3) for i in range(1,4,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.9999998>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce_sum adds all elements of a tensor\n",
    "x_s = tf.nn.softmax(x, axis=1)\n",
    "tf.reduce_sum(x_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn an array into a Tensor\n",
    "tf.convert_to_tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Automatic differentiation is also introduced, which is just machine-precise differentiation. See:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(1.0)\n",
    "\n",
    "def f(x):\n",
    "  y = x**2 + 2*x - 5\n",
    "  return y\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = f(x)\n",
    "\n",
    "g_x = tape.gradient(y, x)  # g(x) = dy/dx\n",
    "print(g_x) # df/dx (1) is 2x + 2 evaluated at 1, which is 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - I thought this syntax was strange, especially with the `tape`, but it [turns out](https://stackoverflow.com/questions/53953099/what-is-the-purpose-of-the-tensorflow-gradient-tape) that this syntax is necessary with eager execution since originally Tensorflow wasn't really built for working with notebooks. The [with](https://stackoverflow.com/questions/1369526/what-is-the-python-keyword-with-used-for) statement is something I haven't seen before either, but it's just a fancy wrapper for `try/finally` when objects support it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Another important thing I learned is that `tf.Tensor` objects are *immutable*, once they are created they cannot be modified. Then, `tf.Variable` objects are needed, which *are* mutable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before assignment:\n",
      "Tensor:\n",
      " tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "Variable:\n",
      " <tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [3, 4]], dtype=int32)> \n",
      "\n",
      "After assignment:\n",
      "Tensor:\n",
      " tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "Variable:\n",
      " <tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[5, 5],\n",
      "       [5, 5]], dtype=int32)> \n",
      "\n",
      "After addition:\n",
      "Tensor:\n",
      " tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "Variable:\n",
      " <tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[6, 7],\n",
      "       [8, 9]], dtype=int32)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imm: immutable (Tensor)\n",
    "# mut: mutable (Variable)\n",
    "t_imm = tf.constant([[1,2],[3,4]])\n",
    "t_mut = tf.Variable([[1,2],[3,4]])\n",
    "\n",
    "print(\"Before assignment:\")\n",
    "print(\"Tensor:\\n\", t_imm, \"\\n\")\n",
    "print(\"Variable:\\n\", t_mut, \"\\n\")\n",
    "\n",
    "#t_imm[0][0] = 5 # THIS WILL FAIL (not valid syntax)\n",
    "#t_mut[0][0] = 5 # THIS WILL FAIL (not valid syntax)\n",
    "#t_imm.assign([5,5]) # THIS WILL FAIL (cannot assign to immutable Tensor)\n",
    "#t_mut.assign([5,5]) # THIS WILL FAIL (needs to be the same shape)\n",
    "t_mut.assign([[5,5],[5,5]]) # OK\n",
    "\n",
    "print(\"After assignment:\")\n",
    "print(\"Tensor:\\n\", t_imm, \"\\n\")\n",
    "print(\"Variable:\\n\", t_mut, \"\\n\")\n",
    "\n",
    "#t_imm.assign_add(1) # THIS WILL FAIL (immutable Tensor)\n",
    "#t_mut.assign_add(1) # THIS WILL FAIL (needs to be the same shape)\n",
    "t_mut.assign_add([[1,2],[3,4]]) # OK\n",
    "\n",
    "print(\"After addition:\")\n",
    "print(\"Tensor:\\n\", t_imm, \"\\n\")\n",
    "print(\"Variable:\\n\", t_mut, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC Content\n",
    "\n",
    "## Getting Latex to Work\n",
    " - For this report I wanted to be able to convert latex code to something I could put in this MarkDown report.\n",
    " - I [installed](https://wiki.archlinux.org/title/TeX_Live) the following packages using `sudo pacman -S {package name}`: `texlive-core` (for regular Latex) and `texlive-latexextra` (for `amsmath`).\n",
    " - Here is a sample Latex file I wanted to compile:\n",
    "\n",
    "```latex\n",
    "\\documentclass[]{standalone}\n",
    "\\usepackage{amsmath}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "$\\sum_{1}^{2}3$\n",
    "\n",
    "\\end{document}\n",
    "```\n",
    "\n",
    " - The `standalone` document class makes the document box only contain the formula I want to save to a file (instead of having the formula in a standard-sized page).\n",
    " - The command I use to convert from the Latex file to a jpg:\\\n",
    " `pdflatex [FILE].tex && latexmk -c && convert -density 10000 [FILE].pdf -quality 100 [FILE].jpg`\\\n",
    "   Which is just three commands glued together:\n",
    "   - `pdflatex [FILE].tex` converts the Latex file to a pdf, generating a bunch of other intermediary files.\n",
    "   - `latexmk -c` cleans up the intermediary files from the previous step.\n",
    "   - `convert -density 10000 [FILE].pdf -quality 100 [FILE].jpg` converts the pdf file to a jpg.\n",
    " - The final result is a jpg file which I can put in this markdown document!\n",
    " - Note that `convert` was probably installed from another package I already had (`imagemagick`).\n",
    " - I want to also mention that I tried a lot of other things before I got this solution working. I tried `dvisgm` which converts a DVI file generated by TexLive through the command `dvilualatex`. This caused problems with fonts not working properly and what not, and I couldn't be bothered getting this particular solution to work.\n",
    " - Also, converting to a PNG works fine as well, but the theme for the text editor I'm using to create this document is black, so I can't see the png files since they have transparent backgrounds and black text. The jpg files produced have white backgrounds, resolving this issue.\n",
    " - Note: I'm using VSCode to write this markdown document. You can press `ctrl+shift+v` to open up a preview window of your markdown, which is what I'm doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
